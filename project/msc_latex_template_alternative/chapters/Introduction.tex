\chapter{Introduction}



\section{Motivation and Context}




\subsection*{Some History}

\noindent \textbf{Hilbert’s Optimistic Vision of Mathematics}
In September 1928, David Hilbert presented his vision for the foundations of mathematics at the International Congress of Mathematicians in Bologna. He believed it possible to place mathematics on an absolutely secure foundation. This would mean that no matter how difficult a mathematical problem might be, one would only need to ``take up the pen, sit at the abacus, and calculate" \cite{ferreiraProblemaDecisaoMaquina}. The process would be entirely deterministic, requiring no intuition or creativity, only strict adherence to formal rules, like performing multiplication in decimal notation. Every problem would, in principle, be solvable by such mechanical procedures. Mathematics would be both complete (able to answer every question) and consistent (free of contradictions). 

\noindent \textbf{ Gödel’s Incompleteness Theorems} However, this vision was shattered by Kurt Gödel’s Incompleteness Theorems (1931) \cite{Godel}, which showed that no set of mathematical rules powerful enough to handle basic arithmetic could ever be both complete (answering every question) and consistent (free of contradictions) at the same time. 

\noindent \textbf{ Gödel’s Completeness Theorems} 
Interestingly, in his doctoral thesis, Gödel proved a foundational result in logic: first-order predicate logic---a formal system used to express statements involving quantifiers like ``for all'' and ``there exists''---is \emph{complete}~\cite{godelVollstandigkeitAxiomeLogischen1930}. It is important to note that the term ``completeness'' here differs from its use in Gödel’s later Incompleteness Theorems.
Before exploring this notion of completeness, it is helpful to introduce a few core concepts. \emph{Syntax} refers to the formal symbols and inference rules used to construct well-formed statements, while \emph{semantics} concerns the meaning assigned to these statements through interpretations. A \emph{model} of a first-order system is a mathematical structure in which the axioms (or rules) hold true under a given interpretation.
With these notions in place, we can now turn to Gödel’s result, known as the \emph{Completeness Theorem}. This theorem states that  if a statement holds in every possible model of a theory, then it can also be \emph{syntactically} proven using the system’s formal rules. In other words, completeness is the property that all universally valid statements are provable within the system.  The converse also holds: any statement provable syntactically must hold true in all models. This is known as \emph{soundness} \cite{franzenGodelsTheorem2008}.
 


\noindent \textbf{Entscheidungsproblem}
We have just introduced two of the main  pillars of this thesis ---
soundness and completeness. We now introduce a third,  deeply tied to Hilbert's ambitious vision for mathematics. Recall that Hilbert not only sought a complete and consistent foundation for mathematics, but also believed in the possibility of an entirely \emph{mechanical} method to resolve any mathematical problem---a process requiring no intuition or creative insight. In 1928, he formulated the Entscheidungsproblem (German for “decision problem”), which sought an \emph{effective method} (also called a mechanical procedure or algorithm)  to determine the truth or falsity of any mathematical statement \cite{hilbert1928}.  A method or procedure is effective if:
\begin{enumerate}
\item it can be described by a finite number of exact instructions;
\item it produces the desired result after a finite number of steps (provided the instructions are followed without error);
\item it can, in principle, be carried out by a human using only paper and pencil;
\item it does not require any creativity or insight from the human.
\end{enumerate}
The algorithms that children learn to perform basic arithmetic operations are examples of effective procedures. 

   

\noindent \textbf{Alonzo Church and the $\lambda$-calculus enter the scene}
Remarkably, it was Alonzo Church—using $\lambda$-\textit{calculus}—who first addressed Hilbert’s \emph{Entscheidungsproblem}. This brings us to the third central theme of this dissertation: $\lambda$-\textit{calculus}. In 1936, Alonzo Church published a solution to the \emph{Entscheidungsproblem}, proving that no universal algorithmic method could decide the truth of all mathematical statements~\cite{church1936}. Today, this result is often referred to as \emph{Church’s Theorem}. In the same work, he provided a mathematically precise notion of an effective method. He proposed that a function is effectively computable if and only if it can be written as a lambda term. This equivalence provided the first rigorous mathematical criterion for computability.

This calculus played an important role in functional programming, influencing the design of languages like LISP, Pascal, and GEDANKEN—many of which incorporate $\lambda$-calculus-inspired features, either explicitly or implicitly. Furthermore, $\lambda$-calculus may be employed to prove properties of programming language (such as: a well-formed program will not crash) and as a tool in the construction of compilers \cite{jonesImplementationFunctionalProgramming}.

The idea that such important aspects of modern computer science emerged from foundational questions in mathematics is nothing short of extraordinary.

\noindent \textbf{A note on Turing's work}
Around the same time, another researcher—unaware of Church’s work—was independently addressing the same problem: Alan Turing, now widely regarded as the father of computer science. He introduced the concept of a universal machine, now known as the \emph{Turing Machine}. While Turing’s model is groundbreaking in its own right, it is largely orthogonal to this dissertation, as it is more closely associated with automata theory than with the syntax and semantics of programming languages.

%In science, credit typically goes to the first person to prove a result. Yet Turing’s 1937 paper, On Computable Numbers, with an Application to the Entscheidungsproblem, was groundbreaking in its own right. In it, he introduced the concept of a universal machine and demonstrated that any function computable by a human following a systematic procedure could also be computed by a Turing machine \cite{turingComputableNumbersApplication1937}. Moreover, Turing proved that his definition of computability aligned with Church’s.


%Nowadays, computers are ubiquitous household appliances, as commonplace as televisions or refrigerators. Yet there is  something astounding in the fact that the personal computers we use daily trace their roots to an idea born from questions about the foundations of mathematics: the principle of universality in computation. 



\subsection*{ $\lambda$-calculus}
\noindent \textbf{$\lambda$-calculus and functions}
The $\lambda$-calculus is a formal language that captures a key feature of higher-order functional languages: treating functions as ``first-class citizens'' that can be passed as arguments. Here, functions are expressed as \emph{abstractions} of the form $\lambda x.\,f(x)$, with application denoted by juxtaposing the abstraction with its argument. For example, the expression $f(2)$, where $f(x) = x + 1$, is written as $(\lambda x.\,x + 1)(2)$.



\noindent \textbf{Typed-lambda calculus}
In this work, we use the typed $\lambda$-calculus, a variant where each term is ``labeled'' by a syntactic object called a \emph{type}. Types serve as a mechanism for ensuring that programs are meaningful. In contrast, the untyped version allows, for example, a function to be applied to itself, as in $(\lambda x.\,x\,x)(\lambda x.\,x\,x)$, leading to non-termination. It also allows nonsensical operations, such as applying a boolean to a number or a function to a string.


\noindent \textbf{$\lambda$-calculus and logic} 
We previously mentioned that one of our main results pertains to soundness and completeness---a notion we deliberately introduced in the setting of (first-order) logic. In fact, the typed lambda calculus itself is equipped with an equational logic, i.e., a system of equations. These equations arise because the $\lambda$-calculus includes $n$-ary function symbols, which may be accompanied by equality axioms specifying their intended properties. Moreover, the lambda calculus allows us to establish a correspondence between logical proofs and programs. This is known as the \emph{Curry-Howard isomorphism} \cite{girardProofsTypes1989}.

%For instance, one such equation states that a function applying another function $f$ to an argument $x$ can be simplified to $f$ itself: $\lambda x.(f, x) = f$. 
 

%Moreover, the $\lambda$-calculus includes a set of equations that clarify and simplify the meaning of operations. For instance, one such equation states that a function applying another function $f$ to an argument $x$ can be simplified to $f$ itself: $\lambda x.(f, x) = f$. When viewed as a programming language, the $\lambda$-calculus is typically equipped with a  semantics, \ie, its terms are assigned specific meanings. In this work, we are particularly interested in interpreting these terms as mathematical objects, especially those arising in category theory. If the aforementioned equations hold under a given interpretation, we refer to it as a \emph{model}.


\noindent \textbf{Semantics:$\lambda$-calculus and category theory} In this work, we interpret programs as mathematical objects, particularly those arising in category theory. But why choose a categorical interpretation over other alternatives?

Consider an ancient Indian parable: six blind men encounter an elephant for the first time. Each man touches a different part of the animal—the side, tusk, trunk, leg, ear, or tail—and draws a conclusion based solely on that limited experience. One describes it as a spear (the tusk), another as a snake (the trunk), and another as a fan (the ear). Each is convinced of his own interpretation and dismisses the others as incorrect. None of them realise that they are each experiencing only a part of the same elephant, and that their individual descriptions are incomplete. In some versions of the story, the men stop arguing, begin listening to one another, and collaborate to form a more accurate understanding of the whole elephant. 

Category theory plays a similar role in computer science. Each category embodies a distinct perspective ---a "part of the elephant" --- capturing a specific computational paradigm. Adopting a categorical approach allows us to generalize our results across diverse computational paradigms.


However, there is a deeper reason for using category theory in this setting: it is intimately connected to the $\lambda$-calculus. First, it should be noted that $\lambda$-calculus is a type theory --- and here lies the twist: categories themselves can be viewed as type theories. The objects may be regarded as types (of sorts), and the arrows as functions between those types. In this sense, a category may be thought of as a type theory stripped of its syntax. With this perspective in mind, in the 1970s, Joachim Lambek established a correspondence between cartesian closed categories and the $\lambda$-calculus \cite{lambek1980lambda}. That is, types correspond to objects, and programs correspond to arrows in such categories. This correspondence extends further to logic, under the so-called Curry–Howard–Lambek correspondence, where formulas correspond to types and proofs to arrows.  Later, Lambek and Dana Scott independently observed that C-monoids (\ie categories equipped with products, exponentials, and a single non-terminal object) correspond to the untyped $\lambda$-calculus \cite{lambekscott988}.





%any λ-theory (def lambda theory) T determines a Cartesian closed category C(T). Inversely, each Cartesian closed category E determines a λ-theory Th(E). Furtermore this connection extends to the logic: Each lambda calculus theory is the internal logical language of a cartesian closed ategory (the category's own structure defines the logic;The structure of the category (how objects and arrows interact) directly encodes the rules of logic or lambda calculus.)  

%If the aforementioned equations hold under a given interpretation, we refer to it as a \emph{model}.
%One way to study the lambda calculus is to give mathematical models of it, i.e.,to provide spaces in which lambda terms can be given meaning. Such models are  constructed using methods from algebra, partially ordered sets, topology,category theory, and other areas of mathematics



\subsection*{Going quantitative}

Beyond its foundational aspects, this calculus incorporates extensions for modeling side effects, including probabilistic or non-deterministic behaviors and shared memory. In this work, we are concerned with a version of $\lambda$-calculus that allows us to reason about approximate equivalence of programs, referred to as \emph{metric }$\lambda$\emph{-calculus}. The metric lambda calculus integrates notions of
approximation into the equational system of linear lambda calculus, a variant of lambda calculus that restricts each variable to being exactly once.

Program equivalence and its underlying theories traditionally rely on a binary notion of equivalence: two programs are either equivalent or not \cite{winskel93}. While this dichotomy is often sufficient for classical programming, it proves too coarse-grained for other computational paradigms. For instance, in various programming paradigms, interaction with the physical environment calls for notions of approximate program equivalence. 


To address this, \cite{dahlqvistInternalLanguage2022,dahlqvist2023syntactic} incorporate a notion of approximate equivalence into the equational system of the affine $\lambda$-calculus by introducing, among other elements, \emph{metric equations} \cite{mardare2016quantitative, mardare2017axiomatizability}. These are equations of the form $t =_{\varepsilon} s$, where $\varepsilon$ is a non-negative real number representing the ``maximum distance'' between terms $t$ and $s$. Here we begin exploring the incorporation of a metric equation for the case statements (\ie\ conditionals). Our motivation for it is highly practical: in trying to reason quantitatively about higher-order programs, we often fell short when these involved conditionals.

Quantitative logics offer a way forward, extending beyond  $\lambda$-calculus. They reflect a broader effort to move beyond rigid binary concepts, such as equality and bisimulation, and toward more flexible frameworks better suited to real-world computation.

Other works in the spirit of this dissertation include \cite{mardare2016quantitative, mardare2017axiomatizability, mio24, jurka24}, which explore (generalized) metric universal algebras. In simple terms, a universal algebra is a set equipped with any number of operations, further defined by axioms typically expressed as identities or equational laws. In a (generalized) metric universal algebra, these axioms are relaxed into (generalized) metric equations rather than strict equalities. In the higher-order setting, \cite{lago22}, following the framework introduced by Mardare \cite{mardare2016quantitative}, investigates the problem of defining quantitative algebras that are capable of interpreting terms in higher-order calculi.


Probabilistic programs are quite ubiquitous: they control autonomous systems, verify security protocols, and implement randomized algorithms for solving computationally intractable problems. At their core, they aim to democratize probabilistic modeling by providing programmers with expressive, high-level abstractions for machine learning and statistical reasoning \cite{bartheFoundationsProbabilisticProgramming2020}. In this context, concerns such as developing more eco-friendly programs and algorithms could greatly benefit from a quantitative approach.

%Probabilistic programs offer a structured approach to drawing statistical conclusions from uncertain data or real-world observations. They generalize probabilistic graphical models beyond the capabilities of Bayesian networks and are expected to have broader applications in machine intelligence. These programs are are employed across various domains. They control autonomous systems, verify security protocols, and implement randomized algorithms for solving computationally intractable problems. As a result, they are becoming increasingly central to AI development. At their core, they aim to democratize probabilistic modeling by providing programmers with expressive, high-level abstractions for machine learning and statistical reasoning \cite{bartheFoundationsProbabilisticProgramming2020}. 






\subsection*{Quantum Computation}

In 1994, Peter Shor demonstrated that a quantum computer with sufficiently many qubits could pose a significant threat to the security of confidential data transmitted over the Internet \cite{shor1994algorithms}. This breakthrough spurred widespread interest in quantum computing. Nevertheless, \acrfull{nisq}  computers are expected to operate with severely limited hardware resources. Precisely controlling qubits in these systems comes at a high cost, is susceptible to errors, and faces scarcity challenges. Therefore, quantitative reasoning is indispensable for the design, optimization, and assessment of NISQ computing. 


%While holding immense promise, this paradigm encounters challenges in the \acrfull{nisq} era, namely quantum decoherence which restricts the reliable execution of quantum circuits \cite{preskill2018quantum}. Consequently, it is unreasonable to expect that the idealized quantum algorithm will run perfectly on a quantum device, instead only a mere approximation will be observed, which suggests the development of appropriate notions of approximate program equivalence.





\section{Contributions}

Our contributions fall into three categories: syntactic, semantic, and  concerned with the connection between the two.

\subsection*{Syntatic}

We build on the work of \cite{dahlqvist2023syntactic} by introducing a metric equation for conditionals.  Next, to illustrate the utility of the metric equation introduced, we present a simple example: a metric version of the copairing's extensionality. 
Moreover, we illustrate the usefulness of our(metric) equational system by using it as a bridge to connect a certain type to Boolean algebra.
%s: reasoning about approximate equivalence between boolean terms (\ie terms of type $ \typeI \oplus \typeI $), and the extensionality of copairing.


\subsection*{Semantic}

Returning to our earlier elephant parable, we study various “perspectives” by proving that the corresponding categories are indeed models suitable for reasoning about approximate equivalence using this equation:
\begin{itemize}
\item The category of metric spaces $\catMet$;
\item Cocompletion of a $\catMet$-category $\catC$;
\item Category $ \catBan $ of Banach spaces and short maps;
\item Selinger’s \( \catQ \), the category of quantum operationts (\ie, completely positive, trace nonincreasing superoperators) \cite{selinger2004towards}, and Cho’s \( \WstarCPSUop \), the opposite category of \( \WstarCPSU \), the category of W$^*$-algebras and normal, completely positive, subunital maps~\cite{choSemanticsQuantumProgramming2016}.
\end{itemize}
This demonstrates that our work is applicable across several domains. For the last two quantum models, we restrict our attention to the first-order fragment of the $\lambda$-calculus, noting that extensions to higher-order are possible using more advanced categorical tools, as in \cite{dahlqvist2023syntactic}.

We investigate two computational paradigms in greater detail: probabilistic (via $\catBan$) and quantum computation (via $\catQ$ and $\WstarCPSUop$). In the probabilistic setting, we use a random walk to reason about approximate equivalence. In the quantum setting, we use three examples: quantum state discrimination, quantum teleportation, and quantum random walks.
 

%In this context, we show that the category of metric spaces and the cocompletion of a $\catMet$-category $\catC$ satisfy the requirements for a model suitable for reasoning about approximate equivalence using this equation.

%Next, we explore two computational paradigms in greater detail: probabilistic and quantum computation. For the former, we prove that the category $ \catBan $ of Banach spaces and short maps constitutes a model in this setting, and use a random walk as an illustrative example. For the latter, we consider a restriction of $\lambda$-calculus to first order, knowing that later they can be extended to higher-oder using more advanced categorical machinery as in \cite{dahlqvist2023syntactic}.  That beeing said, we consider two categorical models: Selinger’s \( \catQ \), the category of quantum operationts (\ie, completely positive, trace-nonincreasing superoperators) \cite{selinger2004towards}, and Cho’s \( \WstarCPSUop \), the opposite category of W$^*$-algebras and normal, completely positive, subunital maps~\cite{choSemanticsQuantumProgramming2016}. We show that both are first-order models. Finally, we use quantum state discrimination, quantum teleportation, and quantum random walks as illustrative examples in this setting.

\subsection*{Connection between syntax and semantics}
We prove that the metric equation introduced is sound and complete. Soundness ensures that if a metric equation $t =_{\varepsilon} s$ can be derived in the calculus, then the distance between the interpretations of $t$ and $s$ is at most $\varepsilon$. Completeness guarantees that if $\varepsilon$ is the maximum distance between the interpretations of two programs, then we can derive $t =_{\varepsilon} s$ in the calculus. 

\vspace{10pt}
Across all these areas, we also prove several folklore results about conditionals, such as soundness and completeness, that, to our knowledge, are missing from the literature.


%for enriched symmetric monoidal closed categories with binary copr0ducts over metric spaces.


\section{Document Structure}

\Cref{ch:metriclambda} introduces (metric) $\lambda$-calculus along with its categorical interpretation. One advantage of working with a metric equational system is the ability to reason syntactically about approximate equivalence. We leverage this idea in an interlude on booleans (i.e., terms of type $\typeI \oplus \typeI$) to illustrate the usefulness of the classical equational system. In \Cref{ch:conditionals}, we introduce a metric equation for conditionals, prove its soundness and completeness, and present a few models in this setting, along with a few illustrative syntactic examples. Then, in \Cref{ch:pp} and \Cref{ch:qc}, we focus on reasoning about higher-order probabilistic and first-order quantum programs, respectively,  including the necessary background in each domain. The thesis concludes with directions for future work in \Cref{ch:future_work}. An overview of the categorical concepts and results employed in this thesis is provided in \Cref{ch:math_back}. 

Although this work uses knowledge across multiple areas, the author's engagement with them is mostly limited to the scope of this thesis.


