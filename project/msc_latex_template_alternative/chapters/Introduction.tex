\chapter{Introduction}



\section{Motivation and Context}


%Back then the concept of a computer algorithm had not taken shape. Turing first established a working definition for the term – algorithm
%Despite its simplicity, the Turing machine can simulate any computer algorithm, no matter how complex it is. Alan Turing demonstrated that everything that can be reasonably said to be computed by a human using a standard procedure can be computed by the Turing machine. Turing used a human as the basis for his model. He abstracted the actions of a human ‘computer’ using paper and pencil to perform a calculation into a hypothesised machine that could manipulate symbols on an infinite paper tape.


% O	problema da	decisão	e a	máquina	universal de Turing -> Leibneiz, Hilbert (queria por a matematica em terriotio absolutamente seguro),  Church and Turing um lambda calculus outro maquina de turing provaram a a resposta era negativa. Turing no apendice do seu artigo mpstra que as defs de comtutabilidade dos 2 são equivalente . Maquinas de turing -> pi, lambda calculus ->pf
%Em Setembro de 1928, realizou-se em Bolonha o VI Congresso Internacional de Matemáticos. Na sua comunicação ao  congresso, fez um apelo:  É claro que, para o completo desenvolvimento desta tarefa, é necessária a  fervorosa colaboração da geração mais jovem de matemáticos.  De que tarefa falava Hilbert? Tratava-se, sem mais nem menos, de pôr a  matemática em solo absolutamente seguro. Mas porquê? Para isso precisamos de recuar mais uns anos...
%Em 1902 cai sobre a comunidade matemática uma  espécie de bomba: o paradoxo de Russell. Dada uma qualquer condição (p. ex., a  condição de ser homem) podemos considerar a sua extensão (o conjunto de  todos os homens). Normalmente, um conjunto não é membro de si próprio. O  conjunto de todos os homens não é um homem. Mas, por vezes, é-o: o conjunto de todas as abstrações é, ele próprio, uma abstração. Bertrand Russell  considerou o conjunto de todos os conjuntos que não são membros de si  próprios. Põe-se a questão: é o conjunto de Russell membro de si próprio? Se é,  então dada a sua definição (trata-se do conjunto formado pelos conjuntos que não são membros de si próprios), não é. Mas se não é, é. Chegamos a uma  situação paradoxal: o conjunto de Russell nem pode ser membro de si próprio, nem pode deixar de o ser! 
%A descoberta do paradoxo de Russell provocou reações díspares e, por vezes, emotivas, entre os matemáticos. Uns matemáticos  os modernos, atarefaram-se em tentar compreender as razões para o aparecimento do paradoxo e propuseram programas de fundamentação da matemática. Hilbert encontrava-se entre estes últimos e, nos anos vinte do século passado, propôs o seu próprio programa de  fundamentação da matemática: o formalismo. Este programa competia com o  programa logicista do próprio Russell e com o denominado programa  intuicionista do matemático holandês L. E. J. Brouwer. 
% O programa de Hilbert concebia a matemática como uma espécie de jogo. No jogo  de xadrez há a partida propriamente dita, jogada de acordo com regras exatas.  Há, também, a teoria sobre o jogo de xadrez. Faz parte desta teoria a afirmação  de que não é possível dar cheque-mate com apenas dois cavalos a um rei isolado.  Já não se trata agora de jogar uma partida de xadrez mas sim de investigar o jogo  em si. É metaxadrez. A matemática, quando vista axiomaticamente, também tem  regras bem definidas. Elas são dadas pelos axiomas e pelas regras de inferência.  Os axiomas correspondem ao tabuleiro inicial duma partida de xadrez – as peças  na sua posição inicial – e as regras de inferência correspondem às jogadas  permitidas. A ideia fundamental do formalismo é ver a matemática como um jogo  (dedutivo) simbólico. As peças deste jogo simbólico são as fórmulas da linguagem da matemática. Para que este jogo tenha regras exatas, como acontece  no xadrez, é necessário simbolizar (formalizar) completamente a linguagem da  matemática. Não basta enunciar o primeiro axioma da geometria euclidiana  dizendo que por dois pontos passa uma reta. É necessário formulá-lo numa  linguagem exata. Na notação da lógica simbólica, o axioma de Euclides escreve-se  assim: ∀x∀y(Px ∧ Py → ∃z(Rz ∧ Ixz ∧ Iyz)) As regras de inferência permitem gerar (deduzir) novas  fórmulas a partir de outras fórmulas. Uma regra de inferência básica é a regra do  modus ponens. Por exemplo:   Se x incide sobre z, então x é um ponto  x incide sobre z ___________________________________________________________________  Logo, x é um ponto  Na notação da lógica, podemos inferir a configuração simbólica “Px” a partir das configurações “Ixz → Px” e “Ixz”. O “jogo de fórmulas” consiste em gerar certas  sequências de símbolos (os teoremas) a partir de determinadas sequências  dadas inicialmente (os axiomas) por intermédio das regras de inferência. Hilbert exige que, no cálculo axiomático formal, não se chegue a contradições, i. e. não se chegue a fórmulas da forma “A e não-A” (simbolicamente “A ∧¬A”). O  “jogo”, neste caso, deixaria de ter interesse até porque, duma contradição, é  possível inferir qualquer fórmula. Por isso o paradoxo de Russell é tão  devastador: toda a fórmula seria um teorema. 
%Na sua palestra em Bolonha, Hilbert menciona vários problemas relacionados  com o seu programa fundacional. Um é o problema de consistência que acabámos  de discutir. Hilbert pretendia mostrar que as regras do jogo formal das  axiomáticas importantes para a matemática – um jogo cujas regras são  completamente exatas, como é o xadrez – nunca levariam à dedução de  contradições. Outro dos problemas  mencionados na palestra foi o problema da completude. No caso da axiomática da  aritmética, este problema consiste em mostrar que, para qualquer asserção simbólica da linguagem formal da aritmética, ou ela ou a sua negação é um  eorema. Um terceiro problema foi também mencionado em 1928, mas num livro de Hilbert com o seu aluno Wilhelm Ackermann. É o problema da decisão,  também conhecido pelo seu nome alemão: Entscheidungsproblem. Hilbert e  Ackermann escreveram que “o Entscheidungsproblem deve ser considerado o  problema principal da lógica matemática” (em itálico no original).

%Hilbert e Ackermann escreveram que “o Entscheidungsproblem deve ser considerado o  problema principal da lógica matemática” (em itálico no original). O problema da decisão é o problema de encontrar um método efetivo (também  se diz mecânico ou algorítmico) de acordo com o qual, dada uma fórmula da  linguagem do cálculo de predicados, se determina se essa fórmula é, ou não, um  teorema da lógica (i. e. deduzível apenas a partir dos axiomas do cálculo de  predicados).  Um método ou procedimento é efetivo se:  1. puder ser descrito através dum número finito de instruções exatas;  2. produzir o resultado desejável ao fim dum número finito de passos  (desde que se sigam as instruções sem erro);  3. puder, em princípio, ser executado por um ser humano apenas com a  ajuda de papel e lápis;  4. não exigir nem criatividade nem perspicácia por parte do ser humano.  Os algoritmos que as crianças aprendem para efetuar as operações básicas da aritmética são exemplos de procedimentos efetivos. 
%Uma solução positiva para os três problemas hilbertianos da completude, consistência e decisão subscreveria uma visão magnífica da matemática. A  matemática poderia ser vista como um grandioso cálculo formal – consistente, completo e decidível. A visão leibniziana dum calculus ratiocinator no domínio  da matemática teria sido plenamente justificada. Perante um problema  matemático, por mais difícil que fosse, bastaria “pegar na pena e sentar-se ao  ábaco e (na presença de um amigo, se se quiser) dizer um para o outro: calculemus”. O cálculo seria efetivo, cego, como na multiplicação em numeração  decimal, e daria sempre resposta. A matemática seria segura (consistência) e responderia a todas as questões (completude)
%Não se tinham passado ainda três anos sobre a palestra de Hilbert em Bolonha  quando o jovem matemático austríaco Kurt Gödel (nascido em Brno, hoje na  República Checa, em 1906) mostra que a axiomática da aritmética, desde que seja consistente, não é completa. Umas semanas mais tarde, mostra que não é  possível demonstrar a consistência da aritmética por meio de métodos aceitáveis  para Hilbert (os métodos da metamatemática). Curiosamente, Gödel fez o ,anúncio público do primeiro destes resultados também em Königsberg,  precisamente na véspera da comunicação de Hilbert de 1931. A volte-face foi  totalmente inesperada e apanhou de surpresa o mundo matemático. Os  resultados de Gödel demoraram algum tempo a ser plenamente apreciados e  compreendidos. Afinal, o programa de Hilbert não era possível de levar a cabo. Afinal o grande matemático estava errado.
%O artigo de Turing de 1936 “On computable numbers with an application to the  Entscheidungsproblem” (Sobre números computáveis com uma aplicação ao  Entscheidungsproblem) é brilhante. Contém um análise conceptual do que é um  processo efetivo e, concomitantemente, propõe uma definição matematicamente  rigorosa do que se pode calcular com tais processos. Contém a descrição duma  “super-máquina” com poder computacional para executar qualquer processo  efetivo. Mostra que há problemas de decisão (i. e. de resposta “sim” ou “não”)  que não têm soluções através de processos efetivos. Finalmente, contém a solução para o Entscheidungsproblem.
%Turing participou na história da  construção de computadores, nomeadamente na construção de um dos  primeiros computadores eletrónicos, o ACE (Automatic Computing Engine). O  relatório técnico que Turing escreveu para o ACE (provavelmente nos últimos  meses de 1945) continha discussões detalhadas de engenharia e, inclusivamente,  avançava com uma verba concreta para o custo da construção da máquina.  Naturalmente, a construção dum computador digital é em grande parte um  empreendimento de engenharia. Porém, ainda que seja interessante falar dos  problemas complicados de engenharia e de programação que tiveram que ser  superados para construir os computadores electrónicos assim como mencionar o  incrível avanço tecnológico a que temos assistido desde meados dos anos quarenta, não se pode esquecer que venceu uma conceção de computador que  desafia o senso comum. Ainda em 1956 (vinte anos depois do artigo seminal de  Turing!), Howard Aiken, um dos pioneiros da computação e professor na Universidade de Harvard afirmava: Howard Aiken, um dos pioneiros da computação e professor na  Universidade de Harvard afirmava: … se porventura vier a acontecer que as bases lógicas de uma máquina  concebida para a solução numérica de equações diferenciais coincide com as  lógicas duma máquina concebida para a contabilidade de um armazém,  considerá-la-ia a mais espantosa coincidência que alguma vez encontrei. 
%Este é o senso comum, mas os computadores atuais não são como o senso comum sugere. São computadores de propósito geral, capazes de ler software adequado, este sim de propósito específico. Os computadores atuais permitem ter como entradas instruções (software) desenhadas para desempenhar tarefas específicas: são, em suma, incorporações da ideia de máquina universal. Hoje em dia os computadores são um utensílio doméstico, como uma televisão ou um frigorífico. Há qualquer coisa de pasmoso no facto de que, na raíz dos computadores pessoais que usamos hoje diariamente, esteja uma ideia (a ideia da universalidade em computação) que surgiu por intermédio de questões da fundamentação da matemática. 
%Na parte final do seu artigo, como aplicação dos conceitos anteriormente  introduzidos, Turing resolve o Entscheidungsproblem. O título da última secção é,  precisamente, “Aplicação ao Entscheidungsproblem”. Turing baseia-se em  trabalho que efetuou numa secção prévia, onde exibe um problema de decisão  que não pode ser decidido de modo efetivo (um problema destes diz-se  indecidível). O problema é o seguinte: decidir, dadas entradas αM e β, se a  máquina M para com a entrada β, i. e. se M(β)↓. A este problema chama-se o  problema da paragem (“halting problem”, em inglês). O raciocínio de Turing é  por redução ao absurdo. Turing vai supor que o problema da paragem é  decidível e, a partir desta suposição, vai chegar a uma contradição. O problema da paragem é o problema indecidível paradigmático. Em geral,  mostra-se que um dado problema é indecidível através da redução do problema  da paragem a esse problema. A ideia é simples. Para mostrar que um problema é  indecidível mostra-se que se fosse decidível então o problema da paragem  também o seria. Quod non (o que não é o caso). 
%Quando Turing estava a terminar o seu artigo na primavera de 1936, tomou  conhecimento de um trabalho do lógico americano Alonzo Church que também  continha uma solução para o Entscheidungsproblem. O artigo intitulava-se “A  note on the Entscheidungsproblem”.
%o artigo  de Turing introduz o conceito de máquina universal e argumenta que a noção de  máquina de Turing captura a noção informal de efetividade computacional. 
%A identificação da noção informal de função efetivamente computável com uma noção matematicamente rigorosa tinha sido primeiramente proposta, também por Church, num outro artigo do ano de 1936.
%(mais tarde, no apêndice ao seu artigo de 1936, Turing mostra que  a sua definição de computabilidade em termos de máquina de Turing coincide  com a noção de Church e Kleene: as três noções são, pois, equivalentes). A  identificação entre estas noções formais e a noção informal de computabilidade  efetiva ficou conhecida na literatura por tese de Church.
%No caso da computabilidade efetiva, dá-se um “milagre” porque é possível  caracterizar os processos mecânicos básicos que dão origem a toda e qualquer computação efetiva, o que está estreitamente ligado a não ser possível decidir  efetivamente se uma determinada lista de instruções básicas dá, ou não, origem a um processo que termina.


%Any problem for which the answer is yes or no is a 'decision problem'. Other problems for which the answer is to find an optimal solution (such as a shortest path or something similar) are not 'decision problems'. The halting problem is the decision problem asking whether a given program will terminate in finite time (on given input). The halting problem is proven to be unsolvable over Turing machines. Meaning no computer program can decide whether another program will terminate or run forever.
%O Church foi o primeiro a provar -> The part of the λ-calculus dealing only with functions turned out to be  quite successful. Using this theory Church proposed a formalization of the  notion "effectively computable" by the concept of λ-definability. Kleene  [1936] showed that λ-definability is equivalent to Gödel-Herbrand re cursiveness and in the meantime Church formulated his thesis (stating that  recursiveness is the proper formalization of effective computability). Turing [1936], [1937] gave an analysis of machine computability and showed  that the resulting notion (Turing computable) is equivalent to λ-definability.
%By the analysis of Turing it follows that in spite of its very simple syntax,  the λ-calculus is strong enough to describe all mechanically computable  functions. Therefore the λ-calculus can be viewed as a paradigmatic  programming language. This certainly does not imply that one should use  it to write actual programs. What is meant is that in the λ-calculus several  programming problems, specifically those concerning procedure calls, are  in a pure form. For example, several programming languages have features inspired by  λ-calculus (perhaps unconsciously so). 


\subsection*{Some History}

In September 1928, David Hilbert presented his vision for the foundations of mathematics at the International Congress of Mathematicians in Bologna. He believed it would be possible to place mathematics on an absolutely secure foundation. This would mean that no matter how difficult a mathematical problem might be, one would only need to "take up the pen, sit at the abacus, and calculate." The process would be entirely deterministic—requiring no intuition or creativity, only strict adherence to formal rules, like performing multiplication in decimal notation. Every problem would, in principle, be solvable by such mechanical procedures. Mathematics, would be both complete (able to answer every question) and consistent (free of contradictions). 

However, this vision was shattered by Kurt Gödel’s Incompleteness Theorems (1931) \cite{Godel}, which showed that no set of mathematical rules powerful enough to handle basic arithmetic could ever be both complete (answering every question) and consistent (free of contradictions) at the same time.


Why is this relevant to the present dissertation? Remarkably, it was Alonzo Church—using $\lambda$-\textit{calculus}—who first addressed Hilbert’s \emph{Entscheidungsproblem} (German for ``decision problem'') \cite{hilbert1938}, a cornerstone in his formalism. This \emph{Entscheidungsproblem} sought an effective method (also called a mechanical procedure or algorithm) to determine the truth or falsity of any mathematical statement. A method or procedure is effective if:
\begin{enumerate}
    \item it can be described by a finite number of exact instructions;
    \item it produces the desired result after a finite number of steps (provided the instructions are followed without error);
    \item it can, in principle, be carried out by a human using only paper and pencil;
    \item it does not require any creativity or insight from the human.
\end{enumerate}
The algorithms that children learn to perform basic arithmetic operations are examples of effective procedures.

 In 1936, Alonzo Church published a solution to the \emph{Entscheidungsproblem}, proving that no such universal method exists for all mathematical statements~\cite{church1936}. In the same work, he introduced a mathematically precise definition of effective computability using what is now known as the $\lambda$-calculus. Today, this result is often referred to as Church’s Theorem.

 At the same time, another individual was working on this problem independently—without prior knowledge of Church’s work. His name was Alan Turing. In science, credit typically goes to the first person to prove a result. Yet Turing’s 1937 paper, On Computable Numbers, with an Application to the Entscheidungsproblem, was groundbreaking in its own right. In it, he introduced the concept of a universal machine and demonstrated that any function computable by a human following a systematic procedure could also be computed by a Turing machine \cite{turingComputableNumbersApplication1937}. Moreover, Turing proved that his definition of computability aligned with Church’s.

By Turing’s analysis, it became clear that the $\lambda$-calculus was sufficiently powerful to express all mechanically computable functions.
This calculus played an important role in functional programming, influencing the design of languages like LISP, Pascal, and GEDANKEN—many of which incorporate $\lambda$-calculus-inspired features, either explicitly or implicitly.

Nowadays, computers are ubiquitous household appliances, as commonplace as televisions or refrigerators. Yet there is  something astounding in the fact that the personal computers we use daily trace their roots to an idea born from questions about the foundations of mathematics: the principle of universality in computation. 

The idea that such important aspects of modern computer science emerged from foundational questions in mathematics is nothing short of extraordinary.




\subsection*{$\lambda$-calculus}

%The Lambda Calculus, developed by Church and Curry in the 1930s, serves as a formal language capturing the key attribute of higher-order functional languages, treating functions as first-class citizens, allowing them to be passed as arguments. 



The concept of a function takes a central role in the lambda calculus. But what exactly is a function?  In most mathematics, the ``functions as graph'' paradigm is the most elegant and appropriate framework for understanding functions. Within this paradigm, each function $f$ has a fixed domain $X$ and a fixed codomain $Y$. The function $f$ is then a subset of $X \times Y$ that satisfies the property that for each $x \in X$ there is a unique $y \in Y$ such that $(x,y) \in f$. Two functions $f$ and $g$ are equal if they yield the same output on each input, that is if $f(x) = g(x)$ for all $x \in X$. This perspective is known as the \emph{extensional} view of functions, as it emphasizes that the only observable property of a function is how it maps inputs to outputs.

From a Computer Science perspective, this is not very useful. We are typically just as concerned with how a function computes its result as we are with what it produces. For instance, consider sorting: every correct sorting algorithm, from the simplest to the most sophisticated, produces the same output for a given input. Yet entire books and research papers are devoted to analyzing different sorting techniques. Clearly, something important is being overlooked. The casual use of the term ``algorithm'' in that context is revealing: a function should be represented not by its graph, but by the rule or process that describes how its result is computed. Such a rule is often expressed in the form of a formula, for example, \( f(x) = x^2 \). As with the mathematical paradigm, two functions are considered extensionally equal if they exhibit the same input-output behavior. However, this view also introduces the notion of \emph{intensional} equality: two functions are intensionally equal if they are defined by (essentially) the same formula.



In the lambda calculus, functions are described explicitly as \emph{formulae}. The function $f:x \mapsto f(x)$ is represented as $\lambda x.f(x)$.  Applying a function to an argument is done by juxtaposing the two expressions. For instance consider the function $f:x \mapsto x+1$, to compute $f(2)$ one writes $(\lambda x.x+1)(2)$.

A major limitation of the notation appears to be that we can only define unary functions, that is, we can introduce only one  argument at a time. However, this is not a true restriction.  Suppose we have a binary function represented as an expression with formal arguments $ x $ and $ y $, say $f(x, y)$. Then we can define a new function $ g $ as $g = \lambda y.\, (\lambda x.\, f(x, y))$.
This function $ g $ is equivalent to the original binary function $ f $, but it takes its arguments one at a time. This idea, known as \emph{currying}, shows how functions of multiple arguments can be represented using only unary functions.

The expression of \emph{higher‑order functions}, functions whose inputs and/or outputs are themselves functions, in a simple manner is an essential feature of lambda calculus. For example, the composition operator $f,g \mapsto f \circ g$ is written as $\lambda f. \lambda g. \lambda x. f(g(x))$. Considering the functions $f:x \mapsto x^2$ and $g:x \mapsto x+1$, to compute $(f \circ g)(2)$ one writes $$(\lambda f. \lambda g. \lambda x. f(g(x)))(\lambda x.x^2)(\lambda x.x+1)(2).$$

As mentioned above,  within  the “functions as rules” paradigm, is not
always necessary to specify the domain and codomain of a function in advance. For instance, the identity function $f: x \mapsto x$, can have any set $X$ as its domain and codomain, provided that the domain and codomain are the same. In this case, one says that $f$ has type $X \rightarrow{} X$.% In the case of the composition operator, $h=\lambda f. \lambda g. \lambda x. f(g(x))$, the domain and codomain of the functions $f$ and $g$ must match. Specifically, $f$ can have any set $X$ as its domain and any set  $Y$ as its codomain, provided that $Y$ is the domain of $g$. Similarly, $g$ can have any set $Z$ as its codomain.  Thus,  $h$ has type $$(X \rightarrow{} Y) \rightarrow{} (Y \rightarrow{} Z) \rightarrow{} (X \rightarrow{} Z).$$ 

This flexibility regarding domains and codomains enables operations on functions that are not possible in ordinary mathematics. For instance, if $f = \lambda x.x$ is the identity function, then one has that $f(x) = x$ for any $x$. In particular, by substituting $f$ for $x$, one obtains $f(f) = (\lambda x.x)(f) = f$. Note that the equation $f(f) = f$ is not valid in conventional mathematics, as it is not permissible, due to set-theoretic constraints, for a function to belong to its own domain.

Nevertheless this remarkable aspect of lambda calculus, this work focuses on a more restricted version of the lambda calculus, known as the simply-typed lambda calculus. Here, each expression is always assigned a type, which is very similar to the situation in mathematics. A function may only be applied to an argument if the argument's type aligns with the function's expected domain. Consequently, terms such as $f(f)$ are not allowed, even if $f$ represents the identity function.



\todo[inline]{Incluir cena do Curry-Howard isomorphism? -> Não quero q me comecem a fazer perguntas sobre isto}
On another note, the lambda calculus not only allows us to express all computable functions, but also establishes a correspondence between logical proofs and programs. This is known as the \emph{Curry-Howard isomorphism} \cite{girardProofsTypes1989}.




\subsubsection*{Metric $\lambda$-calculus}

Beyond its foundational aspects, this calculus incorporates extensions for modeling side effects, including probabilistic or non-deterministic behaviors and shared memory. In this work, we are concerned with a version of $\lambda$-calculus that allows us to reason about approximate equivalence of programs, refered to as \emph{metric }$\lambda$\emph{-calculus}.

Program equivalence and its underlying theories traditionally rely on a binary notion of equivalence: two programs are either equivalent or not \cite{winskel93}. While this dichotomy is often sufficient for classical programming, it proves too coarse-grained for other computational paradigms. For instance, in various programming paradigms, interaction with the physical environment calls for notions of approximate program equivalence. 

To address this, \cite{dahlqvistInternalLanguage2022,dahlqvist2023syntactic} incorporate a notion of approximate equivalence into the equational system of the affine $\lambda$-calculus by introducing, among other elements, \emph{metric equations} \cite{mardare2016quantitative, mardare2017axiomatizability}. These are equations of the form $t =_{\varepsilon} s$, where $\varepsilon$ is a non-negative real number representing the ``maximum distance'' between terms $t$ and $s$. Here we start investigating the incorporation of a metric equation for the case statements (\ie\ conditionals). Our motivation for it is highly practical: in trying to reason quantitatively about higher-order programs we often fell short when these involved conditionals.


Other works in the spirit of this dissertation include \cite{mardare2016quantitative, mardare2017axiomatizability, mio24, jurka24}, which explore (generalized) metric universal algebras. A universal algebra, in simple terms, is a set equipped with any number of operations, further defined  by axioms typically expressed as identities or equational laws. In a (generalized) metric universal algebra, these axioms are relaxed into (generalized) metric equations rather than strict equalities. In the higher-order setting, \cite{lago22}, following the framework introduced by Mardare \cite{mardare2016quantitative}, investigates the problem of defining quantitative algebras that are capable of interpreting terms in higher-order calculi.
\todo[inline,size=\normalsize]{Falar do $\&$, não falor do $\&$, eis a questão }



\begin{comment}
Remarkably a number of important results already considered additive structure
in the quantalic setting, even if sometimes implicitly.
References~\cite{mardare2016quantitative,mardare2017axiomatizability,mio24,jurka24}
for example are framed in the setting of universal algebra and therefore
involve additive conjunction (\ie\ $\&$), typically interpreted via categorical
products.  In the higher-order setting, \cite{lago22} enforces additive
conjunction to be left adjoint to implication (interpreted via
Cartesian-closedness), with a series of negative results emerging from this.
Our work is orthogonal to these in that we study the dual of $\&$ (\ie\
$\oplus$) and furthermore we assume the left adjoint of implication to be
multiplicative conjunction (\ie\ $\otimes$) instead of the additive
counterpart. Among other things, this removes the obstacles discussed
in~\cite{lago22}.
\end{comment}



\subsection*{Probabilistic Programming}


%Probabilistic programs offer a structured approach to drawing statistical conclusions from uncertain data or real-world observations. They generalize probabilistic graphical models beyond the capabilities of Bayesian networks and are expected to have broader applications in machine intelligence. These programs are are employed across various domains. They control autonomous systems, verify security protocols, and implement randomized algorithms for solving computationally intractable problems. As a result, they are becoming increasingly central to AI development. At their core, they aim to democratize probabilistic modeling by providing programmers with expressive, high-level abstractions for machine learning and statistical reasoning \cite{bartheFoundationsProbabilisticProgramming2020}. 


Computer science and probability theory have shared a fruitful relationship since the early days \cite{de1956computability}. Over the years, probabilistic algorithms have emerged as powerful tools, across diverse domains—from machine learning \cite{pearl2014}  and robotics \cite{thrun2002robotic} to computational linguistics \cite{manning1999foundations}. These algorithms play a pivotal role in modern cryptography, particularly in public-key systems \cite{goldwasserProbabilisticEncryption1984}, and tackle computationally intractable  problems \cite{motwaniRandomizedAlgorithms1995}.

The growing influence of probabilistic methods has also spurred the development of probabilistic programming languages, both concrete and abstract. Early examples include higher-order probabilistic languages like Church \cite{Church2008}, while more recent innovations, such as Anglican \cite{Anglican2015}, continue to expand the expressive power and practicality of probabilistic programming.


In this setting, Crubillé and Dal Lago introduced the notion of a \emph{context distance} in~\cite{crubilleMetricReasoningLterms2015,crubilleMetricReasoninglambda2017}, as a metric analogue of Morris' context equivalence. In Morris' framework, two programs are said to be \emph{context equivalent} if their observable behavior---that is, 
what an external observer can measure during execution---is identical in any context.  This distance was first developed for an affine $\lambda$-calculus and later extended to a more general 
setting that, for instance, allows copying.

In \cite{dahlqvist2023syntactic}, the authors reason about approximate equivalence using the \emph{operator norm} on the space of probabilistic programs (which are interpreted as short maps between Banach spaces).








%maybe questões de energia



\subsection*{Quantum Computation}


\todo[inline,size=\normalsize]{Reduzir parte quântica: tirar histórias das linguagens e assim, mas deixar as do selinger}

\todo[inline,size=\normalsize]{Parte das op algebras?-> Tradicionalmente faz-se blá, mas existe outra perspetiva... Kenta Cho vs Selinger + temos espaços infinitos}



Quantum computing dates back to 1982 when Nobel laureate Richard Feynman proposed the idea of constructing computers based on quantum mechanics principles to efficiently simulate quantum phenomena \cite{feynman2018simulating}. 

The field has since evolved into a multidisciplinary research area that combines quantum mechanics, computer science, and information theory. Quantum information theory, in particular, is based on the idea that if there are new physics laws, there should be new ways to process and transmit information.  In classical information theory, all systems (computers, communication channels, etc.) are fundamentally equivalent, meaning they adhere to consistent scaling laws. These laws, therefore, govern the ultimate limits of such systems. For instance, if the time required to solve a particular problem, such as the factorization of a large number, increases exponentially with the size of the problem, this scaling behavior remains true irrespective of the computational power available.  Such a problem, growing exponentially with the size of the object, is known as a "difficult problem". However, as demonstrated by Peter Shor, the use of a quantum computer with a sufficient number of quantum bits (qubits) could significantly accelerate the factorization of large numbers \cite{shor1994algorithms}.  This advancement poses a significant threat to the security of confidential data transmitted over the Internet, as the RSA algorithm is based on the computational difficulty of factorizing large numbers.

%In classical information theory, all systems (computers, communication channels, etc.) are equivalent. The systems might be quick or slow, but the scaling rules are always the same. Consequently, these laws rule the ultimate limits of the systems. If the calculation time to process a given  problem, for example, the factorization of a number, increases exponentially with the size of the object under consideration, this law of scale will be true regardless of the power of the computer. Such a problem, growing exponentially with the size of the object, is known as a "difficult problem". As shown by Peter Shor, factorizing large numbers would be tremendously accelerated if one has a quantum computer with many quantum bits. This would ruin the actual encoding of confidential data on the internet, as the RSA algorithm is based on the difficulty of factorizing large numbers.
%In 1994, Peter Shor introduced an algorithm that could factorize large numbers in polynomial time, a task that is believed to be intractable for classical computers [\cite{shor1994algorithms}]. In 1996, Lov Grover presented a quantum search algorithm that could search an unsorted database quadratically faster than classical algorithms [\cite{grover1996fast}].

%as classical computers seemed ill-suited for this task. 

%The quantum computing paradigm holds immense promise, as evidenced by several compelling results in computational complexity theory 

The quantum computing paradigm holds immense promise, as evidenced by this compelling result in computational complexity theory.  While hardware advancements have brought the scientific community closer to realizing this potential, the ultimate goal is yet to be accomplished. A \acrfull{nisq} computer equipped with 50-100 qubits may surpass the capabilities of current classical computers, yet the impact of quantum noise, such as decoherence in entangled states, imposes limitations on the size of quantum circuits that can be executed reliably \cite{preskill2018quantum}. Unfortunately, general-purpose error correction techniques \cite{calderbank1996good, gottesman1997stabilizer, steane1996error} consume a substantial number of qubits, making it difficult for \acrshort{nisq} devices to make use of them in the near term. For instance, the implementation of a single logical qubit may require between $10^3$ and $10^4$ physical qubits \cite{fowler2012surface}. As a result, it is unreasonable to expect that the idealized quantum algorithm will run perfectly on a quantum device, instead
only a mere approximation will be observed.

To reconcile quantum computation with \acrshort{nisq} computers, quantum compilers perform transformations for error mitigation \cite{wallman2016noise} and noise-adaptive optimization \cite{murali2019noise}. Additionally, current quantum computers only support a restricted, albeit universal, set of quantum operations. As a result, nonnative operations must be decomposed into sequences of native operations before execution \cite{harrow2002efficient,burgholzer2020advanced}. The assessment of these compiler transformations necessitates a comparison of the error bounds between the source and compiled quantum programs. 

This suggests the development of appropriate notions of approximate program equivalence, \textit {in lieu} of the classical program equivalence and underlying theories that typically hinge on the idea that equivalence is binary, \textit{i.e.} two programs are either equivalent or they are not \cite{winskel1993formal}.

%Furthermore, in quantum information theory, the concept of an $\epsilon-\text{approximation}$ channel is fundamental when studying quantum teleportation via noisy channels \cite{watrous2018theory}. 

As previously noted, Shor's algorithm has played a pivotal role in sparking heightened interest within the scientific community toward quantum computing research. Several quantum programming languages have surfaced over the past 25 years \cite{zhao2020quantum,serrano2022quantum}. These include imperative languages such as Qiskit \cite{Qiskit} and Silq \cite{bichsel2020silq}, as well as functional languages such as Quipper \cite{green2013quipper} and Q\# \cite{svore2018q}. On one hand, the design of quantum programming languages is strongly oriented towards implementing quantum algorithms. On the other hand, the  definition of functional paradigmatic languages or functional calculi serves as a valuable tool for delving into theoretical aspects of quantum computing, particularly exploring the foundational basis of quantum computation \cite{zorzi2016quantum}. 

%Given the nature of this work, the focus will be on quantum languages designed with this latter aspect in mind. 
%QPL, a quantum language within the functional programming paradigm, marks a significant milestone in this context \cite{selinger2004towards}. It is a first-order functional language with a static type system that integrates classical control and quantum data, and its denotational semantics is based on superoperators. 

Most of the current research on algorithms and programming languages assumes that addressing the challenge of noise during program execution will be resolved either by the hardware or through the implementation of fault-tolerant protocols designed independently of any specific application \cite{chong2017programming}. As previously stated, this assumption is not realistic in the \acrshort{nisq} era. Nonetheless, there have been efforts to address the challenge of approximate program equivalence in the quantum setting. 

\cite{hung2019quantitative} and \cite{tao2021gleipnir} reason about the issue of noise in a quantum while-language by developing a deductive system to determine how similar a quantum program is from its idealised, noise-free version. The former introduces the ($Q$,$\lambda$)-diamond norm which analyzes the output error given that the input quantum state satisfies some quantum predicate $Q$ to degree $\lambda$. However, it does not specify any practical method for obtaining non-trivial quantum predicates. In fact, the methods used in \cite{hung2019quantitative} cannot produce any post conditions other than $(I,0)$ (\textit{i.e.}, the identity matrix $I$ to degree 0, analogous to a ``true” predicate) for large quantum programs. The latter specifically addresses and delves into this aspect.  

An alternative approach was explored in \cite{dahlqvist2023syntactic}, using linear $\lambda$-calculus as basis – \textit{i.e} programs are written as linear $\lambda$-terms – which has deep connections to both logic and category theory \cite{girard1995advances}, \cite{benton1994mixed}. A notion of approximate equivalence is then
integrated in the calculus via the so-called diamond norm , which induces a metric (roughly, a distance function) on the space of quantum programs (seen semantically as completely positive trace-preserving super-operators) \cite{watrous2018theory}. The authors argue that their deductive system allows to compute an approximate distance between two quantum programs easily as opposed to computing an exact distance ``semantically" which tends to involve quite complex operators.  Some positive results were achieved in this setting, but much remains to be done.




%falar do lambda calculus e da sua ambivalencia e mencionar que nos vamos focar no caso quantico

\section{Contributions}
%The notion of approximate equivalence for quantum programming explored in \cite{dahlqvist2023syntactic} does not take important operations into account. Specifically, the corresponding mathematical model does not include measurements, classical control flow, or discard operations. Also, the corresponding typing system is often times too strict and cannot properly handle multiple uses of the same resource, such as sampling exactly $n$-times from a distribution. The overarching goal of this M.Sc. project is to tackle the aforementioned limitations. A successful completion of this goal will provide a fully-fledged quantum programming language on which to study metric program equivalence in various scenarios. This includes not only quantum algorithmics – where, for example, the number of iterations in Grover’s algorithm involves approximations – but also quantum information theory, where, for instance, quantum teleportation and the problem of the discrimination of quantum states have important roles \cite{nielsen2010quantum}.


\section{Document Structure}
%Notação
%dizer que vamos acrescentando novas regras ao longo dos capitulos ao nosso lambda calculua -> construção incremental do sistema

\todo[inline,size=\normalsize]{Nota sobre conhecimentos q assumo}

\todo[inline,size=\normalsize]{Nota sobre conhecimentos q tenho -> conhecimento superficial o qual apresento, não mais de que isso}