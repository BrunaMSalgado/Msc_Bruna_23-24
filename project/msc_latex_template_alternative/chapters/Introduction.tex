\chapter{Introduction}

\newacronym{nisq}{NISQ}{Noisy Intermediate-Scale Quantum}

\section{Motivation and Context}


Quantum computing dates back to 1982 when Nobel laureate Richard Feynman proposed the idea of constructing computers based on quantum mechanics principles to efficiently simulate quantum phenomena \cite{feynman2018simulating}. 

The field has since evolved into a multidisciplinary research area that combines quantum mechanics, computer science, and information theory. Quantum information theory, in particular, is based on the idea that if there are new physics laws, there should be new ways to process and transmit information.  In classical information theory, all systems (computers, communication channels, etc.) are fundamentally equivalent, meaning they adhere to consistent scaling laws. These laws, therefore, govern the ultimate limits of such systems. For instance, if the time required to solve a particular problem, such as the factorization of a large number, increases exponentially with the size of the problem, this scaling behavior remains true irrespective of the computational power available.  Such a problem, growing exponentially with the size of the object, is known as a "difficult problem". However, as demonstrated by Peter Shor, the use of a quantum computer with a sufficient number of quantum bits (qubits) could significantly accelerate the factorization of large numbers \cite{shor1994algorithms}.  This advancement poses a significant threat to the security of confidential data transmitted over the Internet, as the RSA algorithm is based on the computational difficulty of factorizing large numbers.

%In classical information theory, all systems (computers, communication channels, etc.) are equivalent. The systems might be quick or slow, but the scaling rules are always the same. Consequently, these laws rule the ultimate limits of the systems. If the calculation time to process a given  problem, for example, the factorization of a number, increases exponentially with the size of the object under consideration, this law of scale will be true regardless of the power of the computer. Such a problem, growing exponentially with the size of the object, is known as a "difficult problem". As shown by Peter Shor, factorizing large numbers would be tremendously accelerated if one has a quantum computer with many quantum bits. This would ruin the actual encoding of confidential data on the internet, as the RSA algorithm is based on the difficulty of factorizing large numbers.
%In 1994, Peter Shor introduced an algorithm that could factorize large numbers in polynomial time, a task that is believed to be intractable for classical computers [\cite{shor1994algorithms}]. In 1996, Lov Grover presented a quantum search algorithm that could search an unsorted database quadratically faster than classical algorithms [\cite{grover1996fast}].

%as classical computers seemed ill-suited for this task. 

%The quantum computing paradigm holds immense promise, as evidenced by several compelling results in computational complexity theory 

The quantum computing paradigm holds immense promise, as evidenced by this compelling result in computational complexity theory.  While hardware advancements have brought the scientific community closer to realizing this potential, the ultimate goal is yet to be accomplished. A \acrfull{nisq} computer equipped with 50-100 qubits may surpass the capabilities of current classical computers, yet the impact of quantum noise, such as decoherence in entangled states, imposes limitations on the size of quantum circuits that can be executed reliably \cite{preskill2018quantum}. Unfortunately, general-purpose error correction techniques \cite{calderbank1996good, gottesman1997stabilizer, steane1996error} consume a substantial number of qubits, making it difficult for \acrshort{nisq} devices to make use of them in the near term. For instance, the implementation of a single logical qubit may require between $10^3$ and $10^4$ physical qubits \cite{fowler2012surface}. As a result, it is unreasonable to expect that the idealized quantum algorithm will run perfectly on a quantum device, instead
only a mere approximation will be observed.

To reconcile quantum computation with \acrshort{nisq} computers, quantum compilers perform transformations for error mitigation \cite{wallman2016noise} and noise-adaptive optimization \cite{murali2019noise}. Additionally, current quantum computers only support a restricted, albeit universal, set of quantum operations. As a result, nonnative operations must be decomposed into sequences of native operations before execution \cite{harrow2002efficient}, \cite{burgholzer2020advanced}. In general, perfect computational universality is not sought, but only the ability to approximate any quantum algorithm, with a preference for minimizing the use of additional gates beyond the original requirements. The assessment of these compiler transformations necessitates a comparison of the error bounds between the source and compiled quantum programs. Additionally, in quantum information theory, it is essential to account for errors arising from malicious attacks or noisy channels \cite{watrous2018theory}. 

This suggests the development of appropriate notions of approximate program equivalence, \textit {in lieu} of the classical program equivalence and underlying theories that typically hinge on the idea that equivalence is binary, \textit{i.e.} two programs are either equivalent or they are not \cite{winskel1993formal}.

%Furthermore, in quantum information theory, the concept of an $\epsilon-\text{approximation}$ channel is fundamental when studying quantum teleportation via noisy channels \cite{watrous2018theory}. 

As previously noted, Shor's algorithm has played a pivotal role in sparking heightened interest within the scientific community toward quantum computing research. Several quantum programming languages have surfaced over the past 25 years \cite{zhao2020quantum,serrano2022quantum}. These include imperative languages such as Qiskit \cite{Qiskit} and Silq \cite{bichsel2020silq}, as well as functional languages such as Quipper \cite{green2013quipper} and Q\# \cite{svore2018q}. On one hand, the design of quantum programming languages is strongly oriented towards implementing quantum algorithms. On the other hand, the  definition of functional paradigmatic languages or functional calculi serves as a valuable tool for delving into theoretical aspects of quantum computing, particularly exploring the foundational basis of quantum computation \cite{zorzi2016quantum}. 

%Given the nature of this work, the focus will be on quantum languages designed with this latter aspect in mind. 
%QPL, a quantum language within the functional programming paradigm, marks a significant milestone in this context \cite{selinger2004towards}. It is a first-order functional language with a static type system that integrates classical control and quantum data, and its denotational semantics is based on superoperators. 

Most of the current research on algorithms and programming languages assumes that addressing the challenge of noise during program execution will be resolved either by the hardware or through the implementation of fault-tolerant protocols designed independently of any specific application \cite{chong2017programming}. As previously stated, this assumption is not realistic in the \acrshort{nisq} era. Nonetheless, there have been efforts to address the challenge of approximate program equivalence in the quantum setting. 

\cite{hung2019quantitative} and \cite{tao2021gleipnir} reason about the issue of noise in a quantum while-language by developing a deductive system to determine how similar a quantum program is from its idealised, noise-free version. The former introduces the ($Q$,$\lambda$)-diamond norm which analyzes the output error given that the input quantum state satisfies some quantum predicate $Q$ to degree $\lambda$. However, it does not specify any practical method for obtaining non-trivial quantum predicates. In fact, the methods used in [\cite{hung2019quantitative}] cannot produce any post conditions other than $(I,0)$ (\textit{i.e.}, the identity matrix $I$ to degree 0, analogous to a ``true” predicate) for large quantum programs. The latter specifically addresses and delves into this aspect.  

An alternative approach was explored in \cite{dahlqvist2022syntactic}, using linear $\lambda$-calculus as basis – \textit{i.e} programs are written as linear $\lambda$-terms – which has deep connections to both logic and category theory \cite{girard1995advances}, \cite{benton1994mixed}. A notion of approximate equivalence is then
integrated in the calculus via the so-called diamond norm , which induces a metric (roughly, a distance function) on the space of quantum programs (seen semantically as completely positive trace-preserving super-operators) [\cite{watrous2018theory}]. The authors argue that their deductive system allows to compute an approximate distance between two quantum programs easily as opposed to computing an exact distance ``semantically" which tends to involve quite complex operators.  Some positive results were achieved in this setting, but much remains to be done.

\section{Goals}
The notion of approximate equivalence for quantum programming explored in \cite{dahlqvist2022syntactic} does not take important operations into account. Specifically, the corresponding mathematical model does not include measurements, classical control flow, or discard operations. Also, the corresponding typing system is often times too strict and cannot properly handle multiple uses of the same resource, such as sampling exactly $n$-times from a distribution. The overarching goal of this M.Sc. project is to tackle the aforementioned limitations. A successful completion of this goal will provide a fully-fledged quantum programming language on which to study metric program equivalence in various scenarios. This includes not only quantum algorithmics – where, for example, the number of iterations in Grover’s algorithm involves approximations – but also quantum information theory, where, for instance, quantum teleportation and the problem
of the discrimination of quantum states have important roles \cite{nielsen2010quantum}.


\section{Document Structure}
%Notação
%dizer que vamos acrescentando novas regras ao longo dos capitulos ao nosso lambda calculua -> construção incremental do sistema