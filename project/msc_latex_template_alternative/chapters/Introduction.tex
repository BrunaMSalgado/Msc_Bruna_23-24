\chapter{Introduction}



\section{Motivation and Context}




\subsection*{Some History}

\noindent \textbf{Hilbert’s Optimistic Vision of Mathematics}
In September 1928, David Hilbert presented his vision for the foundations of mathematics at the International Congress of Mathematicians in Bologna. He believed it would be possible to place mathematics on an absolutely secure foundation. This would mean that no matter how difficult a mathematical problem might be, one would only need to ``take up the pen, sit at the abacus, and calculate" \cite{ferreiraProblemaDecisaoMaquina}. The process would be entirely deterministic—requiring no intuition or creativity, only strict adherence to formal rules, like performing multiplication in decimal notation. Every problem would, in principle, be solvable by such mechanical procedures. Mathematics, would be both complete (able to answer every question) and consistent (free of contradictions). 

\noindent \textbf{ Gödel’s Incompleteness Theorems} However, this vision was shattered by Kurt Gödel’s Incompleteness Theorems (1931) \cite{Godel}, which showed that no set of mathematical rules powerful enough to handle basic arithmetic could ever be both complete (answering every question) and consistent (free of contradictions) at the same time. 

\noindent \textbf{ Gödel’s Completeness Theorems} 
Interestingly, in his doctoral thesis, Gödel proved a foundational result in logic: first-order predicate logic---a formal system used to express statements involving quantifiers like ``for all'' and ``there exists''---is \emph{complete}~\cite{godelVollstandigkeitAxiomeLogischen1930}. It is important to note that the term ``completeness'' here differs from its use in Gödel’s later Incompleteness Theorems.
Before exploring this notion of completeness, it is helpful to introduce a few core concepts. \emph{Syntax} refers to the formal symbols and inference rules used to construct well-formed statements, while \emph{semantics} concerns the meaning assigned to these statements through interpretations. A \emph{model} of a first-order system is a mathematical structure in which the axioms (or rules) hold true under a given interpretation.
With these notions in place, we can now turn to Gödel’s result, known as the \emph{Completeness Theorem}. This theorem states that  if a statement holds in every possible model of a theory, then it can also be \emph{syntactically} proven using the system’s formal rules. In other words, completeness is the property that all universally valid statements are provable within the system.  The converse also holds: any statement provable syntactically must hold true in all models. This is known as \emph{soundness} \cite{franzenGodelsTheorem2008}.
 


%The formal rules of logical reasoning used in a first-order theory have the property of being sound with respect to the notion of logical consequence. What this means is that anything that can be proved from a set of axioms using these rules of reasoning is also a logical consequence of the axioms in the sense defined.







\noindent \textbf{Entscheidungsproblem}
%we just intoduced two of the main points of focus of this thesis soundness and completeness - we will proced to introduce another the lambda calculus with has close ties with the notion of an effective method/computability fists formalized by hilbert as one of the thing that should hold in its optimistic vision of mathmatics 
Why is this relevant to the present dissertation? Remarkably, it was Alonzo Church—using $\lambda$-\textit{calculus}—who first addressed Hilbert’s \emph{Entscheidungsproblem} (German for ``decision problem''), a cornerstone in his  optimistic vision for mathematics\cite{hilbert1938}. This \emph{Entscheidungsproblem} sought an effective method (also called a mechanical procedure or algorithm) to determine the truth or falsity of any mathematical statement. A method or procedure is effective if:
\begin{enumerate}
    \item it can be described by a finite number of exact instructions;
    \item it produces the desired result after a finite number of steps (provided the instructions are followed without error);
    \item it can, in principle, be carried out by a human using only paper and pencil;
    \item it does not require any creativity or insight from the human.
\end{enumerate}
The algorithms that children learn to perform basic arithmetic operations are examples of effective procedures.

 In 1936, Alonzo Church published a solution to the \emph{Entscheidungsproblem}, proving that no such universal method exists for all mathematical statements~\cite{church1936}. Today, this result is often referred to as Church’s Theorem. In the same work, he introduced a mathematically precise definition of effective computability using what is now known as the $\lambda$-calculus: a function is computable if
 and only if it can be written as a lambda term. This calculus played an important role in functional programming, influencing the design of languages like LISP, Pascal, and GEDANKEN—many of which incorporate $\lambda$-calculus-inspired features, either explicitly or implicitly.Furthermore, Lambda calculus may be employed to prove properties of programming language (such as: a well-formed program will not crash)and as a tool in the construction of compilers \cite{jonesImplementationFunctionalProgramming}.


 At the same time, another individual was working on this problem independently—without prior knowledge of Church’s work. His name was Alan Turing. In science, credit typically goes to the first person to prove a result. Yet Turing’s 1937 paper, On Computable Numbers, with an Application to the Entscheidungsproblem, was groundbreaking in its own right. In it, he introduced the concept of a universal machine and demonstrated that any function computable by a human following a systematic procedure could also be computed by a Turing machine \cite{turingComputableNumbersApplication1937}. Moreover, Turing proved that his definition of computability aligned with Church’s.

%By Turing’s analysis, it became clear that the $\lambda$-calculus was sufficiently powerful to express all mechanically computable functions.

Nowadays, computers are ubiquitous household appliances, as commonplace as televisions or refrigerators. Yet there is  something astounding in the fact that the personal computers we use daily trace their roots to an idea born from questions about the foundations of mathematics: the principle of universality in computation. 

The idea that such important aspects of modern computer science emerged from foundational questions in mathematics is nothing short of extraordinary.




\subsection*{ $\lambda$-calculus}


%Our previous use of a  (firt-order) logic as a way to introduce soundness and completeness to the reader is not completely innocent. In fact, the lambda calculus not only allows us to express all computable functions, but also establishes a correspondence between logical proofs and programs. This is known as the \emph{Curry-Howard isomorphism} \cite{girardProofsTypes1989}. Moreover, the lambda calculus itsef comes equipped with an equation logic, \ie, a set of equs, and that is why the concepts of soundness and completeness arise in this context ...  lambda theorey...-> ver isto melhor com o prof Renato

Moreover, the $\lambda$-calculus includes a set of equations that clarify and simplify the meaning of operations. For instance, one such equation states that a function applying another function $f$ to an argument $x$ can be simplified to $f$ itself: $\lambda x.(f, x) = f$. When viewed as a programming language, the $\lambda$-calculus is typically equipped with a  semantics, \ie, its terms are assigned specific meanings. In this work, we are particularly interested in interpreting these terms as mathematical objects, especially those arising in category theory. If the aforementioned equations hold under a given interpretation, we refer to it as a \emph{model}.
%One way to study the lambda calculus is to give mathematical models of it, i.e.,to provide spaces in which lambda terms can be given meaning. Such models are  constructed using methods from algebra, partially ordered sets, topology,category theory, and other areas of mathematics

On another note, the lambda calculus not only allows us to express all computable functions, but also establishes a correspondence between logical proofs and programs. This is known as the \emph{Curry-Howard isomorphism} \cite{girardProofsTypes1989}.

% Categorical logic stuff


%categories&(simply-typed)lambda-calculus
%But why chosing a categorical interpretation? Well, firstly is has deep connections with $\lambda$-calculus. Actually we take this cue to make a detour and introduce the typed-lambda calculus, a version of lambda calculus where each term has an bject of syntatic nature associated know as type (and the onde used in this work). In a way types are a way of making sure that our programs make sence, given the untyped version allows  for instance , a function can be applied to itself, e.g., (λx. x x)(λx. x x), leading to infinite loops (non-termination) and applying a "boolean" to a "number", or a "function" to a "string" — there's no way to prevent nonsense. Simply typded $\lambda$-calculus is an example of a type-theory. And here comes the funny part: categories can themselves be viewed as type theories of a certain kind.  In thinking of a category as a type theory, the objects of a category are regarded as types (or sorts) and the arrows as mappings between the corresponding types. Roughly speaking, a category may be thought of a type theory shorn of its syntax ... lambert e tal
%categories & the Blind men and an elephant parabole




% Ângulo Interpretação categorica -> categorias e parabola do elefante; 

%here is a parable that originated in India thousands of years ago about six blind men and an elephant. Having never encountered an elephant before, each man touches a different part of the animal—the side, tusk, trunk, leg, ear, or tail—and draws a conclusion based solely on that limited experience. One describes it as a spear (the tusk), another as a snake (the trunk), and another as a fan (the ear). Each is convinced of his own interpretation and dismisses the others as incorrect. None of them realises that they are each experiencing only a part of the same elephant, and that their individual descriptions are incomplete.
%In some versions of the story, the men stop arguing, begin listening to one another, and collaborate to form a more accurate understanding of the whole elephant.
%Similarly, in category theory, we can think of each category as representing a part of the elephant---that is, as offering a particular perspective on computer science. By applying category theory, we unify these perspectives, allowing us to generalize result across diverse computational paradigms.





% Categorias = multliplas perspetivas, mas não só. Lambda calculus tem laços mais profundos com teoria das categorias. Um destes laços mais "automatico" prende-se com uma restrição do lambda calculus conhecida como simply typed lambda calculus que é uma typed theory Church [1940]. Aqui os termos são assigned a type (ie objects of a syntactic nature that are assigned to lambda terms). In fact categories can themselves be viewed as type theories of a certain kind; this fact alone indicates that type theory is much more closely related to category theory than it is to set theory In thinking of a category as a type theory, the objects of a category are regarded as types (or sorts) and the arrows as mappings between the corresponding types. Roughly speaking, a category may be thought of a type theory shorn of its syntax . In the 1970s Lambek20 established that, viewed in this way, cartesian closed categories correspond to the typed λ-calculus, \ie any λ-theory (def lambda theory) T determines a Cartesian closed category C(T). Inversely, each Cartesian closed category E determines a λ-theory Th(E). Furtermore this connection extends to the logic: Each lambda calculus theory is the internal logical language of a cartesian closed ategory (the category's own structure defines the logic;The structure of the category (how objects and arrows interact) directly encodes the rules of logic or lambda calculus.)  Moreover, Lambek and Dana Scott independently observed that C-monoids, i.e., categories with products and exponentials and a single, nonterminal object correspond to the untyped λ-calculus. 


%A first thought, naturally, is that types resemble the syntactic or grammatical categories forming the basis of language in that the specification of types furnish the conditions for expressions to be well formed, or meaningful. The typing concept also arises in concrete situations. Consider, for example, an automotive toolkit. Here one is provided with nuts, bolts and wrench bits to attach nuts to bolts. This gives three types: N, B, and S. Then given the instruction Use to attach to , it is implicit that the blanks be filled in with (names of) components of types S, N, and B respectively. 

%A category C is an aggregate of abstract elements X, called the objects of thecategory, and abstract elements f, called mappings of the category
%. Category theory describes properties of mathematical structures via their transformations or morphisms.



%In deed there's a branch of logics known as categorical logic in which for a particular logical concept one seeks to identify what properties or extra structure are needed in an arbitrary category to interpret the concept in a way that respects given logical axioms and rules. This logic trancends the capacicied of setbased structures of classical model theory encoding of propositional logic to firstorder, higher-order and other logics.

%ver catl

%Such categorical semantics can provide a completely general and often quite simple formulation of what is required to model a theory in the logic This has proved useful in cases where more traditional settheoretic methods of de ning a notion of model either lack generality or are in conveniently complicated to describe or both




\subsection*{Going quantitative}

Beyond its foundational aspects, this calculus incorporates extensions for modeling side effects, including probabilistic or non-deterministic behaviors and shared memory. In this work, we are concerned with a version of $\lambda$-calculus that allows us to reason about approximate equivalence of programs, refered to as \emph{metric }$\lambda$\emph{-calculus}.

Program equivalence and its underlying theories traditionally rely on a binary notion of equivalence: two programs are either equivalent or not \cite{winskel93}. While this dichotomy is often sufficient for classical programming, it proves too coarse-grained for other computational paradigms. For instance, in various programming paradigms, interaction with the physical environment calls for notions of approximate program equivalence. 

To address this, \cite{dahlqvistInternalLanguage2022,dahlqvist2023syntactic} incorporate a notion of approximate equivalence into the equational system of the affine $\lambda$-calculus by introducing, among other elements, \emph{metric equations} \cite{mardare2016quantitative, mardare2017axiomatizability}. These are equations of the form $t =_{\varepsilon} s$, where $\varepsilon$ is a non-negative real number representing the ``maximum distance'' between terms $t$ and $s$. Here we start investigating the incorporation of a metric equation for the case statements (\ie\ conditionals). Our motivation for it is highly practical: in trying to reason quantitatively about higher-order programs we often fell short when these involved conditionals.


Other works in the spirit of this dissertation include \cite{mardare2016quantitative, mardare2017axiomatizability, mio24, jurka24}, which explore (generalized) metric universal algebras. A universal algebra, in simple terms, is a set equipped with any number of operations, further defined  by axioms typically expressed as identities or equational laws. In a (generalized) metric universal algebra, these axioms are relaxed into (generalized) metric equations rather than strict equalities. In the higher-order setting, \cite{lago22}, following the framework introduced by Mardare \cite{mardare2016quantitative}, investigates the problem of defining quantitative algebras that are capable of interpreting terms in higher-order calculi.
\todo[inline,size=\normalsize]{Falar do $\&$, não falor do $\&$, eis a questão }





\subsection*{Probabilistic Programming}

Probabilistic programs are quite  ubiquitous: they control autonomous systems, verify security protocols, and implement randomized algorithms for solving computationally intractable problems. At their core, they aim to democratize probabilistic modeling by providing programmers with expressive, high-level abstractions for machine learning and statistical reasoning \cite{bartheFoundationsProbabilisticProgramming2020}. In this context, concerns such as the development of more eco-friendly programs and algorithms could greatly benefit from a quantitative approach.

%Probabilistic programs offer a structured approach to drawing statistical conclusions from uncertain data or real-world observations. They generalize probabilistic graphical models beyond the capabilities of Bayesian networks and are expected to have broader applications in machine intelligence. These programs are are employed across various domains. They control autonomous systems, verify security protocols, and implement randomized algorithms for solving computationally intractable problems. As a result, they are becoming increasingly central to AI development. At their core, they aim to democratize probabilistic modeling by providing programmers with expressive, high-level abstractions for machine learning and statistical reasoning \cite{bartheFoundationsProbabilisticProgramming2020}. 






\subsection*{Quantum Computation}

In 1994, Peter Shor demonstrated that a quantum computer with sufficiently many qubits could pose a significant threat to the security of confidential data transmitted over the Internet \cite{shor1994algorithms}. This breakthrough spurred widespread interest in quantum computing. Nevertheless, \acrfull{nisq}  computers are expected to operate with severely limited hardware resources. Precisely controlling qubits in these systems comes at a high cost, is susceptible to errors, and faces scarcity challenges. Therefore, quantitative reasoning is indispensable for the design, optimization, and assessment of NISQ computing. 


%While holding immense promise, this paradigm encounters challenges in the \acrfull{nisq} era, namely quantum decoherence which restricts the reliable execution of quantum circuits \cite{preskill2018quantum}. Consequently, it is unreasonable to expect that the idealized quantum algorithm will run perfectly on a quantum device, instead only a mere approximation will be observed, which suggests the development of appropriate notions of approximate program equivalence.





%falar do lambda calculus e da sua ambivalencia e mencionar que nos vamos focar no caso quantico

\section{Contributions}
We build on the work of \cite{dahlqvist2023syntactic} by introducing a metric equation for conditionals and proving both its soundness and completeness. Soundness ensures that if a metric equation $t =_{\varepsilon} s$ can be derived in the calculus, then the distance between the interpretations of $t$ and $s$ is at most $\varepsilon$. Completeness guarantees that if $\varepsilon$ is the maximal distance between the interpretations of two programs, then we can derive $t =_{\varepsilon} s$ in the calculus. 

We then show that the category of metric spaces and the cocompletion of a $ \catMet $-category $ C $ satisfy the requirements for a model suitable for reasoning about approximate equivalence using this equation.  Next, to illustrate the utility of the metric equation introduced, we present two simple examples: reasoning about approximate equivalence between boolean terms (\ie terms of type $ \typeI \oplus \typeI $) the extensionality of copairing.


Next, we explore two computational paradigms in greater detail: probabilistic and quantum computation. For the former, we prove that the category $ \catBan $ of Banach spaces and short maps constitutes a model in this setting, and use a random walk as an illustrative example. For the latter, we consider two categorical models: Selinger’s \( \catQ \), the category of quantum operationts (\ie, completely positive, trace-nonincreasing superoperators) \cite{selinger2004towards}, and Cho’s \( \WstarCPSUop \), the opposite category of W$^*$-algebras and normal, completely positive, subunital maps~\cite{choSemanticsQuantumProgramming2016}. We show that both are first-order models. Finally, we use quantum state discrimination, quantum teleportation, and quantum random walks as illustrative examples in this setting.


%for enriched symmetric monoidal closed categories with binary copr0ducts over metric spaces.


\section{Document Structure}

\Cref{ch:metriclambda} introduces (metric) $\lambda$-calculus along with its categorical interpretation, accompanied by the necessary notions from category theory used throughout the thesis. One advantage of working with a metric equational system is the ability to reason syntactically about approximate equivalence. We leverage this idea in an interlude on booleans (i.e., terms of type $\typeI \oplus \typeI$) to illustrate the usefulness of the classical equational system. In \Cref{ch:conditionals}, we introduce a metric equation for conditionals, prove its soundness and completeness, and present a few models in this setting, along with a few illustrative syntactic examples. Then, in \Cref{ch:pp} and \Cref{ch:qc}, we focus on reasoning about higher-order probabilistic programs and first-order quantum programs, respectively,  including the necessary background in each domain. The thesis concludes with directions for future work in \Cref{ch:future_work}.

Although this work uses knowlage across multiple areas, the author's engagement with them is mostly limited to the scope of this thesis.


