







%It should be noted that the concept of a norm, as well as the properties of some norms, are relevant here,  as the existence of a metric system implies that operators have a well-defined norm. 



%\section{Functional Analysis} \label{sec:funal}



 %Functional analysis can be viewed as the  branch of mathematics that studies how the properties of finite-dimensional vector spaces generalize in infinite dimensional spaces.

%As a result, 
%p.26/28




%p.57->Bruce
%p97/100 -> conway




%p110, p.85


%\begin{lemma}
  %In the finite dimensional setting the norm topology and the weak topology coincide.
%\end{lemma}




%p.243 infinite dimensional analysis 

%IMPORTANTE:Since a finite dimensional space has only one Hausdorff linear topology, the norm topology and the weak topology must be the same.



%\section{Probabilistic Programming/Measure theory} \label{sec:pp}
 






%\subsection{Probabilistic Programming and Measure Theory}
% livro probabilistic programming 

% Probabilistic programs describe recipes for inferring statistical conclusions from a complex mixture of uncertain data and real-world observations. They can represent probabilistic graphical models are expected to have a major impact on machine intelligence. Probabilistic programs are ubiquitous. They steer autonomous robots and self-driving cars, are key to describe security mechanisms, naturally code up randomised algorithms for solving NP-hard or even unsolvable problems, and are rapidly encroaching on AI. Probabilistic programming aims to make probabilistic modelling and machine learning accessible to the programmer.


% INTRO Medidas e espaços mensuraveis 



  


   







%\todo[inline,size=\normalsize]{Normas} 

% def de valor abs p.133 (2.4) measure_theoritic_probability_by_athreya__lahiri.

%Lebesgue decomposition p.360 - measure theory


%\section{C* and W*-Algebras} \label{sec:c*_w*}

%Proposition 5.13. theory of op algebras 1 Takesaki -> f \otimes g é cp se f e g são cp







\chapter{Metric Lambda Calculus} \label{ch:metriclambda}


 %Moreover, lambda calculus has been proven to be universal in the sense that any computable function can be represented as an expression within the language \cite{bernays1936alonzo} .  Higher-order functions form a pivotal abstraction in practical programming languages such as LISP, Scheme, ML, and Haskell.

%The metric lambda calculus incorporates a metric equational system, enabling reasoning about approximate program equivalence.

This chapter introduces the metric lambda calculus as presented in \cite{dahlqvist2023syntactic}, drawing also from \cite{mackieLanguageAutonomous1993,croleCategoriesTypes1994,selinger2013lecture}.  After some intuitions about (metric) lambda calculus, the chapter overviews its syntax, metric equational system, and interpretation. %Since the interpretation is categorical, the chapter also includes an exposition of the definitions and results from category theory, drawing from  \cite{yanofskyMonoidalCategoryTheory2024,barrCategoryTheoryComputing1990,maclane13}. 
Our presentation on lambda calculus will involve conditionals; and in this regard, we will include proofs of results that are folklore, but whose proof we could not find in the literature. Finally, we illustrate the usefulness of the (metric) equational system by using it as a bridge to connect a certain type to Boolean algebra.  
For a more detailed study of lambda calculus theory, the reader is referred to, \eg, \cite{barendregt1984lambda}. 

%It is worth noting that this chapter includes minor contributions such as the proofs of results on conditionals that are folklore, but whose proof we could not find in the literature and the preliminary study about Booleans.

 
%We then illustrate the use of the language for describing quantum and probabilistic programs.
%For structural reasons, soundness and completeness with respect to the ``traditional'' equational system are discussed in this chapter, while the corresponding notions for the metric $\lambda$-theory are addressed in the following chapter. 



\section{A first look at lambda Calculus}


%The Lambda Calculus, developed by Church and Curry in the 1930s, serves as a formal language capturing the key attribute of higher-order functional languages, treating functions as first-class citizens, allowing them to be passed as arguments. 



The concept of a function emerges naturally in lambda calculus. But what exactly is a function?  In most mathematics, the ``functions as graphs'' paradigm is the most elegant and appropriate framework for understanding functions. Within this paradigm, each function $f$ has a fixed domain $X$ and a fixed codomain $Y$. The function $f$ is then a subset of $X \times Y$ that satisfies the property that for each $x \in X$ there is a unique $y \in Y$ such that $(x,y) \in f$. Two functions $f$ and $g$ are equal if they yield the same output on each input, that is, if $f(x) = g(x)$ for all $x \in X$. This perspective is known as the \emph{extensional} view of functions, as it emphasizes that the only observable property of a function is how it maps inputs to outputs.

From a Computer Science perspective, this does not always suffice. We are typically just as concerned with how a function computes its result as we are with what it produces. For instance, consider sorting: every correct sorting algorithm produces the same output for a given input, from the simplest to the most sophisticated. Yet, entire books and research papers are devoted to analyzing different sorting techniques. Clearly, something important is being overlooked. The casual use of the term ``algorithm'' in that context is revealing: a function should be represented not by its graph, but by the rule or process that describes how its result is computed. This view gives rise to the notion of \emph{intensional} equality: two functions are intensionally equal if they are defined by (essentially) the same formula.

In the lambda calculus, functions are described explicitly as \emph{abstractions}. A function $f:x \mapsto f(x)$ is represented as $\lambda x.f(x)$.  Applying a function to an argument is done by juxtaposing the abstraction with its argument. For instance, given the function $f:x \mapsto x+1$, the term $f(2)$ is represented by $(\lambda x.x+1)(2)$.

A major limitation of this notation appears to be that we can only define unary functions, that is, we can introduce only one argument at a time. However, this is not a true restriction.  Suppose we have a binary function represented as an expression with formal arguments $ x $ and $ y $, say $f(x, y)$. It can be represented as  $g = \lambda y.\, (\lambda x.\, f(x, y))$.
This function $ g $ is equivalent to the original binary function $ f $, but it takes its arguments \emph{one at a time}. This idea, based on \emph{currying}, shows how functions of multiple arguments can be represented using only unary functions.

The expression of \emph{higher‑order functions}, functions whose inputs and/or outputs are themselves functions, in a simple manner, is another important feature of lambda calculus. For example, the composition operator $f,g \mapsto f \circ g$ is written as $\lambda f. \lambda g. \lambda x. f(g(x))$. Considering the functions $f:x \mapsto x^2$ and $g:x \mapsto x+1$, to compute $(f \circ g)(2)$ one writes $$(\lambda f. \lambda g. \lambda x. f(g(x)))(\lambda x.x^2)(\lambda x.x+1)(2).$$

As mentioned above,  within  the ``functions as rule'' paradigm, is not
always necessary to specify the domain and codomain of a function in advance. For instance, the identity function $f: x \mapsto x$, can have any set $X$ as its domain and codomain, provided that the domain and codomain are the same. In this case, one says that $f$ has type $X \rightarrow{} X$. This flexibility regarding domains and codomains enables operations on functions that are not possible in ordinary mathematics. For instance, if $f = \lambda x.x$ is the identity function, then one has that $f(x) = x$ for any $x$. In particular, by substituting $f$ for $x$, one obtains $f(f) = (\lambda x.x)(f) = f$. Note that the equation $f(f) = f$ is not valid in conventional mathematics, as it is not permissible, due to set-theoretic constraints, for a function to belong to its own domain.

However, this remarkable feature of the lambda calculus can also lead to complications. 
As previously mentioned, applying a function to itself, as in the term $(\lambda x.\,x\,x)(\lambda x.\,x\,x)$, can result in non-termination. 
The typed variant of the lambda calculus, known as the \emph{simply-typed lambda calculus}, addresses this issue by assigning a type to every expression. 
Here, a function may only be applied to an argument if the argument's type is the same as the function's expected domain. Consequently, terms such as $f(f)$ are not allowed, even if $f$ represents the identity function.



%In order to be able to manipulate lambda-terms more easily, one can associate a type to each lambda-term


\section{Syntax}


\subsection{(Raw)Terms}


The expressions of the lambda calculus are called lambda terms. In the simply-typed lambda calculus, each lambda term is assigned a type. The terms without the specification of a type are called \emph{raw lambda terms}. The grammar of raw lambda terms is given by the \acrfull{bnf} below.
\begin{equation*} \label{eq:grammarlambda}
\begin{split}
 v\hspace{10 pt} ::= \hspace{10 pt}& x \hspace{3 pt} \vert \hspace{3 pt} f(v_1, \ldots, v_n) \hspace{3 pt} \vert \hspace{3 pt} *  \hspace{3 pt} \vert \hspace{3 pt} (\lambda x. v )\hspace{3 pt} \vert \hspace{3 pt} v\,w \hspace{3 pt}  \vert \hspace{3 pt} v \otimes w \hspace{3 pt} \vert
 \\&    \text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} x \otimes y. w  \hspace{3 pt}  \vert \hspace{3 pt} v \hspace{3 pt} \text{ to } *.w \hspace{3 pt} \vert \hspace{3 pt} \text{dis}(v) \hspace{3 pt} \vert \hspace{3 pt} \text{inl} (v) \hspace{3 pt} \vert \hspace{3 pt} \text{inr} (v) \hspace{3 pt} \vert
 \\& \text{ case } v \,   \{\text{inl} (x) \Rightarrow w ; \, \text{inr} (y) \Rightarrow u\}
\end{split}
\end{equation*}
Note that this is an inductive definition.
Here $x$ ranges over an infinite set of variables, and $f \in \Sigma$, where  $\Sigma$ corresponds to a set of sorted operation symbols. The expression $f(v_1, \ldots, v_n)$ corresponds to the application of the function $f$ to the arguments $v_1, \ldots, v_n$. The symbol $*$ is the unit. The term $(\lambda x. v )$ is the lambda abstraction term, representing a function that takes an argument $x$ and returns the value $v$. The term $v \,w$ is the application term, which applies the function $v$ to the argument $w$.  The term $v \otimes w$ is the tensor product of $v$ and $w$. The term $\text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} x \otimes y. w$ is the pattern-matching construct that deconstructs a tensor product into components $x$ and $y$. The term $v \text{ to } *.w$ is used to discard a variable $v$ (of the unit type). The terms $\text{inl}(v)$ and $\text{inr}(v)$ represent the left and right injections of $v$, respectively. Intuitively, the case statement executes $w$ when $v$ is a left injection, and $u$ when $v$ is a right injection, and a ``mixture'' of both otherwise.

\begin{convention}
   \begin{itemize}
    \item Applications associate to the left; that is, the expression $vwu$ is interpreted as $(vu)u$. This convention is convenient when applying a function to multiple arguments: for example, $fxyz$ is read as $(((f x) y) z)$.
    \item The body of a lambda abstraction, as well as pattern matching and discarding constructs (\ie, the part after the dot), extends as far to the right as possible. For instance, $\lambda x.\,vw$ is interpreted as $\lambda x.\,(vw)$, not $(\lambda x.\,v)w$.
   \end{itemize}
\end{convention}

%evaluates $v$ and stores the value of the right and left components in variables $x$ and $y$, respectively, and then 


%Then, based on (the left and right injections of) these values, 


%These serve a similar role to the traditional boolean values ``true'' and ``false'', as illustrated by the case statement, which evaluates $v$ and stores the value of the right and left components in variables $x$ and $y$, respectively. Then, we use the left and right injections of  $x$ and $y$, respectively, as if they were ``true'' and ``false'', respectively, in the sense that we evaluate 

 %to $w$ when $v$ is a left injection, and to $u$ when $v$ is a right injection. 

%? The term in1M is simply an elementof the left component of A + B.
%The term (caseM ofxA ⇒ N |yB ⇒ P) is a case distinction: evaluate M of type A + B. The answer is either an element o 
%the left component A or of the right component B. In the first case, assign the answer to the variable x and evaluate N. In the second case, assign the answer to the variable y and evaluate P. Since both N and P are of type C, we get a final result of type C. 


\subsection{Free and Bound Variables}
An occurrence of a variable $x$ within a term of the form $\lambda x.v$ is referred to as a \emph{bound} variable.  Similarly, the variables $x$ and $y$ in the term $\text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} x \otimes y. w$ are also bound. A variable occurrence that is not bound is said to be \emph{free}. For example, in the term $\lambda x.xy$, the variable $y$ is free, whereas the variable $x$ is bound.  

The set of free variables of a term $v$ is denoted by \gls{fv}, and is defined inductively as follows:
\begin{equation*}
\begin{split}
&FV(x) = \{x\}, &&  FV(*) = \emptyset,  \\
&FV(f(v_1, \ldots, v_n)), = FV(v_1) \cup \ldots \cup FV(v_n)&& FV(\lambda x: \mathbb{A}. v) = FV(v) \backslash \{x\}, \\
&FV(v w) = FV(v) \cup FV(w), && FV(v \otimes w) = FV(v) \cup FV(w), \\
& FV(\text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} x \otimes y. w), = FV(v) \cup (FV(w)  \backslash \{x,y\}) &&  FV(\text{inl}_\typeB (v)) =  FV(\text{inr}_\typeA (v)) = FV(v)\\
&FV(v \text{ to } *.w) = FV(v) \cup FV(w)  &&   \\
\end{split}
\end{equation*}
\vspace{-18pt}
\vspace{1pt}
\begin{equation*}
  \begin{split}
& \hspace{-30pt} FV(\text{case } v \,   \{\text{inl}_{\typeB} (x) \Rightarrow w ; \, \text{inr}_{\typeA} (y) \Rightarrow u\}) = FV(v) \cup  (FV(w) \backslash \{x\}) \cup (FV(u)\backslash \{y\}).
\end{split}
\end{equation*}

\begin{comment}
\begin{equation*}
\begin{split}
FV(x) &= \{x\}, &  FV(*) &= \emptyset,  \\
FV(f(v_1, \ldots, v_n)), &= FV(v_1) \cup \ldots \cup FV(v_n)& FV(\lambda x: \mathbb{A}. v) &= FV(v) \backslash \{x\}, \\
FV(v w) &= FV(v) \cup FV(w), & FV(v \otimes w) &= FV(v) \cup FV(w), \\
FV(\text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} x \otimes y. w), &= FV(v) \cup (FV(w)  \backslash \{x,y\}) & FV(\text{dis}(v)) &= FV(v),\\
FV(v \text{ to } *.w) &= FV(v) \cup FV(w)  &  FV(\text{inl}_\typeB) &=  FV(v). \\
\end{split}
\end{equation*}
\end{comment}


\subsection{$\alpha$-equivalence}


 
A natural notion of equivalence should stem from the fact that terms that differ only in the names of their bound variables represent the same program. For instance, the functions $\lambda x.x $ and $\lambda y.y$ have the same input-output behavior, despite being represented by different lambda terms. The equivalence we are referring to is called $\alpha$\emph{-equivalence}.

\begin{definition} [$\alpha$-renaming]
  The $\alpha$-equivalence is an equivalence relation on lambda terms that is used to, among other things, rename bound variables (se will see that such is essential in defining crucial operations in lambda-calculus). To rename a variable $x$ as $y$ in a term $v$, denoted by $v\{y/x\}$, is to replace all occurrences of $x$ in $v$ by $y$. Two terms $v$ and $w$ are $\alpha$-equivalent, written $=_{\alpha}$, if one can be derived from the other by a series of changes of bound variables.
\end{definition}

\begin{convention}
  Terms are considered up to $\alpha$-equivalence from now on, \ie,  terms are treated as equal if they differ only by the renaming of bound variables.
\end{convention}

\subsection{Substitution}



The substitution of a variable $x$ for a term $w$ in a term $v$ is denoted by \gls{substitution}. It is only permitted to replace free variables. In this context, it is necessary to avoid the unintended binding of free variables. For example, consider terms $ v \triangleq \lambda x. \, yx$ and $ w \triangleq \lambda z. \, xz$. Note that $x$ is bounded in $v$ and free in $w$, Consequently, the term $v[w/y]$ is not the same as $  \lambda x. \,  (\lambda z. \, xz) x$.  The proper thing to do is to rename the bound variable \emph{before} the substitution:
\[v[w/y] = \lambda x'. \, yx' [w/y]=  \lambda x'. \,  (\lambda z. \, xz) x'. \]

Thus, the operation of substitution may require renaming bound variables. 
In such cases, it is preferable to select a \emph{fresh} variable---that is, 
a variable that has not yet been used---as the new name for the bound variable. 
The assumption that the set of variables is infinite ensures that a fresh variable is always available when needed.

\begin{definition}
Given terms $v$ and $w$, the substitution $ v[w/x]$ is defined below. 
\begin{align*}
   \hspace{-23pt}  x[w/x] &= w \\
   \hspace{-23pt}  y[w/x] &= y && \text{ if $x \neq y$} \\
  \hspace{-23pt}   *[w/x] &=   * \\
  \hspace{-23pt}  (\lambda x. v)[w/x] &=  \lambda x. v  \\
  \hspace{-23pt}  (\lambda y. v)[w/x] &=  \lambda y. v[w/x]  &&\text{ if $x \neq y$ and $y \notin FV(w)$}  \\
   \hspace{-23pt}  \lambda y. v[w/x] &=  \lambda y'. v\{y'/y\}[w/y] &&\text{ if $x \neq y$, $y \in FV(w)$,}  \\
   \hspace{-23pt}& && \text{and $y'$ is fresh}\\
   \hspace{-23pt} (v \, u) [w/x] &= v [w/x] \, u [w/x] \\
   \hspace{-23pt}  (f(v_1, \ldots, v_n))[w/x] & = f(v_1[w/x], \ldots, v_n[w/x]) \\
   \hspace{-23pt}  (v \otimes u)[w/x] & = (v[w/x] \otimes u[w/x]) \\
   \hspace{-23pt} (\text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} y \otimes z. u)[w/x] &= \text{pm} \hspace{3 pt} v[w/x] \hspace{3 pt} \text{to} \hspace{3 pt} y \otimes z. u [w/x]  &&   \text{if $y \notin FV(w)$, $z \notin FV(w)$} \\
   \hspace{-23pt} (\text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} y \otimes z. u)[w/x] &= \text{pm} \hspace{3 pt} v [w/x] \hspace{3 pt} \text{to} \hspace{3 pt} y' \otimes z.  &&  \text{if $y \in FV(w)$, $z \notin FV(w)$,  } \\
   \hspace{-23pt}  &  \hspace{16pt} u \{ y'/y\} [w/x] && \text{and $y'$ is fresh}\\
   \hspace{-23pt} (\text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} y \otimes z. u)[w/x] &= \text{pm} \hspace{3 pt} v [w/x] \hspace{3 pt} \text{to} \hspace{3 pt} y \otimes z'.   &&  \text{if $y \notin FV(w)$, $z \in FV(w)$,} \\
    \hspace{-23pt}& \hspace{16pt} u \{ z'/z\} [w/x] && \text{and $z'$ is fresh}\\
   \hspace{-23pt} (\text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} y \otimes z. u)[w/x] &= \text{pm} \hspace{3 pt} v [w/x] \hspace{3 pt} \text{to} \hspace{3 pt} y' \otimes z'. &&  \text{if $y \in FV(w)$, $z \in FV(w)$,} \\
   \hspace{-23pt}& \hspace{16pt} u \{ y'/y\} \{ z'/z\} [w/x] && \text{and $y',z'$ are fresh}\\
   \hspace{-23pt} (v \hspace{3 pt} \text{to} \hspace{3 pt} *.u)[w/x] &=  v[w/x] \hspace{3 pt} \text{to} \hspace{3 pt} *.u[w/x] && \\
    \hspace{-23pt}(\text{inl}(v))[w/x] &= \text{inl}(v[w/x])\\
  \hspace{-23pt}(\text{inr}(v))[w/x] &= \text{inr}(v[w/x])\\
  \hspace{-23pt} \text{case } v \,  
  \left\{
    \begin{aligned} 
    &\inl(y) \Rightarrow p; \\
    &\inr(z) \Rightarrow  q  \\ 
  \end{aligned}  
  \right\}[w/x] &=  \text{case } v [w/x]  \,  
  \left\{
    \begin{aligned} 
    &\inl(y) \Rightarrow  \\
    & p[w/x];\\
    &\inr(z) \Rightarrow  \\ 
    & q[w/x]
  \end{aligned}  
  \right\} && \text{if $y \notin FV(w)$, $z \notin FV(w)$}
\\ 
& (\ldots) \\
 \hspace{-23pt}     & \hspace{16pt}  \text{case } v [w/x]  && \\
   \hspace{-23pt} \text{case } v \left\{
    \begin{aligned} 
    &\inl(y) \Rightarrow p; \\
    &\inr(z) \Rightarrow  q  \\ 
  \end{aligned}  
  \right\}[w/x] &= \left\{ 
    \begin{aligned}
    &\inl(y') \Rightarrow  p\{y'/y\}
    \\
    &  \{z'/z\}[w/x]; \\
    &\inr(z') \Rightarrow q\{y'/y\} \\
      & \{z'/z\}[w/x]
  \end{aligned}
  \right\} && \begin{aligned}
   & \text{if $y \in FV(w)$, $z \in FV(w),$} \\ 
  & \text{and $y',z'$ are fresh}
  \end{aligned} 
\end{align*}
\end{definition}


\begin{comment}
  \hspace{-23pt}\text{In the next two cases, }\Gamma,x: \mathbb{A}  \in \text{Sf}(&\Gamma_1,\Gamma_2), \Gamma_1 \triangleright v \text{, } \Gamma_2, y: \mathbb{D}\triangleright a   \text{, and } \\
   & \hspace{-66pt} \Gamma_2, z: \mathbb{E}\triangleright b  \\
   \hspace{-23pt} \text{case } v \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeI}(y) \Rightarrow a; \\
    &\inr_{\typeI}(z) \Rightarrow  b  \\ 
  \end{aligned}  
  \right\}[w/x] &=  \text{case } v [w/x]  \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeI}(y) \Rightarrow a; \\
    &\inr_{\typeI}(z) \Rightarrow  b  \\ 
  \end{aligned}  
  \right\}  &  (\text{ if } x: \mathbb{A} \in \Gamma_1) \\
  \text{case } v \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeE}(y) \Rightarrow a; \\
    &\inr_{\typeD}(z) \Rightarrow  b  \\ 
  \end{aligned}  
  \right\}[w/x] &=  \text{case } v   \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeE}(y) \Rightarrow a [w/x]; \\
    &\inr_{\typeD}(z) \Rightarrow  b [w/x]  \\ 
  \end{aligned}  
  \right\}  &  (\text{ if } x: \mathbb{A} \in \Gamma_2) \\
     \end{comment}

%The sequential substitutions $v[w_1/x_1] \ldots [w_n/x_n]$ are writen as $v[w_1/x_1, \ldots ,w_n/x_n]$.


\subsection{Type system}

As previously mentioned, this work focuses on the simply-typed lambda calculus, where each lambda term is assigned a \emph{type}. Unlike sets, types are \emph{syntactic} objects, meaning they can be discussed independently of their elements. One can conceptualize types as names or labels for a set. Let $G$ represent a set of ground types. The BNF grammar of types for affine lambda calculus is as follows:
\begin{equation*} \label{eq:grammartypes}
\centering
\hspace{95pt} \mathbb{A} ::= X \in G \hspace{3 pt} \vert \hspace{3 pt} \mathbb{I}  \hspace{3 pt}  \vert \hspace{3 pt} \mathbb{A}  \otimes  \mathbb{A} \hspace{3 pt}  \vert \hspace{3 pt} \mathbb{A}  \oplus  \mathbb{A} \hspace{3 pt} \vert  \hspace{3 pt}  \mathbb{A} \multimap  \mathbb{A}
\end{equation*}
 Ground types can be such things as booleans, integers, and so forth. The type $\mathbb{I}$ is the so-called unit. The type $\mathbb{A} \otimes \mathbb{A}$ corresponds to the tensor of two types. The type $\mathbb{A} \oplus \mathbb{A}$ can be seen as the coproduct/disjunction of two types. Finally, the type $\mathbb{A} \multimap \mathbb{A}$, for instance, in a set-theoretical perspective, can be seen as the type of functions from one type to another.


\subsection{Typing rules}

To prevent the formation of nonsensical terms within the context of lambda calculus, such as $(v \otimes w) (u)$, certain \emph{typing rules} are imposed.

Typing rules are formulated using \emph{typing judgments}. A typing judgment is an expression of the form $x_{1}: \mathbb{A}_{1}, \ldots, x_{n}: \mathbb{A}_{n} \hspace{1pt} \triangleright \hspace{1pt} v: \mathbb{A}$ (where $n \geq 1$), which asserts that the term $v$ is a well-typed term of type $\mathbb{A}$ under the assumption that each variable $x_{i}$ has type $\mathbb{A}_{i}$, for $1 \leq i \leq n$. The list $x_{1}: \mathbb{A}_{1}, \ldots, x_{n}: \mathbb{A}_{n}$ of typed variables is called the \emph{typing context} of the judgment, and it might be empty.  Each variable $x_i$ (where $1 \leq i \leq n$) must occur at most once in $x_1, \ldots, x_n$. Typing contexts are denoted by Greek letters \gls{typingcontexts}, and from now on, when referring to an abstract judgment, the notation \gls{judgement} will be employed.
The empty context is denoted by $-$. Note that in the linear lambda calculus, when different contexts appear sequenced (\eg $\Gamma, \Delta, \ldots$) they do not share variables amongst themselves. In other words, the typing system is linear: every variable is used exactly once.

There are certain typing rules that are not explicitly stated and whose validity follows from the existing rules of the system. These are called \emph{admissible rules}.
The concept of \emph{shuffling} is employed to construct a linear typing system that ensures the admissibility of the exchange rule (which allows reordering variables within the same context) and enables unambiguous reference to judgment's interpretation denoted $[\![ \Gamma \triangleright v: \mathbb{A} ]\!]$.  Shuffling is defined as a permutation of typed variables in a sequence of contexts, $\Gamma_1, \ldots, \Gamma_n$, preserving the relative order of variables within each $\Gamma_i$ \cite{shulman2019practical}. For instance, if $\Gamma_1=x:\mathbb{A}, y:\mathbb{B}$ and $\Gamma_2=z:\mathbb{D}$, then $z:\mathbb{D}, x:\mathbb{A}, y:\mathbb{B}$ is a valid shuffle of $\Gamma_1, \Gamma_2$. On the other hand, $y:\mathbb{B}, x:\mathbb{A}, z:\mathbb{D}$ is not a shuffle because it alters the occurrence order of $x$ and $y$ in $\Gamma_1$. The set of shuffles based on $\Gamma_1, \ldots, \Gamma_n$ is denoted as $\text{Sf} (\Gamma_1; \ldots; \Gamma_n)$. A valid typing derivation is constructed using the inductive rules shown in \autoref{fig:typing_rules_linear}.
%An admissible rule is not explicitly included in the formal definition of the type theory, but it can be proven valid using the fact that whenever one has derivations of its premises it is possible to construct a derivation of its conclusion.
%an admissible rule is one that is not asserted as part of the specification of the type theory, but for which we can prove after the fact that whenever we have derivations of its premises we can construct a derivation of its conclusion — usually by inductively traversing and modifying the given derivations of its premises. 
\begin{figure} [H]
  \small{
\begin{equation*}
\begin{split}
\begin{aligned}
& \hspace{-20pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \Gamma_{i} \triangleright v_{i}: \mathbb{A}_{i} \quad f: \mathbb{A}_{1}, \ldots, \mathbb{A}_{n} \xrightarrow{} \mathbb{A} \in \Sigma \quad E \in \text{Sf}(\Gamma_{1}; \ldots; \Gamma_{n})\\
    \hline
   E \triangleright f( v_{1},\ldots,v_{n}): \mathbb{A}
\end{array}
$
\end{minipage}
\hspace{147pt}
\text{(ax)} 
 \hspace{36pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
      \\
    \hline
   x:\mathbb{A} \triangleright x:\mathbb{A}
\end{array}
$ \end{minipage}
\hspace{-68pt} \text{(hyp)} \\
& \hspace{-20pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    \\
    \hline
   - \triangleright *: \mathbb{I}
\end{array}
$
\end{minipage}
\hspace{-87pt}
\text{($\mathbb{I}_{i}$)} 
 \hspace{90pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \Gamma \triangleright v: \mathbb{A} \otimes \mathbb{B} \quad  \Delta,x: \mathbb{A}, y: \mathbb{B}  \triangleright w: \mathbb{D}  \quad E \in \text{Sf}(\Gamma;\Delta)\\
    \hline
   E\triangleright \text{pm } v \text{ to } x \otimes y. w :\mathbb{D}
\end{array}
$ \end{minipage}
\hspace{117pt} (\otimes_{e}) \\
& \hspace{-20pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \Gamma \triangleright v: \mathbb{A} \quad  \Delta \triangleright w: \mathbb{B}  \quad E \in \text{Sf}(\Gamma;\Delta) \\
    \hline
   E \triangleright v \otimes w: \mathbb{A} \otimes \mathbb{B} 
\end{array}
$
\end{minipage}
\hspace{41pt} (\otimes_{i}) 
 \hspace{39pt}
 \begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \Gamma \triangleright v: \mathbb{I} \quad  \Delta \triangleright w: \mathbb{A}  \quad E \in \text{Sf}(\Gamma;\Delta)  \\
    \hline
   E \triangleright v \text { to } *.w: \mathbb{A}  
\end{array}
$ \end{minipage}
\hspace{38pt} (\mathbb{I}_{e}) \\
& \hspace{-20pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \Gamma,x:\mathbb{A} \triangleright v: \mathbb{B} \\
    \hline
   \Gamma \triangleright \lambda x:\mathbb{A} . \, v: \mathbb{A} \multimap \mathbb{B} 
\end{array}
$
\end{minipage}
\hspace{-27pt} (\multimap_{i}) 
 \hspace{66pt}
 \begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \Gamma \triangleright v: \mathbb{A} \multimap \mathbb{B} \quad  \Delta \triangleright w: \mathbb{A}  \quad E \in \text{Sf}(\Gamma;\Delta)  \\
    \hline
   E \triangleright v \, w: \mathbb{B}  
\end{array}
$ \end{minipage}
\hspace{67pt} (\multimap_{e}) \\
& \hspace{-20pt}
    %
    \begin{prooftree}
        \hypo{\Gamma \vljud v: \typeA}
        \infer1[(inl)]{\Gamma \vljud \inl_{\typeB}(v): \typeA \oplus \typeB}
    \end{prooftree}
    %
    \hspace{203pt}
    %
    \begin{prooftree}
        \hypo{\Gamma \vljud v: \typeB}
        \infer1[(inr)]{\Gamma \vljud \inr_{\typeA}(v): \typeA \oplus \typeB}
    \end{prooftree} 
    %
    \\[10pt]
    &\hspace{20pt}
    %
    \begin{prooftree}
        \hypo{\Gamma \vljud v: \typeA \oplus \typeB}
        \hypo{\Delta, x: \typeA \vljud w: \typeD}
        \hypo{\Delta, y: \typeB \vljud u: \typeD}
        \hypo{E \in \Shuff(\Gamma; \Delta)}
        \infer4[(case)]{E \vljud \text{case } v\,
        \{\inl_{\typeB}(x) 
            \Rightarrow w ; \,
          \inr_{\typeA}(y) \Rightarrow u
        \}: \typeD}
    \end{prooftree}
\end{aligned}
\end{split}
\end{equation*}
  }
\caption{Term formation rules of linear lambda calculus.}
\label{fig:typing_rules_linear}
\end{figure}
\begin{comment}
\todo[inline]{Tirar texto?}
The rule (ax) states that if there is a function $f \in \Sigma$ that has type $\mathbb{A}_1, \ldots, \mathbb{A}_n \rightarrow \mathbb{A}$ and a set of variables $v_1,\ldots, v_n$ whose types match the type of the arguments of $f$, then if that function is applied to $v_1,…,v_n$ the respective result is of type $\mathbb{A}$.
The rule (hyp) is a tautology: under the assumption that $x$ has type $\mathbb{A}$, $x$ has type $\mathbb{A}$. 
The rule ($\mathbb{I}_{i}$) asserts that the unit element $*$ always has type $\mathbb{I}$. 
The rule ($\multimap_i$) expresses that if $v$ is a term of type $\mathbb{B}$ with a variable $x$ of type $\mathbb{A}$, then $\lambda x:\mathbb{A} . v$ is a function of type $\mathbb{A} \multimap \mathbb{B} $. 
The rule $(\multimap_e)$ states that a function of type $\mathbb{A} \multimap \mathbb{B}$ can be applied to an argument of type $\mathbb{A}$ to produce a result of type $\mathbb{B}$. 
The rule $(\mathbb{\otimes}_i)$ asserts that if there is a term $v$ of type $\mathbb{A}$ and a term $w$ of type $\mathbb{B}$,  then the tensor of these terms is of type $\mathbb{A} \otimes \mathbb{B}$.
The rule $(\mathbb{\otimes}_e)$ expresses if there is a term $w$ of type $\mathbb{D}$ with variables $x$ and $y$ of types $\mathbb{A}$ and $\mathbb{B}$, respectively, and a term $v$ of type $\mathbb{A} \otimes \mathbb{B}$, then $v$ can be deconstructed into $x \otimes y$. 
The rule $(\mathbb{I}_e)$ states that if there is a term $w$ of type $\mathbb{A}$ and a term $v$ of type $\mathbb{I}$, then $v$ can be discarded, and only the term $w$ remains. 
The rule $(\text{inl}_\typeB)$ (resp. $(\text{inr}_\typeA)$) states that if we inject a term $v$ of type $\typeA$ (resp. $\typeB$) into $\typeA \oplus \typeB$ we obtain a term of type $\typeA$ or $\typeB$. 
Finally, the rule $\text{(case)}$ states that if there are two programs of type $\typeD$ to be executed depending on the value of a term $v$ of type $\typeA \oplus \typeB$ (whose right and left components are assigned to variables $x$ and $y$, respectively), then the resulting program also has type $\typeD$.
\end{comment}
 



A few straightforward programming examples are provided for a better understanding of the rules.  

\begin{example} \label{example:prog_swap}

For instance, the program that swaps the elements of a tensor product can be written as follows:
\begin{comment}
\begin{equation*}
\begin{split}
 \textbf{SwapTensor} \triangleq & - \triangleright \lambda x: \mathbb{A \otimes B}. \text{pm} \hspace{3 pt} x\hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a : \mathbb{B} \otimes \mathbb{A}
\end{split}
\end{equation*}
Now, to prove that this program is well-typed one can write the following typing derivation:
\begin{equation*}
\begin{split}
1 \hspace{10 pt} & x : \mathbb{A} \triangleright x : \mathbb{A} \otimes \mathbb{B}   \hspace{10 pt} & {(\text{hyp})} \\
2 \hspace{10 pt} &  b : \mathbb{B} \triangleright   b : \mathbb{B} \hspace{10 pt}&{(\text{hyp})} \\
3 \hspace{10 pt} &   a : \mathbb{A} \triangleright  a : \mathbb{A} \hspace{10 pt}&{(\text{hyp})} \\
4 \hspace{10 pt} &   b : \mathbb{B},a : \mathbb{A} \triangleright b \otimes a : \mathbb{B} \otimes \mathbb{A} \hspace{10pt} &\text{($2,3,\otimes_i$)} \\
5\hspace{10 pt}& x : \mathbb{A} \otimes \typeB \triangleright \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a : \mathbb{B} \otimes \mathbb{A}& \hspace{10pt} \text{($4,\otimes_e$)} \\
6 \hspace{10 pt}& - \triangleright \lambda x: \mathbb{A \otimes B}. \text{pm} \hspace{3 pt} x\hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a : \mathbb{B} \otimes \mathbb{A} & \text{($5,\multimap_i$)}
\end{split}
\end{equation*}
\end{comment}
\begin{equation*}
\begin{split}
&  \textbf{SwapTensor} \triangleq x : \mathbb{A} \otimes  \mathbb{B}  \triangleright \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a : \mathbb{B} \otimes \mathbb{A}
\end{split}
\end{equation*}
Now, to prove that this program is well-typed one can write the following typing derivation:
\begin{equation*}
\begin{split}
1 \hspace{10 pt} & x : \mathbb{A} \otimes \mathbb{B} \triangleright   y : \mathbb{A} \otimes \mathbb{B} \hspace{10 pt} & {(\text{hyp})} \\
2 \hspace{10 pt} &  b : \mathbb{B} \triangleright   b : \mathbb{B} \hspace{10 pt}&{(\text{hyp})} \\
3 \hspace{10 pt} &   a : \mathbb{A} \triangleright  a : \mathbb{A} \hspace{10 pt}&{(\text{hyp})} \\
4 \hspace{10 pt} &   b : \mathbb{B},a : \mathbb{A} \triangleright b \otimes a : \mathbb{B} \otimes \mathbb{A} \hspace{10pt} &\text{($2,3,\otimes_i$)} \\
5\hspace{10 pt}& x : \mathbb{A} \otimes \mathbb{B}  \triangleright \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a : \mathbb{B} \otimes \mathbb{A}& \hspace{10pt} \text{($1,4,\otimes_e$)}
\end{split}
\end{equation*}


Observe that in the notation of the third column, the numbers correspond to the premises utilized in the application of the rule.
\end{example}

\begin{example} \label{example:prog_Dis2nd}
Another example is the function that recieves a tensor product of type $\typeI \otimes \typeI$ and returns first element, discarding the second:
\begin{equation*}
\begin{split}
& \textbf{Dis2nd} \triangleq - \triangleright \lambda x: \mathbb{I \otimes I}. \, \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1 pt} b \hspace{3 pt} \text{ to } *. \,a:\mathbb{I}
\end{split}
\end{equation*}
To prove that this program is well-typed one can write the following typing derivation:
\begin{equation*}
\begin{split}
1  \hspace{10 pt} & b : \mathbb{I} \triangleright b : \mathbb{I}  \hspace{10 pt} & {(\text{hyp})} \\
2 \hspace{10 pt} & a : \mathbb{I} \triangleright a : \mathbb{I}  \hspace{10 pt} & {(\text{hyp})} \\
3 \hspace{10 pt} &  a : \mathbb{I}, b : \mathbb{I}  \triangleright b \hspace{3 pt} \text{ to } *.\,a: \typeI  & {(1,2,\mathbb{I}_{e})} \\
4 \hspace{10 pt} & x : \mathbb{I \otimes I} \triangleright x : \mathbb{I \otimes I}  \hspace{10 pt} & {(\text{hyp})} \\
5 \hspace{10 pt} & x : \mathbb{I \otimes I} \triangleright \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1 pt} b \hspace{3 pt} \text{ to } *.\,a : \mathbb{I} \hspace{10pt} & \text{($3,4,\otimes_{e}$)} \\
6 \hspace{10 pt} & - \triangleright \lambda x: \mathbb{I \otimes I}. \, \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1 pt} b \hspace{3 pt} \text{ to } *.\,a: \mathbb{I} \hspace{10pt} & \text{($5,\multimap_i$)}
\end{split}
\end{equation*}
\end{example}

\begin{example}
Next, consider the following program which can be seen as the distributive property:
\begin{equation*}
  \begin{split}
    z:(\typeA \oplus \mathbb{\typeB}) \otimes \mathbb{\typeD} \vljud \text{pm } z \text{ to } x \otimes d. \text{ case } x\,\left\{ 
      \begin{aligned}
    &\inl_{\typeB}(a) \Rightarrow \inl_{\typeB \otimes \typeD}(a \otimes d); \\
    & \inr_{\typeA}(b) \Rightarrow \inr_{\typeA \otimes \typeD} (b \otimes d)
      \end{aligned}
    \right\}: (\typeA \otimes \typeD) \oplus (\typeB \otimes \typeD)
\end{split}
\end{equation*}

To prove that this program is well-typed, we reason as follows:
\begin{equation*}
\begin{split}
1  \hspace{10 pt} & a : \mathbb{A}  \triangleright x : \mathbb{A}  & {(\text{hyp})} \\
2  \hspace{10 pt} & d : \mathbb{D}  \triangleright x : \mathbb{D}  & {(\text{hyp})} \\
3  \hspace{10 pt} & d : \mathbb{D}, a : \mathbb{A}  \triangleright \inl_{\typeB \otimes \typeD}(a \otimes d): (\typeA \otimes \typeD) \oplus (\typeB \otimes \typeD)  & {(1,2,\text{inl})} \\
4  \hspace{10 pt} & b : \mathbb{B}  \triangleright  b: \mathbb{B}  & {(\text{hyp})} \\
5  \hspace{10 pt} & d : \mathbb{D},b : \mathbb{B}  \triangleright \inr_{\typeA \otimes \typeD}(b \otimes d): (\typeA \otimes \typeD) \oplus (\typeB \otimes \typeD)  & {(2,4,\text{inr})} \\
6  \hspace{10 pt} & x : \mathbb{A} \oplus \mathbb{B} \triangleright x : \mathbb{A} \otimes \mathbb{B}  \hspace{10 pt} & {(\text{hyp})} \\ 
7 \hspace{10 pt} &    x: \mathbb{\typeA \oplus \typeB},  d: \mathbb{D} \triangleright \, 
\text{case } x \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeB}(a) \Rightarrow \inl_{\typeB \otimes \typeD}(a \otimes d); \\
    &\inr_{\typeB}(b) \Rightarrow \inr_{\typeA \otimes \typeD}(b \otimes d)   \\ 
  \end{aligned}  
  \right\}: (\typeA \otimes \typeD) \oplus (\typeB \otimes \typeD)  & \text{($6,3,5,\text{case}$)} \\
  8 \hspace{10 pt} & z : \mathbb{(A \oplus B) \otimes D} \triangleright z : \mathbb{(A \oplus B) \otimes D}  \hspace{10 pt} & {(\text{hyp})} \\
9 \hspace{10 pt} &  z:(\typeA \oplus \mathbb{\typeB}) \otimes \mathbb{\typeD} \vljud \text{pm } z \text{ to } x \otimes d. \text{ case } x\,\left\{ \ldots
    \right\}: (\typeA \otimes \typeD) \oplus (\typeB \otimes \typeD) & \text{($8,7,\otimes_{e}$)} \\
\end{split}
\end{equation*}


It should be noted that there are two distinct conventions for typing terms. One is the Church-style typing, in which all subterms are explicitly typed. This is the convention we adopt. The other is the Curry-style typing, where only the outermost term is assigned a type, and the types of subterms are left implicit.  For instance, consider the following Curry-style typing judgment:
\[
x : \typeA \triangleright (\lambda f.\,x)(\lambda y.\,y) : \typeA.
\]
Here, the variable $x$ has an explicitly assigned type, but the variable $y$ does not. Its type is not constrained and could be anything. The consequence is that a typed term alone does not uniquely determine its typing derivation.

\begin{convention}
  \begin{itemize}
    \item A judgment $\Gamma \triangleright v: \mathbb{A}$ will often be abbreviated into $\Gamma \triangleright v $ or even just $v$ when no ambiguities arise.
    \item The type annotations in terms $\Gamma \triangleright \lambda x:\mathbb{A} . \, v$, $\Gamma \triangleright \inl_{\typeB}(v)$ and $\Gamma \triangleright \inr_{\typeA}(v)$ will also often be ommited when no ambiguities arise.
  \end{itemize}
\end{convention}




\end{example}

\begin{comment}
\begin{example} \label{example:prog_Dis1stor2nd}
  Consider an analogous program to the previous example that that recieves a tensor product and returns
  second element, discarding the first: 
  \begin{equation*}
\begin{split}
& \textbf{Dis1st} \triangleq - \triangleright \lambda x: \mathbb{A \otimes A}. \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1 pt} \text{dis}(a) \hspace{3 pt} \text{ to } *.b: \mathbb{A \otimes A} \multimap \mathbb{A}
\end{split}
\end{equation*}
Following a similar line of reasoning as in the previous example, it is straightforward to verify that this $\lambda$-term is also well-typed.  
Now, let us examine a program that receives both a coproduct/sum and a tensor product, and applies either $\textbf{Dis1st}$ or $\textbf{Dis2nd}$ to the tensor product (or a mixture of both), depending on the value of the coproduct:
\begin{equation*}
\begin{split}
&  \textbf{Dis1stOR2nd} \triangleq - \triangleright  \lambda x: \mathbb{\typeB \oplus \typeB}. \, \lambda y: \mathbb{A \otimes A}. \, 
\text{case } x \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeB}(w) \Rightarrow \text{dis}(w) \text{ to} * . \, \textbf{Dis1st} \, y; \\
    &\inr_{\typeB}(z) \Rightarrow  \text{dis}(z) \text{ to} *. \textbf{Dis2nd} \, y   \\ 
  \end{aligned}  
  \right\}  & \\
\end{split}
\end{equation*}



To prove that this program is well-typed one can write the following typing derivation (the derivations regarding \textbf{Dis1st} and \textbf{Dis2nd} will be omitted):
\begin{equation*}
\begin{split}
1  \hspace{10 pt} & y : \mathbb{A} \otimes \mathbb{A}  \triangleright y : \mathbb{A}  \otimes \mathbb{A} \hspace{10 pt} & {(\text{hyp})} \\
2  \hspace{10 pt} & y : \mathbb{A}  \otimes \mathbb{A} \triangleright \textbf{Dis1st}   \, y : \mathbb{A} & {(\multimap_e)} \\
3  \hspace{10 pt} & y : \mathbb{A}  \otimes \mathbb{A}  \triangleright \textbf{Dis2nd}  \, y : \mathbb{A}  & {(\multimap_e)} \\
4  \hspace{10 pt} & w : \mathbb{B} \triangleright w : \mathbb{B}  \hspace{10 pt} & {(\text{hyp})} \\
5 \hspace{10 pt} & w : \mathbb{B} \triangleright \text{dis}(w): \mathbb{I} \hspace{10 pt} & {(4,\text{dis})} \\
6  \hspace{10 pt} & z : \mathbb{B} \triangleright z : \mathbb{B}  \hspace{10 pt} & {(\text{hyp})} \\
7 \hspace{10 pt} & z : \mathbb{B} \triangleright \text{dis}(z): \mathbb{I} \hspace{10 pt} & {(6,\text{dis})} \\
8 \hspace{10 pt} &  y : \mathbb{A}  \otimes \mathbb{A},  w : \mathbb{B}   \triangleright \text{dis}(w) \text{ to} * . \, \textbf{Dis1st} \, y : \typeA  & {(2,5,\mathbb{I}_{e})} \\
9 \hspace{10 pt} &   y : \mathbb{A}  \otimes \mathbb{A}, z : \mathbb{B}   \triangleright \text{dis}(z) \text{ to} * . \, \textbf{Dis2nd} \, y : \typeA & {(3,7,\mathbb{I}_{e})} \\
10 \hspace{10 pt} & x : \mathbb{B \oplus B} \triangleright x : \mathbb{B \oplus B}  \hspace{10 pt} & {(\text{hyp})} \\
11 \hspace{10 pt} &    x: \mathbb{\typeB \oplus \typeB},  y: \mathbb{A \otimes A} \triangleright \, 
\text{case } x \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeB}(w) \Rightarrow \text{dis}(w) \text{ to} * . \, \textbf{Dis1st} \, y; \\
    &\inr_{\typeB}(z) \Rightarrow  \text{dis}(z) \text{ to} *. \textbf{Dis2nd} \, y   \\ 
  \end{aligned}  
  \right\}  & \text{($8,9,10,\text{case}$)} \\
12 \hspace{10 pt} & x: \mathbb{\typeB \oplus \typeB} \triangleright \lambda y: \mathbb{A \otimes A}. \, 
\text{case } x \,  
  \left\{ \ldots 
  \right\}   & \text{($11,\multimap_i$)} \\
13 \hspace{10 pt} & - \triangleright  \lambda x: \mathbb{\typeB \oplus \typeB}. \, \lambda y: \mathbb{A \otimes A}. \, 
\text{case } x \,  
  \left\{ \ldots  
  \right\}   & \text{($12,\multimap_i$)}
\end{split}
\end{equation*}
\end{example}
\end{comment}






\subsection{Properties}


The calculus defined in \autoref{fig:typing_rules_linear} possesses several desirable properties, which are listed below. Before detailing them, it is necessary to introduce some auxiliary notation. Given a context $\Gamma$, $te(\Gamma)$ denotes context $\Gamma$ with all types erased. The expression $\Gamma \simeq_{\pi} \Gamma'$ means  contexts $\Gamma$ is a permutation of context $\Gamma'$. This notation also applies to non-repetitive lists of untyped variables $te(\Gamma)$.


\begin{theorem} \label {theorem:unique_der}
   The lambda calculus defined by the rules of \autoref{fig:typing_rules_linear} has the following properties:
   \begin{enumerate}
     \item\label{perm} for all judgements $\Gamma \vljud v$ and $\Gamma'
             \vljud v$, te($\Gamma$) $\simeq_{\pi}$  te($\Gamma'$); 
     %
     \item\label{type} additionally if $\Gamma \vljud v: \typeA,
       \Gamma' \vljud v: \typeA'$, and $\Gamma \simeq_{\pi}
       \Gamma'$, then $\typeA$ must be equal to $\typeA'$;
     %
     \item\label{der} all judgements $\Gamma \vljud v:\typeA$ have a unique derivation.
\end{enumerate}
\end{theorem}

\begin{proof}
Since these properties are established in \cite[Theorem 2.3]{dahlqvistCompleteVEquationalSystem2023} for the lambda calculus without conditionals,  it suffices to consider the cases involving conditionals.
It follows in all three cases from induction over the length of judgement derivation
trees. 

Let us focus first on Property~\eqref{perm}. The case of the rules concerning
injections is direct. As for rule~$(\text{case})$ take two contexts $E$ and
$E'$ for the same conditional. According to this rule we obtain contexts
$\Gamma$, $\Gamma'$, $\Delta$, $\Delta'$ such that $E \in
\Shuff(\Gamma;\Delta)$ and $E' \in \Shuff(\Gamma';\Delta')$. It follows from
induction that  $\text{te}(\Gamma) \simeq_\pi \text{te}(\Gamma')$ and $\text{te}(\Delta) \simeq_\pi \text{te}(\Delta')$,
and the proof is then obtained from the sequence of equivalences,
\begin{align*}
        \text{te}(E) & \simeq_\pi \text{te}(\Gamma, \Delta) 
        \\
        & \simeq_\pi \text{te}(\Gamma', \Delta')
        \\
        & \simeq_\pi \text{te}(E')
\end{align*}
Concerning Property~\eqref{type}, the case of the rules concerning injections
is direct and the case of rule~$(\text{case})$ is a corollary of
Property~\eqref{perm}. Finally let us consider Property~\eqref{der}. Again the
case concerning injections is direct and we thus focus only on
rule~$(\text{case})$. According to this rule we obtain contexts $\Gamma$,
$\Gamma'$, $\Delta$, $\Delta'$ such that $E \in \Shuff(\Gamma;\Delta)$ and $E
\in \Shuff(\Gamma';\Delta')$. By an appeal to Property~\eqref{perm} we also
obtain $\Gamma \simeq_\pi \Gamma'$ and $\Delta \simeq_\pi \Delta'$, and thus
since shuffling preserves relative orders we obtain $\Gamma = \Gamma'$ and
$\Delta = \Delta'$. The proof then follows by induction.
\end{proof}


\begin{lemma}[Exchange and Substitution]
\label{lem:exh_and_sub} 
For every judgement $\Gamma,x: \typeA, y: \typeB, \Delta \, \vljud \, v: \typeD$ the
judgement $\Gamma, y:\typeB, x:\typeA, \Delta \, \vljud \, v:
\typeD$ is derivable. Not only this, given judgements  $\Gamma,x:\typeA \vljud
v: \typeB$ and $\Delta \vljud w: \typeA$ the judgement $\Gamma, \Delta \vljud
v[w/x]: \typeB$ is also derivable.
\end{lemma}

\begin{proof}
  Once again, these properties are established in \cite[Theorem 2.1]{dahlqvist2023syntactic} for the lambda calculus without conditionals, so it suffices to consider the cases involving conditionals.

  We start with the exchange property which follows by induction over the length
of derivation trees. The rules that involve injections are direct.  The rule
$(\text{case})$ calls for case distinction, more specifically we  distinguish
between the cases in which both variables ($x : \typeA$ and $y : \typeB$) are
in $\Gamma$, both are in $\Delta$, and otherwise. The first two cases follow
straightforwardly by induction and the definition of a shuffle. For the third
case consider a judgement $E_1,x : \typeA, y : \typeB, E_2 \vljud \text{case }
v\, \{\inl_{\typeF}(a) \Rightarrow w ; \, \inr_{\typeE}(b) \Rightarrow u \}:
\typeD$, and assume with no loss of generality that $\Gamma$ is of the form
$\Gamma_1, x : \typeA, \Gamma_2$ and $\Delta$ of the form $\Delta_1, y :
\typeB, \Delta_2$. The proof now follows directly from the implication,
\begin{align*}
        & E_1, x : \typeA, y : \typeB, E_2 \in \Shuff(\Gamma_1, x : \typeA, \Gamma_2 ; \,
        \Delta_1, y : \typeB, \Delta_2) \Longrightarrow  \\
        &
        E_1, y : \typeB, x : \typeA, E_2 \in \Shuff(\Gamma_1, x : \typeA, \Gamma_2 ; \,
        \Delta_1, y : \typeB, \Delta_2)
\end{align*}
(which holds by the definition of a shuffle).

Finally we now focus on the substitution rule which also follows by induction over the
length of judgement derivation trees. Again the cases involving the injections are direct,
and we thus only detail the proof of rule $(\text{case})$. Consider then
judgements $E,x : \typeA \vljud \text{case } v\, \{\inl_{\typeD}(a) \Rightarrow
w ; \, \inr_{\typeE}(b) \Rightarrow u \}: \typeB$ and
$Z \vljud t : \typeA$ with $E \in \Shuff(\Gamma; \Delta)$. According to the definition
of a shuffle either $\Gamma$ is of the form $\Gamma_1, x: \typeA$ or $\Delta$ is
of the form $\Delta_1, x : \typeA$. The first case follows directly and the second case
is a corollary of the exchange rule.
\end{proof}

\begin{convention}
  Given programs  $\textbf{A} \triangleq \Gamma, x:\typeA \vljud v:\typeB$ and $\textbf{B} \triangleq \Delta \vljud w:\typeA$, we will often abuse notation by writing $\Gamma,\Delta \triangleright \textbf{A} [\textbf{B}/x]:\typeB$ to mean $\Gamma,\Delta \triangleq v[w/x]:\typeB$. Variants such as simply writing $\textbf{A}[\textbf{B}/x]$ to refer to the term $v[w/x]$ will also be used.
\end{convention}

 


\subsection{Equations-in-context}

The simply typed lambda calculus is a formal language that captures operations like the application of a function to an argument and the elimination of variables. To express these operations, there is a set of equations which fall into two primary categories: the $\beta$\emph{-equations}, which intuitively perform operations and enforce the intended meaning of the term, and $\eta$\emph{-equations}, which simplify terms by exploiting extensionality. 
There is also a secondary class of equations known as \emph{commuting conversions}, which serve to disambiguate terms that, while equivalent, have different representations.
As a result, affine $\lambda$-calculus comes equipped with the so-called equations-in-context \gls{equation-in-context}, which are often abbreviated as $v = w : \mathbb{A}$, or simply $v = w$ when the type is clear from context. These equations are illustrated in \autoref{fig:equations-linear-lambda}.
\begin{figure}[H]
  \centering
  \begin{tabular}{ |ccccc| }
    \hline
$(\beta)$ &  $ (\lambda x : \mathbb{A}.$ $v) \, w = v[w/x] $ & &$(\eta)$ &  $ \lambda x : \mathbb{A} . \, (v \, x) = v $ \\
$(\beta_{\mathbb{I}_{e}})$ &   $  * \text { to } *.$ $v = v$ && $(\eta_{\mathbb{I}_{e}})$ & $v$ to $*$ . $w[* / z] = w[v / z]$  \\
$(\beta_{\otimes_{e}})$   &\multicolumn{4}{c|}{$ \hspace{-10pt}  \text{pm } v \otimes w$ to $x \otimes y.$ $u = u[v/x,w/y]$ }\\
$(\eta_{\otimes_{e}})$   &\multicolumn{4}{c|}{$\text{pm } v$ to $x \otimes y.$ $u[x \otimes y/z] = u[v/z] $}\\
 $(c_{\mathbb{I}_{e}})$  &\multicolumn{4}{c|}{ $ u[v \text{ to } \ast . w/z] = v \text{ to } \ast . u[w/z]$ }\\
$(c_{\otimes_{e}})$ & \multicolumn{4}{c|}{ $ u[$pm $v$ to $x \otimes y.$ $w/z] =$ pm $v$ to $x \otimes y.$ $u[w/z] $ }\\
$(\beta_{case}^{inl})$ &\multicolumn{4}{c|} {$\text{case } 
          \inl_{\typeB}(v)\, \{ \inl_{\typeB} (x) \Rightarrow w 
          ;\, \inr_{\typeA} (y) 
          \Rightarrow u\}= w[v/x]$}\\
$(\beta_{case}^{inr})$ &\multicolumn{4}{c|} {$\text{case } 
          \inl_{\typeB}(v)\, \{ \inl_{\typeB} (x) \Rightarrow w 
          ;\, \inr_{\typeA} (y) 
          \Rightarrow u\}= u[v/y]$}\\
$(\eta_{case})$ &\multicolumn{4}{c|} { $\text{case } v\, \{\text{inl}_{\typeB} (y) \Rightarrow w [ \text{inl}_{\typeB}(y)/x] ;\, \text{inr}_{\typeA} (z) \Rightarrow w [ \text{inr}_{\typeA}(z)/x]\} = w[v/x]$} \\
\hline
  \end{tabular}
\caption{Equations-in-context for linear lambda calculus}
\label{fig:equations-linear-lambda}
\end{figure}

It is evident that, for example, equation $(\beta)$ enforces the meaning of application in  $(\lambda x : \mathbb{A}.$ $v) \hspace{1pt} w $, which is interpreted as ``$v$ with $w$ in place of $x$".
On the other hand, the equation $(\eta)$ is a simplification rule exploring extensionality: it states that a function that applies another function $v$ to an argument $x$ can be simplified to the function $v$ itself. 
The remaining $\beta$ e $\eta$ equations follow similar reasoning.

The following example demonstrates how these equations can be used in practice.

\begin{example} \label{ex:eq_contex_gen}
 For instance, consider a program that receives a tensor of terms whose second component is $*$ and discards it. This program can be simplified to the term corresponding to the first component of the tensor. In other words, we will show that the $\lambda$-term
\[
 - \vljud \big(\lambda z : \typeA \otimes \typeI.\, \text{pm } z \text{ to } x \otimes y.\, y \text{ to } *.\, x\big)\, (v \otimes *) : \typeA
\]
can be simplified to $v : \typeA$.

Applying equation $\beta$ , we have:
\begin{equation*}
  \begin{split}
 - \triangleright \big(\lambda z : \typeA \otimes \typeI. \, \text{pm } z \text{ to } x \otimes y. \, y \text{ to } *.\, x \big) \, (v \otimes  *) 
&= \text{pm }  v \otimes \, * \text{ to } x \otimes y. \, y \text{ to } *.\, x  : \typeA. 
 \end{split}
\end{equation*}
Next, applying  equation $\beta_{\otimes_e}$, it follows:
\begin{align*}
\text{pm }  v \otimes \, * \text{ to } x \otimes y. \, y \text{ to } *.\, x  
=   * \text{ to } *.\, v: \typeA
\end{align*}
Finally, applying  equation $\beta_{\typeI_e}$, we have:
\begin{align*}
 * \text{ to } *.\, v = v: \typeA
\end{align*}

\end{example}


 


\begin{comment}
\begin{definition}
  Let \( S \) be a set. A \emph{relation} on \( S \) is a subset \( R \subseteq S \times S \). An ordered pair \( (s_1, s_2) \in R \) means that \( s_1 \) is related to \( s_2 \).
\end{definition}
\end{comment}

\begin{comment}
\begin{definition}
  A relation $\sim$ on a set \( S \) is an \emph{equivalence relation} if it is
\begin{itemize}
  \item reflexive: for all \( x \in S \), \( x \sim x \),
  \item symmetric: for all \( x, y \in S \), if \( x \sim y \) then \( y \sim x \), and
  \item transitive: for all \( x, y, z \in S \), if \( x \sim y \) and \( y \sim z \), then \( x \sim z \).
\end{itemize} 
\end{definition}

\begin{definition}
  Given an equivalence relation on a set \( S \), we can describe the so-called \emph{equivalence classes}. If \( s \in S \), then the \emph{equivalence class} of \( s \) is the set of all elements related to it:
\[
[s] = \{ r \in S \mid r \sim s \}.
\]
That is, \( [s] \) is the set of all elements that are considered “the same” as \( s \) under the relation \( \sim \). For a given set \( S \) and an equivalence relation \( \sim \) on \( S \), we define the \emph{quotient set}, denoted \( S /\sim \), whose elements are all the equivalence classes of elements in \( S \). 
Observe that the quotient mapping
\[
q : S \longrightarrow S/\!\sim,
\]
which takes an element \( s \in S \) to its equivalence class \( [s] \), has the property that a map \( f : X \to Y \) extends along \( q \),
\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=4em,column sep=7em,minimum width=2em]
  {
   S &  S/\!\sim   \\
      & P \\
  };
  \path[-stealth]
    (m-1-1) edge  node [above] {$q$} (m-1-2)
    (m-1-1) edge  node [above] {$f$} (m-2-2)
    (m-1-2) edge [dotted]  node [right] {} (m-2-2);
    ;
\end{tikzpicture}
\]
\end{definition}
 just in case $f$ respects the equivalence relation, in the sense that $s ~ p$ implies
 $f(s)=f(p)$.

%For instance, consider a set of cars \( S \). We can define an equivalence relation on \( S \) by grouping cars according to their colour. This results in subsets such as the set of blue cars, the set of red cars, the set of green cars, and so on — these subsets are the \emph{equivalence classes}. Moreover, the collection of all such equivalence classes forms a new set, called the \emph{quotient set}.

\noindent Here, a \emph{congruence} is an equivalence relation closed under:
\begin{itemize}
    \item Term constructors (abstraction, application, etc.),
    \item Substitution of free variables,
    \item Exchange of typing contexts.
\end{itemize}
\end{comment}





\begin{definition} \label{def:linear_lambda_theory}
  Consider a pair $(G, \Sigma)$, where $G$ is a set of ground types and $\Sigma$ is a set of sorted operation symbols. A \emph{ linear $\lambda$-theory} is a triple $((G, \Sigma), Ax)$, where $Ax$ is a set of equations-in-context over $\lambda$-terms constructed from $(G, \Sigma)$. The elements of $Ax$ are called the \emph{axioms} of the theory.
\end{definition}

 Let $Th(Ax)$ denote the smallest
 congruence  containing $Ax$, the equations presented in \autoref{fig:equations-linear-lambda}, and closed under exchange and substitution (\autoref{lem:exh_and_sub}). The elements of $Th(Ax)$ are called the \emph{theorems} of the theory.

 We will often denote the triple $((G, \Sigma), Ax)$ by $T$ when referring to a  linear $\lambda$-theory.

 For instance recall \autoref{ex:eq_contex_gen}: 
 $$ - \vljud \big(\lambda z : \typeI \otimes \typeA.\, \text{pm } z \text{ to } x \otimes y.\, x \text{ to } *.\, y\big)\, (* \otimes v)=  v : \typeA $$
 is a theorem.

\subsection{Interlude: Booleans - Part 1} \label{subsec:interlude_bool}

%The term dis(v) is the discard term, which is used to mark a term $v$ as discardable.


In this subsection, we illustrate the usefulness of the classical equational system by showing how it can be used to connect the type $\typeI \oplus \typeI$ to Boolean algebra. More precisely, we use the previously introduced calculus to write programs corresponding to Boolean operations such as conjunction, disjunction, and negation. We then use the extensionality of the copairing and the equations-in-context to demonstrate that these operations satisfy the properties required by Boolean algebra.

The type $\typeI \oplus \typeI$ can be used to represent truth-values, in which case $\text{True} = \inl(*)$, $\text{False} = \inr(*)$ \cite{selinger2013lecture}. We will use the equations in \autoref{fig:equations-linear-lambda} to demonstrate that they possess certain properties typical of Boolean algebras.

\subsubsection{Boolean operators}

As a part of our $\lambda$-theory we consider an operation $\text{dis}: \typeI \oplus \typeI \to \typeI$ which discards its input, accompanied by the following axiom which we will denote by $ax_\text{dis}$,
\[\text{dis}(v) = \text{case } v \left\{
    \begin{aligned} 
    &\inl_{\typeI}(x) \Rightarrow x ; \\
    &\inr_{\typeI}(y) \Rightarrow  y  \\ 
  \end{aligned}  
  \right\} . \]

Given variables $a : \typeI \oplus \typeI$ and $b : \typeI \oplus \typeI$, their conjunction and disjunction correspond to the following programs:

\begin{align*}
  & \hspace{-25pt}\textbf{Conjunction}\,(a,b) \triangleq a:\typeI \oplus \typeI, b:\typeI \oplus \typeI \vljud \text{case } a\,
\left\{
    \begin{aligned} 
    &\inl_{\typeI}(x) \Rightarrow x \text{ to} *. \, b ; \\
    &\inr_{\typeI}(y) \Rightarrow y  \text{ to} *. \,  \text{dis}(b) \text{ to} *. \inr_{\typeI}(*)  \\ 
  \end{aligned}  
  \right\}
\end{align*}

\begin{align*}
  & \hspace{-25pt}\textbf{Disjunction}\,(a,b) \triangleq a:\typeI \oplus \typeI, b:\typeI \oplus \typeI \vljud \text{case } a\,
\left\{
    \begin{aligned} 
    &\inl_{\typeI}(x) \Rightarrow  x\text{ to} *. \,  \text{dis}(b) \text{ to} *. \inl_{\typeI}(*) ; \\
    &\inr_{\typeI}(y) \Rightarrow  y \text{ to} *. \,  b  \\ 
  \end{aligned}  
  \right\}
\end{align*}

Moreover, negation can be expressed by the following program:
  \begin{align*}
  & \hspace{-25pt}\textbf{Negation}\,(a) \triangleq a:\typeI \oplus \typeI \vljud \text{case } a\,
\left\{
    \begin{aligned} 
    &\inl_{\typeI}(x) \Rightarrow \inr_{\typeI}(x) ; \\
    &\inr_{\typeI}(y) \Rightarrow  \inl_{\typeI}(y)  \\ 
  \end{aligned}  
  \right\}
\end{align*}

To simplify notation, given terms $\Gamma \vljud v:\typeI \oplus \typeI$ and $\Delta \vljud  w:\typeI \oplus \typeI$, we define:
\begin{equation}
  \begin{split}
    &\Gamma, \Delta  \vljud \textbf{Conjunction}\,(v,w) \triangleq \Gamma, \Delta  \vljud \textbf{Conjunction}\,(a,b) [v/a, w/b] \\
    & \Gamma, \Delta  \vljud \textbf{Disjunction}\,(v,w) \triangleq \Gamma, \Delta  \vljud \textbf{Disjunction}\,(a,b) [v/a, w/b] \\
     &\Gamma \vljud \textbf{Negation}\,(a) \triangleq \Gamma \vljud \textbf{Negation}\,(a) [v/a]
  \end{split}
\end{equation}

 The following property will enable us to verify that the programs we defined above satisfy the desired properties in a systematic and straightforward manner.



\subsubsection{Extensionality of the copairing}

\begin{proposition}
  A $\lambda$-abstraction that receives inputs of a disjunctive type is determined by what it does to inputs ``from the left and from the right'', \ie,
\[
\left\{
\begin{aligned}
&(\lambda x.v)\, \text{inl}(y) = (\lambda x.w)\, \text{inl}(y) \\
&(\lambda x.v)\, \text{inr}(z) =(\lambda x.w)\, \text{inr}(z)
\end{aligned}
\right.
\implies \lambda x.v = \lambda x.w
\]
\end{proposition}


\begin{proof}
  
Using the $\beta$-equation we have
\begin{equation} \label{eq:beta_red}
  \left\{
\begin{aligned}
&(\lambda x.v)\, \text{inl}(y) = (\lambda x.w)\, \text{inl}(y) \\
&(\lambda x.v)\, \text{inr}(z) =(\lambda x.w)\, \text{inr}(z)
\end{aligned}
\right.
\iff  \left\{
\begin{aligned}
&v\,[\text{inl}(y)/x]  = w\, [\text{inl}(y)/x] \\
&v\,[\text{inr}(z)/x]  = w\, [\text{inr}(z)/x] 
\end{aligned}
\right.
\end{equation}
Next, considering the equations above and  $\eta_{case}$ we reason as follows:
\begin{equation*}
\begin{split}
 v &= \text{case } x  
    \left\{ \begin{aligned}
    \inl(y) \Rightarrow v\,[\text{inl}(y)/x] ;  
    \inl(z) \Rightarrow v\,[\text{inr}(z)/x]
  \end{aligned}  \right\} & (\eta_{case})\\
  & = \text{case } x  
    \left\{ \begin{aligned}
    \inl(y) \Rightarrow w\,[\text{inl}(y)/x] ;  
    \inl(z) \Rightarrow w\,[\text{inr}(z)/x]
  \end{aligned}  \right\} = w & (\text{\autoref{eq:beta_red}},\eta_{case})
\end{split}
\end{equation*}
Finally, we derive $ \lambda x.v = \lambda x.w$ from the conjunture.
\end{proof}

\vspace{5pt}

\subsubsection{Properties}

Next, we will show that the programs we have defined verify certain properties of their namesake operations in Boolean algebra. Given we have just established the extensionality of the copairing, it follows that if the desired properties hold for the injections, then they also hold for any terms of type $\typeI \oplus \typeI$.

  \begin{lemma} \label{lemma:inl_neutral}
     $\inl(*)$ acts as the neutral element for conjunction, whereas $\inr(*)$ serves as the absorbing element, \ie,  for $ \Gamma \vljud v: \typeI \oplus \typeI$
     \[
      \left\{
      \begin{aligned}
      & \Gamma \vljud \textbf{Conjunction} \, (\inl(*),v) = v  \\
      & \Gamma \vljud \textbf{Conjunction} \, (\inr(*), v) = \text{dis}(v) \text{ to} *. \inr(*)
      \end{aligned}
      \right.
      \]
  \end{lemma}

  \begin{proof}
    These properties follow from the equations $\beta_{case}^{inl}$, $\beta_{case}^{inr}$, and $\beta_{\mathbb{I}_{e}}$.

    \begin{equation*}
    \begin{split}
      & \textbf{Conjunction} \, (\inl(*),v) \\
      & \triangleq  \text{case } \inl(*)\,
  \{\inl(x) \Rightarrow x \text{ to} *. \, v ;
  \, \inr(y) \Rightarrow y \text{ to} *. \,  \text{dis}(w') \text{ to} *. \inr(*)
  \} \\
  & = * \text{ to} *. \,v & (\beta_{case}^{inl}) \\
  & = v & (\beta_{\mathbb{I}_{e}}) 
    \end{split}
    \end{equation*}

    For the second equality, by the extensionality of the coparing it suffices to prove that 
     \[
      \left\{
      \begin{aligned}
      &   \textbf{Conjunction} \, (\inr(*), \inl(z)) = \text{dis}(\inl(z)) \text{ to} *. \inr(*) \\
      & \textbf{Conjunction} \, (\inr(*), \inr(z)) = \text{dis}(\inl(z)) \text{ to} *. \inr(*)
      \end{aligned}
      \right.
      \]

    For the first equation, we reason as follows:
    \begin{align*}
      & \hspace{-22pt} \textbf{Conjunction} \, (\inr(*), \inl(z)) \\
      & \hspace{-22pt}  \triangleq  \text{case } \inr(*)\,   \left\{\begin{aligned} 
 &\inl(x) \Rightarrow x \text{ to} *. \,  \inl(z) ; \\
  &\inr(y) \Rightarrow y \text{ to} *. \,  \text{dis}(\inl(z)) \text{ to} *. \inr(*) \end{aligned} 
  \right\} \\
  & \hspace{-22pt}  =  *  \text{ to} *. \,  \text{dis}(\inl(z)) \text{ to} *. \inr(*) & (\beta_{case}^{inr}) \\
  & \hspace{-22pt}  =   \text{dis}(\inl(z)) \text{ to} *. \inr(*) & (\beta_{\mathbb{I}_{e}}) \\
    \end{align*}
      The second equation is obtained through similar reasoning.
  \end{proof}


  Note that the idempotency property of conjunction, --- that is, $ \textbf{Conjunction} \, (\inl(*), \inl(*)) = \inl(*)$ and  $ \textbf{Conjunction} \, (\inr(*), \inr(*)) = \inr(*)$ ---   follows directly from the lemma above, and equations $ax_\text{dis}$, $\beta_{case}^{inr}$, and $\beta_{\mathbb{I}_{e}}$.


\begin{proposition} \label{lem:comm}
  %The conjunction, disjunction, and negation of Boolean values satisfy the standard properties of classical logic. In particular, conjunction and disjunction are commutative, associative, idempotent, and distributive with respect to each other. Negation follows the double negation property and De Morgan's laws.
  The conjunction of two terms is commutative , \ie, for $ \Gamma \vljud v: \typeI \oplus \typeI$ and $ \Delta \vljud w: \typeI \oplus \typeI$ 
 $$  \Gamma, \Delta \triangleright \textbf{Conjunction} \, (v,w) = \textbf{Conjunction} \, (w,v)  $$ 
\end{proposition}




\begin{proof}
 Once again, by the extensionality of the copairing, it suffices to prove the equality for the four base cases:
 \[
      \left\{
      \begin{aligned}
      &   \textbf{Conjunction} \, (\inl(c), \inl(d)) =  \textbf{Conjunction} \, (\inl(d), \inl(c))  \\
      &  \textbf{Conjunction} \, (\inl(c), \inr(d)) =  \textbf{Conjunction} \, (\inr(d), \inl(c))  \\
      & \textbf{Conjunction} \, (\inr(c), \inl(d)) =  \textbf{Conjunction} \, (\inl(d), \inr(c))  \\
      &   \textbf{Conjunction} \, (\inr(c), \inr(d)) =  \textbf{Conjunction} \, (\inr(d), \inr(c))  
      \end{aligned}
      \right.
      \]

These equalities follow from the equations 
\(\beta_{case}^{inl}\), \(\beta_{case}^{inr}\), \(\eta_{\mathbb{I}_e}\), and \(ax_\text{dis}\). 
We will explicitly prove the second equality below; the others follow by similar reasoning.
  \begin{align*}
     &   \textbf{Conjunction} \, (\inl(c), \inr(d)) \\
     &  \triangleq \text{case } \inl(c)\,
  \left\{
    \begin{aligned}
    &\inl(x) \Rightarrow x \text{ to} *. \, \inr(d) ;\\
  \,& \inr(y) \Rightarrow y \text{ to} *. \,  \text{dis}(\inr(d)) \text{ to} *. \inr(*)
  \end{aligned} 
  \right\} \\
  & = c \text{  to} *. \, \inr(*) &  (\beta_{case}^{inl}) \\
  & =  \text{dis}(\inl(c)) \text{ to} *. \inr(*) & (ax_\text{dis}, \beta_{case}^{inl}) \\
  &= d \text{  to} *. \, \text{dis}(\inl(c)) \text{ to} *. \inr(*) & (\eta_{\mathbb{I}_e}) \\
  & = \text{case } \inr(d)\, \left\{ \begin{aligned}
    &\inl(x) \Rightarrow x \text{ to} *. \, \inl(c) ;\\
  \,& \inr(y) \Rightarrow y \text{ to} *. \,  \text{dis}(\inl(c)) \text{ to} *. \inr(*)
  \end{aligned} \right\} & (\beta_{case}^{inr}) \\
  & \triangleq \textbf{Conjunction} \, (\inr(d), \inl(c))
\end{align*}
\end{proof}



%To address the double negation property and De~Morgan's laws, we begin by establishing a key equality known as the \emph{syntactic fusion law}.





\begin{lemma} \label{lem:dneg}
  The double negation of a term $v$ is equivalent to $v$ itself, \ie, for $ \Gamma \vljud v: \typeI \oplus \typeI$
  $$ \Gamma \vljud \textbf{Negation} (\textbf{Negation} (v) ) = v.$$
\end{lemma}

\begin{proof}
  By the extensionality of the copairing, it suffices to prove the equality for the base cases, \ie, the injections. This follows directly from the equations \(\beta_{case}^{inl}\) and \(\beta_{case}^{inr}\). Once again, we will prove one of the resulting equalities explicitly; the other follows by similar reasoning.
    \begin{equation*}
    \begin{split}
       &\textbf{Negation} (\textbf{Negation} (\inl(z)) )  \\
        & \triangleq   \textbf{Negation} ( \text{case } \inl(z) \,
    \{\inl(x) 
        \Rightarrow \inr(x) ; \,
      \inr(y) \Rightarrow \inl(y)
    \}) \\
    & = \textbf{Negation}(\inr(z)) & (\beta_{case}^{inl}) \\
    & = \inl(z) & (\beta_{case}^{inr})
    \end{split} 
    \end{equation*}
\end{proof}

\begin{lemma} \label{lem:dmorgan} 
  De Morgan's laws hold for terms  $v: \typeI \oplus \typeI$ and $w: \typeI \oplus \typeI$, \ie, for $ \Gamma \vljud v: \typeI \oplus \typeI$ and $ \Delta \vljud w: \typeI \oplus \typeI$,
  $$  \Gamma, \Delta \triangleright \textbf{Disjunction} (\textbf{Negation} (v) , \textbf{Negation} (w)) =  \textbf{Negation}(\textbf{Conjunction} (v, w)) $$ 
\end{lemma}

\begin{proof}
  Once again, by the extensionality of the copairing, it suffices to prove the equality for the four base cases. The corresponding equalities follow from the equations 
\(\beta_{case}^{inl}\), \(\beta_{case}^{inr}\) and \(c_{\mathbb{I}_e}\). 
We will explicitly prove one of the equalities below; the others follow by similar reasoning.
\begin{equation*}
  \begin{split}
  & \textbf{Disjunction} (\textbf{Negation} (\inl(c)), \textbf{Negation} (\inr(d))) \\ 
  & \triangleq \text{case }  \, 
    \text{case } \inl(c)  
    \left\{ \begin{aligned}
    & \inl(a) \Rightarrow \inr(a); \\
    & \inl(b) \Rightarrow \inl(b) 
  \end{aligned}  \right\}  
   \left\{ \begin{aligned}
    & \inl(x) \Rightarrow  \ldots ; \\
    & \inl(y) \Rightarrow  y \text{ to } *. \, \textbf{Negation} (\inr(d))
   \end{aligned} \right\} \\
   &  =  c \text{ to} *. \, \text{case } \inr(d)  
    \left\{ \begin{aligned}
    & \inl(x) \Rightarrow \inr(x); \\
    & \inl(y) \Rightarrow \inl(y) 
  \end{aligned}  \right\}   & (\beta_{case}^{inl},\beta_{case}^{inr}) \\
  & =  \text{case } c \text{ to} *. \inr(d)  
    \left\{ \begin{aligned}
    & \inl(x) \Rightarrow \inr(x); \\
    & \inl(y) \Rightarrow \inl(y) 
  \end{aligned}  \right\}    & (c_{\mathbb{I}_e})\\
  & = \text{case }  \text{case } \inl(c)\,
  \left\{
    \begin{aligned}
    &\inl(x) \Rightarrow x \text{ to} *. \, \inr(d) ;\\
  \,& \inr(y) \Rightarrow y \text{ to} *. \,  \text{dis}(\inr(d)) \text{ to} *. \inr(*)
  \end{aligned} 
  \right\} \left\{ \begin{aligned}
    &  \ldots; \\
    & \ldots
  \end{aligned}  \right\} & (\beta_{case}^{inl})\\
  & \triangleq \textbf{Negation}(\textbf{Conjunction} (\inl(c), \inr(d))) \\
  \end{split}
\end{equation*}

\end{proof}

The remaining properties such can be proven through similar reasoning. 


\subsection{Metric equational system}

\emph{Metric equations} \cite{mardare2016quantitative,mardare2017axiomatizability} are a strong candidate for reasoning about approximate program equivalence. These equations take the form of \gls{metric-equation}, meaning  terms $v$ and $w$ are  \emph{at most} at a distance $\varepsilon$ from each other. The metric equational system for linear lambda calculus is depicted in \autoref{fig:metric deductive system}. 
Note that the equations $\Gamma \, \triangleright \, v = w : \typeA$ in \autoref{fig:equations-linear-lambda}, which in this setting abbreviate $\Gamma \triangleright w =_0 v : \typeA$, are also part of the metric equational system.


\begin{figure} [H]
\begin{equation*}
\begin{split}
\begin{aligned}
&
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \\
    \hline
   v=_{0}v
\end{array}
$
\end{minipage}
\hspace{-90pt}
\text{(refl)} 
 \hspace{55pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    v=_{q}w \quad w=_{r}u  \\
    \hline
   v=_{q + r} u
\end{array}
$ \end{minipage}
\hspace{-40pt} \text{(trans)} 
\hspace{55pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    v=_{q}w \quad r\geq q  \\
    \hline
   v=_{r} w
\end{array}
$ \end{minipage}
\hspace{-50pt} \text{(weak)} \\
&
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    \forall r > q . \hspace{4pt} v=_{r} w \\
    \hline
   v=_{q}w
\end{array}
$
\end{minipage}
\hspace{-45pt}
\text{(arch)} 
 \hspace{50pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    \forall i \leq n. \hspace{4pt} v=_{q_i} w\\
    \hline
   v=_{\wedge q_i} w
\end{array}
$ \end{minipage}
\hspace{-40pt} \text{(join)} 
\hspace{58pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    v=_{q} w \\
    \hline
   w =_{q } v
\end{array}
$ \end{minipage}
\hspace{-88pt}\text{(sym)}   \\
&
\begin{minipage}[t]{0.3\textwidth}
  $\begin{array}{c}
      v=_{q} w \quad v'=_{r} w' \\
      \hline
     v \otimes v' =_{q + r} w \otimes w'
  \end{array}
  $ \end{minipage}
  \hspace{-14pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
   \forall i \leq n. \hspace{4pt} v_{i}=_{q_i} w_{i}\\
    \hline
   f(v_{1},...,v_{n})=_{\Sigma q_i} f(w_{1},...,,w_{n}) 
\end{array}
$
\end{minipage}
 \hspace{47pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    v=_{q} w  \\
    \hline
  \lambda x : \mathbb{A}. \hspace{4pt} v=_{q} \lambda x:\mathbb{A}. \hspace{4pt} w
\end{array}
$ \end{minipage}
\hspace{20pt}  \\
&
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    v=_{q} w \quad  v'=_{r} w'  \\
    \hline
   \text{pm} \hspace{4pt} v \hspace{4pt} \text{to} \hspace{4pt} x \otimes y. \hspace{4pt} v'=_{q + r}\text{pm} \hspace{4pt} w \hspace{4pt} \text{to} \hspace{4pt} x \otimes y .  \hspace{4pt} w'
\end{array}
$
\end{minipage}
\hspace{195pt}
\begin{minipage}[t]{0.3\textwidth}
  $\begin{array}{c}
      v=_{q} w \quad v'=_{r} w' \\
      \hline
     v \hspace{1pt} v' =_{q + r} w \hspace{1pt}  w'
  \end{array}
  $ \end{minipage}
 \\
 &
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
  \Gamma \triangleright v =_{q} w: \mathbb{A} \quad \Delta \in \text{perm}(\Gamma)\\
    \hline
   \Delta \triangleright v =_{q} w: \mathbb{A}
\end{array}
$
\end{minipage}
\hspace{36pt}
\begin{minipage}[t]{0.3\textwidth}
  $\begin{array}{c}
     v=_{q}w  \quad v'=_{r}w'\\
      \hline
      v \text { to } *.v' =_{q+r} w \text { to } *.w'
  \end{array}
  $ \end{minipage}
  \hspace{14pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    v =_{q} w \quad v'=_{r} w'    \\
    \hline
  v[v'/x]=_{q + r} w[w'/x]
\end{array}
$ \end{minipage}
\hspace{10pt}
\end{aligned}
\end{split}
\end{equation*}
\caption{Metric equational system}
\label{fig:metric deductive system}
\end{figure}

Here, $\text{perm} (\Gamma)$ denotes the set of possible permutations of context $\Gamma$. 
The rules (refl), (trans), and (sym)  generalize the properties of reflexivity, transitivity, and symmetry of equality. 
The rule (weak) asserts that if two terms are at most at a distance $q$ from each other, then they are also at most at a distance $r$ for any $r \geq q$. Rule (arch) states that if $v =_r w$ for all approximations $r$ of $q$, then it necessarily follows that $v =_q w$. 
The rule  (join) expresses that if several maximum distances between two terms are known, then one can safely assume the minimum of these distances. In particular, it is always the case that $v =_{\infty} w$. 
The rule that follows conveys that if the maximum distance between two terms $v$ and $w$ is $q$, and the maximum distance between terms $v'$ and $w'$ is $r$, then the maximum distance between the tensor products $v \otimes v'$ and $w \otimes w'$ is $q + r$, \ie, the distances compound additively. The remaining rules follow similar reasoning.

% dizer o que é perm


\begin{example} \label{ex:metric_eqs}
  To ilustrate the usefulness of these equations, consider the program $\textbf{SwapTensorf}$ that receives a tensor product, swaps its elements and then applies a function $f: \typeA \to \typeD \in \Sigma $ to the new second element of the tensor pair:
\begin{equation*}
\begin{split}
& \textbf{SwapTensorf} \triangleq x : \mathbb{A},  y : \mathbb{B} \triangleright \text{pm} \hspace{3 pt} x \otimes y\hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes f(a) : \mathbb{B} \otimes \mathbb{D} &
\end{split}
\end{equation*}
Let $ f^{\varepsilon} $ be an erroneous implementation of $ f $. The program above is thus rewritten as:
\begin{equation*}
  \begin{split}
  & \textbf{SwapTensorf}^{\varepsilon} \triangleq  x : \mathbb{A},  y : \mathbb{B} \triangleright \text{pm} \hspace{3 pt} x \otimes y\hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes f(a)^{\varepsilon}  : \mathbb{B} \otimes \mathbb{D} &
  \end{split}
  \end{equation*}
Consider we  have the axiom $f(a) =_{\varepsilon} f^{\varepsilon}(a)$. Then, it is possible to show that $\textbf{SwapTensorf} =_{\varepsilon} \textbf{SwapTensorf}^{\varepsilon} $ using our metric equational system. The proof is as follows. %The types and contexts are omitted for brevity as no ambiguity arises.
\begin{equation*}
  \begin{split}
  1 \hspace{10 pt} & f(a)^{\varepsilon} =_{\varepsilon} f(a) \\
  2 \hspace{10 pt} &  b =_{0} b & {(\text{refl})} \\
  3 \hspace{10 pt} & b \otimes f(a)^{\varepsilon}  =_{\varepsilon} b \otimes f(a)  & \text{($1,2,\otimes_i$)} \\
  4 \hspace{10 pt} &   x \otimes y =_{0}  x \otimes y  &{(\text{refl})}  \\
  5\hspace{10 pt}& \text{pm} \hspace{3 pt} x \otimes y\hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes f(a)^{\varepsilon} =_{\varepsilon} \text{pm} \hspace{3 pt} x \otimes y\hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes f(a) & \hspace{10pt} \text{($3,4,\otimes_e$)}
  \end{split}
  \end{equation*}
\end{example}


\begin{definition} \label{def:metric_lambda_theory}
  Consider a tuple \( (G, \Sigma) \), where \( G \) is a set of ground types and \( \Sigma \) is a set of sorted operation symbols of the form \( f : A_1, \ldots, A_n \to A \) with \( n \geq 1 \). A \emph{metric $\lambda$-theory} is a tuple \( ((G, \Sigma), Ax) \), where \( Ax \) is a set of \emph{metric equations-in-context} over  $\lambda$-terms constructed from \( (G, \Sigma) \).
\end{definition}

The elements of \( Ax \) are called the \emph{axioms} of the theory. Let \( Th(Ax) \) denote the smallest class that contains \( Ax \) and is closed under the rules presented in \autoref{fig:equations-linear-lambda} (i.e., the classical equational system) and \autoref{fig:metric deductive system}. The elements of \( Th(Ax) \) are called the \emph{theorems} of the theory.

For instance, in \autoref{ex:metric_eqs}, $\textbf{SwapTensorf} =_{\varepsilon} \textbf{SwapTensorf}^\varepsilon$ is a theorem.




\subsection{Interlude: Booleans - Part 2}

%Although we do not have an explicit equation for conditionals, we can stil  reason about approximate equivalence between them using the substitution rule.


We can now use the extended system to explore the booleans introduced in  \Cref{subsec:interlude_bool}. For instance, given axioms
$$ \Gamma \triangleright v =_{\varepsilon} v' \quad \text{and} \quad \Delta \triangleright w =_{\delta} w', $$
we can ask whether
$$ \textbf{Conjunction} [v, w] =_{\varepsilon + \delta}  \textbf{Conjunction} [v', w'] $$
holds, which indeed follows from a double application of the substitution rule. An analogous reasoning applies to $\textbf{Conjunction} [v, w]$ and $\textbf{Negation} [w]$.

These derivations we have just established are interesting, for they hint at ``quantitative laws'' for the boolean connectives. For example, previously we have established that
\[
      \left\{
      \begin{aligned}
      & \Gamma \vljud \textbf{Conjunction} \, (\inl(*),w) = w  \\
      & \Gamma \vljud \textbf{Conjunction} \, (\inr(*), w) = \text{dis}(w) \text{ to} *. \inr(*).
      \end{aligned}
      \right.
      \]

Now we can assume the existence of a truth value $- \triangleright v: \typeI \oplus \typeI$ between ``true'' and ``false'', \ie,
$\inr(*) =_{\epsilon} v  =_{\delta} \inl(*)$
and derive the quantitative laws
\[
      \left\{
      \begin{aligned}
      & \Gamma \vljud \textbf{Conjunction} \, (v,w) =_{\delta} w  \\
      & \Gamma \vljud \textbf{Conjunction} \, (v, w) =_{\epsilon} \text{dis}(w) \text{ to} *. \inr(*).
      \end{aligned}
      \right.
      \]


 

 










\section{Semantics} \label{sec: Lambda Calculus:Interpretation}


Up to this point, we have discussed $\lambda$-calculus in abstract terms: we explored which programs can be written, but we have not yet assigned them any meaning. This process—assigning meaning to syntactic expressions—is known as the \emph{interpretation} or \emph{semantics} of the language. In fact, the word ``semantics'' comes from the Greek word for ``meaning''.

There are different kinds of semantics, in particular, \emph{denotational semantics} interprets terms as mathematical objects. This is done by defining a function that maps syntactic entities (such as types and terms) to semantic entities (such as sets and functions). This mapping is called the \emph{interpretation function}, typically denoted by $\sem{-}$. Thus, given a term $v$, we write $\sem{v}$ to denote its meaning under a specific interpretation.

Naturally, this raises important questions: what guarantees that the interpretation of terms respects calculus's classical equations? This leads us to the notions of \emph{soundness} and \emph{completeness}.

With respect to a given class of interpretations:
\begin{itemize}
  \item \emph{Soundness} is the property 
  \[
  v = w \;\Rightarrow\; \llbracket v \rrbracket = \llbracket w \rrbracket
  \quad \text{for all interpretations in the class.}
  \]
  That is, if two terms are provably equal, then they are interpreted as equal.
  
  \item \emph{Completeness} is the property 
  \[
  \llbracket v \rrbracket = \llbracket w \rrbracket
  \;\Rightarrow\; v = w
  \quad \text{for all interpretations in the class.}
  \]
  That is, if two terms are interpreted as equal, then they are provably equal.
\end{itemize}

Soundness ensures that our equations are \emph{correct}—all derivable equations are semantically valid. Completeness ensures that our equations are \emph{sufficient}—we can derive all semantically valid equations.
We note that, in the case of the metric equations, the underlying idea is similar, although soundness and completeness are defined differently.

\vspace{20pt}

In order to define the interpretation of judgments $\Gamma \triangleright v: \mathbb{A}$, it is necessary to establish some notation first. Let $\catC$ be a symmetric monoidal closed category and $A$, $B$ and $C$ be objects of this category. 

Recall that in a closed monoidal category $\catC$, we have a natural isomorphism:
\[
\mathrm{Hom}_{\catC}(A \otimes B, C) \cong \mathrm{Hom}_{\catC}(A, B \multimap C).
\]
This isomorphism is known as \emph{currying}. For each morphism $f \colon A \otimes B \to C$, its \emph{curried form} $\overline{f} \colon A \to (B \multimap C)$ is the morphism corresponding to $f$ under this isomorphism.
The inverse operation, called \emph{application} or \emph{evaluation}, is given by the \emph{application morphism} $\text{app}_{B,C} \colon (B \multimap C) \otimes B \to C$.


For all ground types $X \in G$  the interpretation of $[\![X]\!]$  is postulated  to be an object of $\catC$. Types are interpreted inductively using the unit $\mathbb{I}$, the tensor $\otimes$, the coproduct $\oplus$, and the linear map $\multimap$. Given a non-empty context $\Gamma=\Gamma',x: \mathbb{A}$, its interpretation is defined by $[\![\Gamma',x: \mathbb{A}]\!] = [\![\Gamma']\!] \otimes [\![\mathbb{A}]\!]$ if $\Gamma'$ is non-empty and $[\![\Gamma',x: \mathbb{A}]\!] = [\![\mathbb{A}]\!]$ otherwise. The empty context $-$ is interpreted as $[\![-]\!] = \mathbb{I}$. Given $A_{1}, . . . ,A_{n} \in \catC$, the $n$-tensor $((A_1 \otimes A_2) \otimes \ldots ) \otimes A_{n}$ is denoted as $A_1 \otimes \ldots \otimes A_{n}$, and similarly for morphisms. 


\subsection{Semantics}

``Housekeeping" morphisms are employed to handle interactions between context interpretation and the symmetric monoidal struture of $\catC$. Given $\Gamma_{1}, \ldots, \Gamma_{n}$, the morphism that splits $[\![\Gamma_{1}, \ldots, \Gamma_{n}]\!]$ into $[\![\Gamma_{1}]\!] \otimes \ldots \otimes [\![\Gamma_{n}]\!]  $ is denoted by $\text{sp}_{\Gamma_1;\ldots;\Gamma_n}: [\![\Gamma_{1}, \ldots, \Gamma_{n}]\!] \xrightarrow{} [\![\Gamma_{1}]\!] \otimes \ldots \otimes [\![\Gamma_{n}]\!] $. For $n=1$, $\text{sp}_{\Gamma_1} = \id$.
Let $\Gamma_1$ and $\Gamma_2$ be two contexts, $\text{sp}_{\Gamma_1, \Gamma_2}: \sem{ \Gamma_1\otimes \Gamma_2} \rightarrow \sem{\Gamma_1}\otimes \sem{\Gamma_2}$ is defined as:
\begin{equation*}
  \text{sp}_{-; \Gamma_2} = \lambda^{-1} \hspace{1cm} \text{sp}_{\Gamma_1; -} = \rho^{-1} \hspace{1cm} \text{sp}_{\Gamma_1;x:\mathbb{A}} = \id \hspace{1cm} \text{sp}_{\Gamma_1; \Delta, x: \mathbb{A}} = \alpha \cdot (\text{sp}_{\Gamma_1; \Delta} \otimes \id)
\end{equation*}
For $n>2$, $\text{sp}_{\Gamma_1;\ldots;\Gamma_n}: \sem{\Gamma_1;\ldots;\Gamma_n} \to \sem{\Gamma_1} \otimes \ldots \otimes \sem{\Gamma_n} $ is is defined recursively based on the previous definition, using induction on $n$:
\begin{equation*}
  \text{sp}_{\Gamma_1;\ldots;\Gamma_n} = (\text{sp}_{\Gamma_1;\ldots;\Gamma_{n-1}} \otimes \id )\cdot \text{sp}_{\Gamma_1, \ldots, \Gamma_{n-1} ;\Gamma_n}
\end{equation*}
On the other hand, $\text{jn}_{\Gamma_1;\ldots;\Gamma_n}$ denotes the inverse of $\text{sp}_{\Gamma_1;\ldots;\Gamma_n}$. Next, given $\Gamma, x : \mathbb{A}, y : \mathbb{B},\Delta$, the morphism permuting $x$ and $y$ is denoted by $\text{exch}_{\Gamma, x : \mathbb{A}, y : \mathbb{B},\Delta}: [\![\Gamma,\underline{ x : \mathbb{A}, y : \mathbb{B}},\Delta]\!] \xrightarrow{} [\![\Gamma, y : \mathbb{B}, x : \mathbb{A}, \Delta]\!] $ and defined as:
\begin{equation*}
  \text{exch}_{\Gamma, \underline{ x : \mathbb{A}, y : \mathbb{B}},\Delta} = \text{jn}_{\Gamma; y:\mathbb{B}, x:\mathbb{A};\Delta} \cdot (\id \otimes \text{sw} \otimes \id ) \cdot \text{sp}_{\Gamma; x:\mathbb{A}, y:\mathbb{B};\Delta}
\end{equation*} 


The shuffling morphism $\text{sh}_{E}: [\![E]\!] \xrightarrow{} [\![\Gamma_1, \ldots, \Gamma_n ]\!]$ is defined as a suitable composition of exchange morphisms.


For every operation symbol $f: \mathbb{A}_{1}, \ldots, \mathbb{A}_{n} \xrightarrow{} \mathbb{A}$ it is assumed the existence of a morphism $[\![f]\!]: [\![\mathbb{A}_{1}]\!] \otimes \ldots \otimes [\![\mathbb{A}_{n}]\!] \xrightarrow{}  [\![\mathbb{A}]\!] $.
The interpretation of judgments is defined by induction over derivations according to the rules in \autoref{fig:denotational_sem}.
\vspace{-7pt}
\begin{figure} [H]
\begin{equation*}
\begin{split}
\begin{aligned}
&
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
      [\![\Gamma_{i} \triangleright v_{i}: \mathbb{A}_{i} ]\!]=m_{i}  \quad f: \mathbb{A}_{1}, \ldots, \mathbb{A}_{n} \xrightarrow{} \mathbb{A} \in \Sigma \quad E \in \text{Sf}(\Gamma_{1}; \ldots; \Gamma_{n})\\
    \hline
  [\![E \triangleright f( v_{1},\ldots,v_{n}): \mathbb{A} ]\!] = [\![ f]\!] \cdot (m_{1}\otimes \ldots \otimes m_{n}) \cdot \text{sp}_{\Gamma_1;\ldots;\Gamma_n}\cdot \text{sh}_{E}
\end{array}
$
\end{minipage} \\
&
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
      \\
    \hline
  [\![ x:\mathbb{A} \triangleright x:\mathbb{A}]\!] = \id_{[\![\mathbb{A} ]\!]}
\end{array}
$ \end{minipage} 
\hspace{220pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \\  
    \hline
   [\![ - \triangleright *: \mathbb{I}]\!] = \id_{[\![\mathbb{I} ]\!]} 
\end{array}
$
\end{minipage}\\
&
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
      [\![\Gamma \triangleright v: \mathbb{A} \otimes \mathbb{B} ]\!]=m  \quad [\![\Delta,x: \mathbb{A}, y: \mathbb{B}  \triangleright w: \mathbb{D} ]\!] =n  \quad E \in \text{Sf}(\Gamma;\Delta)\\
    \hline
  [\![ E\triangleright \text{pm } v \text{ to } x \otimes y. w :\mathbb{D}]\!] = n \cdot \text{jn}_{\Delta;\mathbb{A};\mathbb{B}} \cdot \alpha \cdot \text{sw}\cdot (m \otimes \id) \cdot \text{sp}_{\Gamma;\Delta} \cdot \text{sh}_{E}
\end{array}
$ \end{minipage} \\
&
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}  
     [\![ \Gamma \triangleright v: \mathbb{A} ]\!] =m \quad [\![\Delta \triangleright w: \mathbb{B} ]\!]=n \quad E \in \text{Sf}(\Gamma;\Delta) \\
    \hline
  [\![ E \triangleright v \otimes w: \mathbb{A} \otimes \mathbb{B} ]\!] = (m \otimes n) \cdot \text{sp}_{\Gamma;\Delta} \cdot \text{sh}_{E}
\end{array} 
$
\end{minipage}\\
&
 \begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c} 
    [\![\Gamma \triangleright v: \mathbb{I} ]\!]=m  \quad [\![\Delta \triangleright w: \mathbb{A}]\!]=n \quad E \in \text{Sf}(\Gamma;\Delta)  \\
    \hline
  [\![E \triangleright v \text { to } *.w: \mathbb{A} ]\!]=n \cdot \lambda \cdot (m \otimes \id) \cdot \text{sp}_{\Gamma;\Delta} \cdot \text{sh}_{E}
\end{array}
$ \end{minipage} 
\hspace{130 pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c} 
     [\![\Gamma,x:\mathbb{A} \triangleright v: \mathbb{B} ]\!] = m \\
    \hline
   [\![ \Gamma \triangleright \lambda x:\mathbb{A} . \hspace{2pt } v: \mathbb{A} \multimap \mathbb{B}]\!] = \overline{m \cdot \text{jn}_{\Gamma;\mathbb{A}}}
\end{array}
$
\end{minipage} \\
&
 \begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c} 
     [\![\Gamma \triangleright v: \mathbb{A} \multimap \mathbb{B} ]\!] = m \quad [\![  \Delta \triangleright w: \mathbb{A} ]\!] =n \quad E \in S\hspace{-3pt}f(\Gamma;\Delta)  \\
    \hline
  [\![ E \triangleright v\,w: \mathbb{B} ]\!] = \text{app} \cdot (m \otimes n) \cdot \text{sp}_{\Gamma;\Delta} \cdot \text{sh}_{E}
\end{array}
$ \end{minipage}
 \\[5pt]
&\hspace{5pt}
  %
  \begin{prooftree}
      \hypo{ [\![\Gamma \vljud v: \typeA]\!] = m }
      \infer1[]{ [\![ \Gamma \vljud \text{inl}_{\typeB} (v):  \typeA \oplus \typeB  ]\!] = \inl  \comp m}
  \end{prooftree}
  %
  \hspace{140pt}
  %
  \begin{prooftree}
    \hypo{ [\![\Gamma \vljud v: \typeB]\!] = m }
    \infer1[]{ [\![ \Gamma \vljud \text{inr}_{\typeA} (v):  \typeA \oplus \typeB  ]\!] = \inr  \comp m}
\end{prooftree}
  %
  \\
&\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
      [\![\Gamma\vljud v: \typeA \oplus \typeB ]\!] = b 
      \quad [\![\Delta, x:\typeA \vljud w: \typeD ]\!] = p 
      \quad [\![\Delta,y:\typeB \vljud u: \typeD ]\!] = q 
      \quad E \in \Shuff(\Gamma; \Delta)
      \\
    \hline
  [\![E \vljud \text{case } v\,  \{\text{inl}_{\typeB} (x) \Rightarrow w ;\, \text{inr}_{\typeA} (y) \Rightarrow u\}: \typeD ]\!] =   [p,q] \comp (\text{jn}_{\Delta;\typeA}\comp \sw \oplus \text{jn}_{\Delta;\typeB}\comp \sw) 
 \comp \dist \comp  
 \\[-5pt] \hspace{160pt}  (b \otimes \id) \comp \text{sp}_{\Gamma;\Delta} \comp \text{sh}_{E} 
\end{array}
$
\end{minipage} \\
\end{aligned}
\end{split}
\end{equation*}
\caption{Judgment interpretation}
\label{fig:denotational_sem} 
\end{figure}



The following diagrams are useful for a clearer understanding of the interpretation of judgements given in \autoref{fig:denotational_sem}.
\begin{align*}
   & \hspace{-20pt} \llbracket \text{ax} \rrbracket : & [\![E]\!] & \xrightarrow{\hspace{2pt}\text{sh}_{E}\hspace{2pt}}   [\![\Gamma_1,\ldots,\Gamma_n ]\!]   \xrightarrow{\hspace{1pt}\text{sp}_{\Gamma;\Delta}\hspace{1pt}}  \llbracket \Gamma_1 \rrbracket \otimes \ldots \otimes \llbracket \Gamma_n \rrbracket  \\
  & & & \xrightarrow{\hspace{2pt}m_1 \otimes \ldots \otimes m_n \hspace{2pt}} \llbracket \mathbb{A}_1 \rrbracket \otimes \ldots \otimes \llbracket \mathbb{A}_n \rrbracket \xrightarrow{\hspace{2pt}\llbracket f \rrbracket \hspace{2pt}} \llbracket \mathbb{A} \rrbracket \\
  & \hspace{-20pt} \llbracket \text{hyp} \rrbracket : & \llbracket \mathbb{A} \rrbracket & \xrightarrow{\hspace{2pt}\id_{\llbracket \mathbb{A}\rrbracket}\hspace{2pt}} \llbracket \mathbb{A}\rrbracket \\
  & \hspace{-20pt}\llbracket \mathbb{I}_i \rrbracket :&  \llbracket \mathbb{I} \rrbracket & \xrightarrow{\hspace{2pt}\id_{\llbracket \mathbb{I}\rrbracket}\hspace{2pt}}  \llbracket \mathbb{I} \rrbracket\\
  & \hspace{-20pt}\llbracket \otimes_e \rrbracket : & [\![E]\!] & \xrightarrow{\hspace{2pt}\text{sh}_{E}\hspace{2pt}}   [\![\Gamma,\Delta ]\!]   \xrightarrow{\hspace{1pt}\text{sp}_{\Gamma;\Delta}\hspace{1pt}}  [\![\Gamma ]\!] \otimes [\![\Delta ]\!] \xrightarrow{ m \hspace{1pt} \otimes \hspace{1pt} \id} ([\![\mathbb{A} ]\!] \otimes [\![\mathbb{B} ]\!]) \otimes [\![\Delta ]\!]   \\
     & &  &\xrightarrow{\hspace{2pt}\text{sw}\hspace{2pt}}  [\![\Delta ]\!] \otimes ([\![\mathbb{A} ]\!] \otimes [\![\mathbb{B} ]\!]) \xrightarrow{\hspace{3pt}\alpha\hspace{3pt}}  ([\![\Delta ]\!] \otimes [\![\mathbb{A} ]\!]) \otimes [\![\mathbb{B} ]\!] \xrightarrow{\hspace{2pt}\text{jn}_{\Delta; \mathbb{A}; \mathbb{B} }\hspace{2pt}}  \llbracket \Delta, \mathbb{A},\mathbb{B}  \rrbracket  \\
     &&& \xrightarrow{\hspace{3pt}n\hspace{3pt}} \llbracket \mathbb{D} \rrbracket \\
    & \hspace{-20pt}\llbracket \otimes_i \rrbracket : & [\![E]\!] & \xrightarrow{\hspace{2pt}\text{sh}_{E}\hspace{2pt}}   [\![\Gamma,\Delta ]\!]   \xrightarrow{\hspace{1pt}\text{sp}_{\Gamma;\Delta}\hspace{1pt}}  [\![\Gamma ]\!] \otimes [\![\Delta ]\!]  \xrightarrow{\hspace{2pt} m \hspace{1pt} \otimes \hspace{1pt} n \hspace{2pt}} [\![\mathbb{A} ]\!] \otimes [\![\mathbb{B} ]\!] \\
    &\hspace{-20pt}\llbracket \mathbb{I}_e \rrbracket :  & [\![E]\!] & \xrightarrow{\hspace{2pt}\text{sh}_{E}\hspace{2pt}}   [\![\Gamma,\Delta ]\!]   \xrightarrow{\hspace{1pt}\text{sp}_{\Gamma;\Delta}\hspace{1pt}}  [\![\Gamma ]\!] \otimes [\![\Delta ]\!] \xrightarrow{ m \hspace{1pt} \otimes \hspace{1pt} \id} [\![\mathbb{I} ]\!]\otimes [\![\Delta ]\!] \xrightarrow{\hspace{2pt} \lambda \hspace{3pt}} [\![\Delta ]\!]  \xrightarrow{\hspace{2pt} n \hspace{3pt}}  \llbracket \mathbb{A} \rrbracket   \\
    &\hspace{-20pt}\llbracket \multimap_i \rrbracket : & \llbracket \Gamma \rrbracket  & \xrightarrow{\hspace{2pt} \overline{m \cdot \text{jn}_{\Gamma;\mathbb{A}}}  \hspace{2pt}}  \llbracket \mathbb{A} \rrbracket \multimap \llbracket \mathbb{B} \rrbracket  \hspace{60pt} (\llbracket \Gamma \rrbracket \otimes \llbracket \mathbb{A} \rrbracket  \xrightarrow{\hspace{2pt} \text{jn}_{\Gamma; \mathbb{A}} \hspace{2pt}} \llbracket \Gamma, \mathbb{A} \rrbracket  \xrightarrow{\hspace{2pt} m \hspace{2pt}}  \llbracket \mathbb{B} \rrbracket )\\
    &\hspace{-20pt}\llbracket \multimap_e \rrbracket : & [\![E]\!] & \xrightarrow{\hspace{2pt}\text{sh}_{E}\hspace{2pt}}   [\![\Gamma,\Delta ]\!]   \xrightarrow{\hspace{1pt}\text{sp}_{\Gamma;\Delta}\hspace{1pt}}  [\![\Gamma ]\!] \otimes [\![\Delta ]\!]  \xrightarrow{\hspace{2pt} m \hspace{1pt} \otimes \hspace{1pt} n \hspace{2pt}} (\llbracket \mathbb{A} \rrbracket \multimap \llbracket \mathbb{B} \rrbracket) \otimes \llbracket \mathbb{A} \rrbracket  \xrightarrow{\hspace{2pt} \text{app} \hspace{2pt}} \llbracket \mathbb{B} \rrbracket \\
    & \hspace{-20pt}\llbracket \text{inl} \rrbracket : & [\![\Gamma]\!] & \xrightarrow{\hspace{2pt} m \hspace{2pt}} \llbracket \mathbb{A} \rrbracket \xrightarrow{\hspace{2pt}\inl \hspace{2pt}} \llbracket \typeA \oplus \typeB \rrbracket\\
    &\hspace{-20pt}\llbracket \text{inr} \rrbracket : & [\![\Gamma]\!] & \xrightarrow{\hspace{2pt} m \hspace{2pt}} \llbracket \mathbb{B} \rrbracket \xrightarrow{\hspace{2pt}\inr \hspace{2pt}} \llbracket \typeA \oplus \typeB \rrbracket\\
    &\hspace{-20pt}\llbracket \text{case} \rrbracket : & [\![E]\!] & \xrightarrow{\hspace{2pt}\text{sh}_{E}\hspace{2pt}}   [\![\Gamma,\Delta ]\!]   \xrightarrow{\hspace{1pt}\text{sp}_{\Gamma;\Delta}\hspace{1pt}}  [\![\Gamma ]\!] \otimes [\![\Delta ]\!] \xrightarrow{ b \hspace{1pt} \otimes \hspace{1pt} \id} ([\![\mathbb{A} ]\!] \oplus[\![\mathbb{B} ]\!]) \otimes [\![\Delta ]\!]   \\
     & &  &\xrightarrow{\hspace{2pt}\dist\hspace{1pt}} \left([\![\mathbb{A} ]\!] \otimes [\![\Delta ]\!]\right) \oplus \left([\![\mathbb{B} ]\!] \otimes [\![\Delta ]\!]\right)  \\
     & & &  \xrightarrow{\hspace{1pt}\text{jn}_{\Delta;\typeA}\comp \sw \,\oplus  \, \text{jn}_{\Delta;\typeB}\comp \sw\hspace{1pt}} [\![ \Delta,\mathbb{A}]\!]\oplus [\![\Delta, \mathbb{B} ]\!]\xrightarrow{\hspace{2pt}[p,q]\hspace{2pt}} \llbracket \mathbb{D} \rrbracket \\
\end{align*} 


Regarding the interpretation of the exhange and substitution properties, we have the following lemma.

\begin{lemma} \label{lem:sub_exch} 
  For any judgements $\Gamma, x: \mathbb{A}, y: \mathbb{B}, \Delta \triangleright v : \mathbb{D}$, $\Gamma, x: \mathbb{A} \triangleright v: \mathbb{B}$, and $\Delta \triangleright w: \mathbb{A}$, the following holds:
\end{lemma}
\vspace{-30pt}
\begin{equation*}
\begin{split}
  \llbracket \Gamma, x: \mathbb{A}, y: \mathbb{B}, \Delta \triangleright v : \mathbb{D} \rrbracket & = \llbracket \Gamma,  y: \mathbb{B}, x: \mathbb{A}, \Delta \triangleright v : \mathbb{D} \rrbracket \cdot \text{exch}_{\Gamma, \underline{x: \mathbb{A}, y: \mathbb{B}}, \Delta}\\
  \llbracket \Gamma,\Delta \triangleright v[w/x]: \mathbb{B} \rrbracket & = \llbracket \Gamma, x: \mathbb{A} \triangleright v: \mathbb{B} \rrbracket \cdot \text{jn}_{\Gamma;\mathbb{A}} \cdot (\id \otimes \llbracket \Delta \triangleright w: \mathbb{A} \rrbracket ) \cdot \text{sp}_{\Gamma; \Delta} 
\end{split}
\end{equation*}


\begin{proof}
  This lemma is proved in \cite[Lemma 2.2]{dahlqvist2023syntactic} for the lambda calculus without conditionals, so we only need to address the conditional cases.

  We begin with the exchange property. The rules involving injections are
  straightforward. As for the rule~$\text{case}$, we distinguish between
  the scenarios where both variables ($x : \typeA$ and $y : \typeB$) are in
  $\Gamma$, both are in $\Delta$, or they are distributed across $\Gamma$ and
  $\Delta$. We begin with the first case. 
  \begin{align*}
    & \sem{\Gamma,x, y, \Delta \vljud \text{case }  v\,  \{\text{inl} (a) \Rightarrow w ;\, \text{inr} (b) \Rightarrow u\}} \\ 
    &\triangleq   [\sem{w}  , \sem{u}] \comp (\text{jn}
    \comp \sw \oplus \text{jn} \comp \sw) \comp \dist 
    \comp (\sem{ v} \otimes \id)  
    \comp \text{sp} 
    \comp \text{sh}
    & \\
    & =  [\sem{w},\sem{u}] \comp (\text{jn}
    \comp \sw \oplus \text{jn} \comp \sw)  \comp \dist 
    \comp (\sem{v} \comp\, \text{exch}
    \otimes \id) \comp \text{sp} 
    \comp \text{sh}
    & \text{(Induction)}\\
    &  =  \dots  \comp (\sem{v} \otimes \id) 
          \comp (\text{exch} \otimes \id) \comp \text{sp} \comp  \text{sh} & \\
    & = \dots \comp (\sem{v} \otimes \id) \comp \text{sp}
          \comp \text{sh}  
          \comp \text{exch} 
    & {\text{(Coherence)}} \\
    & \triangleq \sem{\Gamma,y, x, \Delta \vljud \text{case } v\,  \{\text{inl} (a) \Rightarrow w ;\, \text{inr} (b) \Rightarrow u\}} 
    \comp \text{exch}
  \end{align*}
Let us now focus on the second case, \ie\ both variables live in $\Delta$.

\begin{align*}
  & \hspace{-30pt} \sem{\Gamma,x, y, \Delta \vljud \text{case } \, v\, \{\text{inl} (a) \Rightarrow w ;\, \text{inr} (b) \Rightarrow u\}} \\
  & \hspace{-30pt} \triangleq  [\sem{w} ,\sem{u}]\comp (\text{jn}
  \comp \sw \oplus \text{jn} \comp \sw) 
  \comp \dist \comp (\sem{v} \otimes \id) \comp 
  \text{sp}_{} 
  \comp \text{sh}_{}  \\
  & \hspace{-30pt} = [\sem{ w }  \comp \text{exch}, \sem{u}\comp \text{exch}] \comp (\text{jn}\comp \sw \oplus \text{jn} \comp \sw) \comp \dist \comp (\sem{v} \otimes \id) \comp\text{sp} \comp \text{sh}
  & \text{(Induction)}\\
  &\hspace{-30pt} = [\sem{ w }, \sem{u}] 
  \comp (\text{exch} \oplus \text{exch}) 
  \comp (\text{jn}\comp \sw \oplus \text{jn} \comp \sw) 
  \comp \ldots
  & \text{(Coproduct laws)}
  \\
  & \hspace{-30pt} = [\sem{ w }, \sem{u}] 
  \comp (\text{jn}\comp \sw \oplus \text{jn} \comp \sw) 
  \comp (\id \otimes \text{exch} \oplus \id \otimes \text{exch})
  \comp \dist \comp \ldots
  & \text{(Coherence)}
  \\
  & \hspace{-30pt} = \ldots 
  \comp \dist 
  \comp (\id \otimes \text{exch}) \comp (\sem{v} \otimes \id) \comp\text{sp} \comp \text{sh} 
  & \text{(Naturality)}
  \\
  & \hspace{-30pt} = \ldots
  \comp \dist 
  \comp (\sem{v} \otimes \id) \comp\text{sp} \comp \text{sh} 
  \comp \text{exch}
  & \text{(Coherence)}
  \\
  & \hspace{-30pt} \triangleq \sem{\Gamma,y, x, \Delta \vljud \text{case } v\,  \{\text{inl} (a) \Rightarrow w ;\, \text{inr} (b) \Rightarrow u\}} 
  \comp \text{exch}_{}
\end{align*}

The proof the for the third case follows directly from the coherence theorem
for symmetric monoidal categories.

Regarding the substitution rule, once again the cases involving the injections follow directly by induction on the derivation tree. For the rule (case), we distinguish between the scenarios where the variable $x$ is in $\Gamma$ or in $\Delta$. We start with the first case.
\begin{align*}
  & \hspace{-30pt} [\![E, Z \vljud \text{case } v \,  \{\text{inl} (a) \Rightarrow w ;\, \text{inr} (b) \Rightarrow u\} [t/x]]\!] \\
  & \hspace{-30pt} \triangleq \left[[\![ w ]\!] ,[\![ u ]\!]\right] \comp (\text{jn}\comp \sw \oplus \text{jn} \comp \sw) \comp \dist \comp  ([\![ v [t/x] ]\!]   \otimes \id)  \comp \text{sp} \comp \text{sh} \\
  & \hspace{-30pt} = \left[[\![ w ]\!] ,[\![ u ]\!]\right] \comp (\text{jn} \comp \sw \oplus \text{jn} \comp \sw)  \comp \dist \comp (([\![ v]\!]  \comp \text{jn} \comp (\id \otimes [\![  t ]\!] ) \comp \text{sp} )\otimes \id)  \comp \text{sp} \comp \text{sh} & {\text{(Induction)}} \\
  & \hspace{-30pt} =  \ldots  \comp ([\![ v]\!] \otimes \id) \comp (\text{jn} \comp (\id \otimes [\![  t ]\!] ) \comp \text{sp} \otimes \id)  \comp \text{sp}\comp \text{sh}\\
  & \hspace{-30pt} = \ldots \comp ([\![ v]\!] \otimes \id)  \comp (\text{jn} \comp (\id \otimes [\![  t ]\!] ) \comp \text{sp} \otimes \id)  \comp \text{sp} \comp \text{sh} \comp \text{jn} \comp \text{sp} &  \\
  & \hspace{-30pt} = \ldots  \comp ([\![ v]\!] \otimes \id) \comp (\text{jn}\comp \text{sp} \otimes   \id) \comp \text{sp}   \comp \text{sh} \comp \text{jn}\comp (\id \otimes [\![ t ]\!] ) \comp \text{sp}  & {(\text{Naturality})}   \\
  & \hspace{-30pt} = \ldots  \comp ([\![ v]\!] \otimes \id) \comp \text{sp}  \comp \text{sh}  \comp  \text{jn} \comp (\id \otimes [\![ t ]\!] ) \comp \text{sp}  & {(\text{Coherence})}   \\
  & \hspace{-30pt} \triangleq \, [\![E,  x \vljud \text{case } v\,  \{\text{inl} (x) \Rightarrow w ;\, \text{inr} (y) \Rightarrow u\}]\!]  \comp  \text{jn} \comp (\id \otimes [\![ t ]\!] ) \comp \text{sp}
\end{align*}

  With respect to the second case, \ie, $x$ lives in $\Delta$, we have: 
  \begin{align*}
  &\hspace{-30pt} [\![E, Z \vljud \text{case } v \,  \{\text{inl} (a) \Rightarrow w ;\, \text{inr} (b) \Rightarrow u\} [t/x]]\!] \\
  & \hspace{-30pt} \triangleq \left[[\![ \Delta  , Z ,  a \vljud w[t/x] ]\!] ,[\![ \Delta, Z, b \vljud u[t/x]  ]\!] \right] \comp (\text{jn}\comp \sw \oplus \text{jn} \comp \sw) \comp \dist  \comp ([\![ v ]\!]   \otimes \id)     \\
  &  \hspace{-20pt} \comp \text{sp} \comp \text{sh}  \\
  & \hspace{-30pt} =  \left[[\![ \Delta  ,   a, Z \vljud w[t/x] ]\!] ,[\![ \Delta, b, Z \vljud u[t/x]  ]\!] \right] \comp ( \text{exch} \comp \text{jn}\comp \sw \oplus  \text{exch} \comp \text{jn} \comp \sw)   \comp \ldots   & {(\text{Exch., Cop.})} \\
  & \hspace{-30pt} =   \left[[\![w ]\!] \comp  \text{jn} \comp (\id \otimes [\![t]\!] ) \comp \text{sp}  ,[\![ u]\!] \comp \text{jn} \comp (\id \otimes [\![t]\!] ) \comp \text{sp} \right] \comp (\text{exch} \comp \text{jn} \comp \sw  \oplus \text{exch}  & {\text{(Induction)}}  \\
  & \hspace{-20pt} \comp\text{jn} \comp \sw ) \comp \ldots \\
  & \hspace{-30pt} =  \big[[\![w ]\!] \comp \text{exch} \comp  \text{jn} \comp \sw \comp (\id \otimes \text{jn}) \cdot (\sw \otimes \id) \comp (\id \otimes \text{sp} ) \comp (\id \otimes [\![t]\!] ) \comp \text{sp}  ,  & (\text{Coherence})\\ 
  & \hspace{-10pt}  [\![u ]\!] \comp \text{exch} \comp  \text{jn} \comp \sw \comp (\id \otimes \text{jn}) \cdot (\sw \otimes \id) \comp (\id \otimes \text{sp} ) \comp (\id \otimes [\![t]\!] ) \comp \text{sp} \big]  \cdot \ldots \\
  & \hspace{-30pt} =  \big[[\![w ]\!] \comp \text{exch} \comp  \text{jn} \comp \sw  \comp (\id \otimes \text{jn}) \comp (\id \otimes [\![t]\!] ) \cdot (\sw \otimes \id) \comp (\id \otimes \text{sp} )  \comp \text{sp}  ,  & (\text{Naturality})\\ 
  & \hspace{-10pt}  [\![u ]\!] \comp \text{exch} \comp  \text{jn} \comp \sw \comp (\id \otimes \text{jn}) \comp (\id \otimes [\![t]\!] ) \cdot (\sw \otimes \id) \comp (\id \otimes \text{sp} ) \comp \text{sp} \big]  \cdot \ldots \\
  & \hspace{-20pt} \comp  ( \text{exch} \comp \text{jn}\comp \sw \oplus  \text{exch} \comp \text{jn} \comp \sw)   \comp \ldots  \\
  & \hspace{-30pt} =  \left[[\![w ]\!] ,[\![ u]\!] \right] \comp \big(\text{exch} \comp \text{jn} \comp \sw \comp (\id \otimes  \text{jn} ) \comp (\id \otimes [\![t]\!]) \comp  (\id \otimes \text{sp} ) \oplus \text{exch} \comp \text{jn} \comp \sw   & {(\text{Coherence},} \\
  & \hspace{-30pt} \hspace{10pt} \comp (\id \otimes  \text{jn} ) \comp (\id \otimes [\![t]\!]) \comp  (\id \otimes \text{sp} ) \big)\comp  \ldots & {\text{Cop. laws})} \\
  %& \hspace{-30pt} =  \ldots \comp \big(\ldots  \comp (\id \otimes  (\text{jn} \comp \id \otimes [\![t]\!] \comp \text{sp}) ) \oplus  \ldots  \comp (\id \otimes  (\text{jn}\comp \id \otimes [\![t]\!] \comp \text{sp}) )\big) \comp \dist \comp \ldots &  \\
  & \hspace{-30pt} = [[\![ \Delta  , x ,  a \vljud w]\!] , [\![ \Delta  , x ,  b \vljud u]\!]]  \comp \big(\text{jn} \comp \sw \comp (\id \otimes  (\text{jn} \comp \id \otimes [\![t]\!]  \comp \text{sp}))  \oplus \text{jn} \comp  \sw      & {(\text{Exchange, }}  \\
  & \hspace{-30pt} \hspace{10pt} \comp (\id \otimes  (\text{jn} \comp \id \otimes [\![t]\!] \comp \text{sp}) )\big) \comp \dist \ldots  & { \text{Cop. laws})}\\
  & \hspace{-30pt} = \ldots \comp  (\text{jn} \comp \sw \oplus \text{jn} \comp \sw)  \comp \dist \comp ( \id \otimes (\text{jn} \cdot (\id \otimes [\![t]\!]) \cdot \text{sp} ) ) \comp ([\![v]\!] \otimes \id)  & {(\text{Cop. laws,}}  \\
  & \hspace{-20pt} \comp \text{sp} \comp \text{sh} & {\text{Naturality})} \\
  & \hspace{-30pt} =  \ldots \comp \dist \comp  ([\![v]\!] \otimes \id)  \comp ( \id \otimes (\text{jn} \comp  (\id \otimes [\![t]\!]) \cdot \text{sp} ) ) \comp \text{sp}\comp \text{sh} \comp \text{jn}  \comp \text{sp}    & \\
  & \hspace{-30pt} = \ldots \comp \dist \comp  ([\![v]\!] \otimes \id)  \comp ( \id \otimes \text{jn} \comp \text{sp}) \comp \text{sp}  \comp \text{sh}    \comp \text{jn} \comp (\id \otimes [\![ t ]\!] ) \comp \text{sp} & {(\text{Naturality})}   \\
  & \hspace{-30pt} =  \ldots  \comp \dist   \comp ([\![v  ]\!] \otimes \id) \comp \text{sp} \comp \text{sh} \comp \text{jn} \comp (\id \otimes [\![ t ]\!] ) \comp \text{sp}  & {(\text{Coherence})}     \\ 
  & \hspace{-30pt} \triangleq  [\![E,  x\vljud \text{case } v\,  \{\text{inl} (a) \Rightarrow w ;\, \text{inr} (b) \Rightarrow u\}]\!] \comp \text{jn} \comp (\id \otimes [\![ t ]\!] ) \comp \text{sp}
\end{align*}




\end{proof}

\begin{definition} [Models of linear \(\lambda\)-theories]
Consider a linear \(\lambda\)-theory \(((G, \Sigma), Ax)\) and a symmetric monoidal closed category with coproducts \(\catC\). 
Suppose that for each \(X \in G\), we have an interpretation \(\llbracket X \rrbracket\), which is an object of \(\catC\), 
and analogously for the operation symbols in \(\Sigma\). 
This interpretation structure is a \emph{model} of the theory if all axioms in \(Ax\) are satisfied by the interpretation.
\end{definition}




\begin{theorem} \label{thm:soundness_classical}
The equations presented in \autoref{fig:equations-linear-lambda} are sound with respect to judgement interpretation. 
More specifically, if \(\Gamma \vljud v = w : \typeA\) is one of the equations in \autoref{fig:equations-linear-lambda}, then 
$\llbracket \Gamma \vljud v : A  \rrbracket = \llbracket  \Gamma \vljud w : \typeA\rrbracket.$
\end{theorem}

\begin{proof}
  Since the theorem is already proven in \cite[Theorem 2.3]{dahlqvist2023syntactic} for the lambda calculus without conditionals, it suffices to consider the cases involving conditionals. 
  
  The soundeness of the equations for conditionals follow from \autoref{lem:sub_exch}, the coherence theorem for symmetric monoidal categories, naturality, and the universal property of the coproduct.   
  We will provide a complete proof for the $\beta_{case}^{inl}$ and $\eta_{case}$ equations, noting that the proof for the $\beta_{case}^{inr}$ equation follows analogously from the first. For the $\beta_{case}^{inl}$ equation, we reason as follows:
  \begin{align*}
    & \hspace{-30pt}  \llbracket \Delta,\Gamma \vljud  \text{case }  \text{inl}(v)\, \{\text{inl} (x) \Rightarrow w ;\, \text{inr} (y) \Rightarrow u\} \rrbracket \\
    & \hspace{-30pt}  \triangleq  [\llbracket w\rrbracket,\llbracket u \rrbracket] \comp (\text{jn} \comp \sw \oplus \text{jn} \comp \sw  ) \comp \dist \comp \sw \comp (\inl \comp \llbracket  v \rrbracket \otimes \id) \comp \text{sp} \comp \text{sh} \\
    & \hspace{-30pt} =  \ldots \comp \dist \comp (\inl \otimes \id  ) \comp ( \llbracket  v \rrbracket \otimes \id) \comp \text{sp} \comp \text{sh}  \\
    & \hspace{-30pt} =  \ldots \comp \dist \comp [\inl \otimes \id, \inr \otimes \id] \comp \inl  \comp ( \llbracket  v \rrbracket \otimes \id) \comp \text{sp} \comp \text{sh}  &(\text{Coproduct laws})\\
    & \hspace{-30pt} =  [\llbracket w\rrbracket,\llbracket u \rrbracket] \comp (\text{jn} \comp \sw \oplus \text{jn} \comp \sw )  \comp \inl  \comp ( \llbracket  v \rrbracket \otimes \id) \comp \text{sp} \comp \text{sh} \\
    & \hspace{-30pt}  = \llbracket  w \rrbracket\comp \text{jn} \comp   \sw \comp(\llbracket v \rrbracket \otimes  \id) \comp \text{sp} \comp \text{sh} & {\text{(Coproduct laws)}} \\
    &\hspace{-30pt}  =  \llbracket  w \rrbracket\comp \text{jn} \comp(  \id\otimes \llbracket v \rrbracket ) \comp   \sw \comp \text{sp} \comp \text{sh} & {\text{(Naturality)}} \\
    &\hspace{-30pt}  = \llbracket w \rrbracket \comp \text{jn} \comp( \id \otimes \llbracket v \rrbracket) \comp \text{sp} & {\text{(Coherence)}}\\
    & \hspace{-30pt}  = \llbracket w[v/x]  \rrbracket  & {\text{(\autoref{lem:sub_exch})}}
    \end{align*}
  Regarding the $\eta_{case}$ equation, we have:
\begin{align*}
  &  \hspace{-30pt} \llbracket \Delta,\Gamma \vljud \text{case } v\, \{\text{inl}_{\typeB} (y) \Rightarrow w [ \text{inl}_{\typeB}(y)/x] ;\, \text{inr} (z) \Rightarrow w [ \text{inr}(z)/x]\}: \typeD\rrbracket \\
  &  \hspace{-30pt} \triangleq  [\llbracket  w [ \text{inl}(y)/x] \rrbracket, \llbracket  w [ \text{inr}(z)/x] \rrbracket] \comp (\text{jn} \comp \sw \oplus \text{jn} \comp \sw  ) \comp \dist \comp (\llbracket v\rrbracket \otimes \id) \\
  &  \hspace{-20pt} \comp  \text{sp} \comp \text{sh}  \\
  &  \hspace{-30pt} = \llbracket w \rrbracket \comp [\text{jn} \comp (\id \otimes\inl \comp  \llbracket y \rrbracket) \comp \text{sp} \comp \text{jn} \comp \sw , 
   \text{jn}\comp (\id \otimes\inr  \comp \llbracket z \rrbracket)  \comp \text{sp} \comp \text{jn} \comp \sw  ] \comp \ldots  & {\text{(Lem. \ref{lem:sub_exch},}} \\
  &  & {\text{Cop. laws)}}  \\
  &  \hspace{-30pt} =  \llbracket w \rrbracket \comp \text{jn} \comp [ (\id \otimes\inl)  \comp \sw  ,   (\id \otimes\inr) \comp \sw  ] \comp \ldots  & {\text{(Cop. laws})}   \\
  &  \hspace{-30pt} = \llbracket w \rrbracket \comp \text{jn} \comp   \sw \comp  [\inl \otimes \id , \inr \otimes \id] \comp \dist   \comp (\llbracket v \rrbracket \otimes \id)  \comp  \text{sp}\comp \text{sh}  & {\text{(Naturality, }}  \\
  &  \hspace{-30pt} \hspace{10pt} & {\text{Cop. laws})}   \\
  %& = \llbracket w \rrbracket \comp \text{jn}_{\Delta;\typeA \oplus \typeB} \comp \comp \sw \id   \comp (\llbracket v \oplus \typeB\rrbracket \otimes \id)  \comp  \text{sp}_{\Gamma;\Delta} \comp \text{sh}_{\Delta;\Gamma}  \\
  &  \hspace{-30pt} = \llbracket w \rrbracket \comp \text{jn} \comp ( \id \otimes \llbracket v \rrbracket )  \comp  \sw  \comp  \text{sp} \comp \text{sh} & {\text{(Naturality)}}\\
  &   \hspace{-30pt} = \llbracket w \rrbracket \comp \text{jn} \comp ( \id \otimes \llbracket v \rrbracket )  \comp  \text{sp} & {\text{(Coherence)}}\\
  &  \hspace{-30pt} \triangleq \llbracket w[v/x] : \typeD \rrbracket & (\text{Lem. \ref{lem:sub_exch}})
\end{align*}



\end{proof}



\begin{theorem}[Completeness] \label{thm:completeness_classical}
Consider a linear $\lambda$-theory $T$. Then an equation 
$\Gamma \, \vljud \, v = w : \typeA$
is a theorem of $T$ if and only if it is satisfied by all models of the theory.
\end{theorem}

 \begin{proof}
   Completeness arises from constructing the syntactic category $\catSyn(T)$ of a $\lambda$-theory $T$. The syntactic category of $T$ has as objects the types of $T$ and as morphisms $A \rightarrow B$ the equivalence classes (w.r.t. provability) of terms $v$ for which we can derive $x : \typeA \vljud v : \typeB$.
   This theorem is proved in \cite[Lemma 2.6]{dahlqvist2023syntactic} for the lambda calculus without conditionals, so we only need to address the cases involving conditionals.

   By employing the equations $\beta_{case}^{inl}$, $\beta_{case}^{inr}$, and $\eta_{case}$  in \autoref{fig:equations-linear-lambda}, we show that the universal property of the coproduct is satisfied in $\catSyn(T)$.  We note that the distributivity follows from the category being closed.

   In the syntactic category, the coproduct $[p , q]$ can be seen as the equivalence
  class 
 \begin{equation*}
  \left[z:\typeA \oplus\typeB \vljud \text{case } z\,\{\inl_{\typeB}(x) \Rightarrow p ; \, \inr_{\typeA}(y) \Rightarrow q\}: \typeD\right].
 \end{equation*}

 The proof of the coproduct diagram comutes follow directly from the equations $\beta_{case}^{inl}$ and $\beta_{case}^{inr}$ in \autoref{fig:equations-linear-lambda}, and substituition (\autoref{lem:exh_and_sub}). Specifically, for the left triangle in the coproduct diagram, we have that:
  \begin{align*}
    & \left[\text{case } z\,\{\inl(x) \Rightarrow p ; \, \inr(y) \Rightarrow q\}\right] \comp \left[ \inl(x)\right]  \\
    = & \left[ \text{case } \inl(x) \,\{\inl(a) \Rightarrow p[a/x] ; \, \inr(b) \Rightarrow q[b/y]\} \right] & (\text{\autoref{lem:exh_and_sub} })  \\
    = &   \left[ p[a/x][x/a]  \right]& (\beta_{\text{case}}^{\text{inl}}) \\
    = & \left[ p \right] \\
  \end{align*}
  The proof for the right triangle in the coproduct diagram is analogous.

  Regarding the unicity of the coproduct, the key aspect of the proof lies in proving that the following equality holds: 
  $$\left[z:\typeA \oplus \typeB \vljud  \text{case } z \,\{\inl(x) \Rightarrow m[\inl(x)/ z] ; \, \inr(y) \Rightarrow m[\inr(y)/ z]\} : \typeD \right] = [z:\typeA \oplus \typeB \, \vljud \,m: \typeD].$$ 
  This equality follows direct from the $\eta_{case}$ equation. With this equality at hand, unicity is automatically proven, given that considering any morphism $m'$ from $\typeA \oplus \typeB$ to $\typeD$ and the coproduct diagram, we have that 
  \[\left[ m'[ \inl(x)/z] \right]  = \left[p \right]   \] 
  and
   \[\left[ m'[\inr(y)/z] \right] = \left[ q\right].\] 
  As a result, 
  \begin{align*}
    \left[ m'\right]& = \left[  \text{case } z \,\{\inl(x) \Rightarrow m'[\inl(x)/ z] ; \, \inr(y) \Rightarrow m'[\inr(x)/ z]\}\right]\\
    & = \left[\text{case } z \,\{\inl(x) \Rightarrow p ; \, \inr(y) \Rightarrow q\}\right]. 
  \end{align*}
  


  \begin{comment}
  Se eu sei que [ t . i1 , t . i2 ] = t


então a unicidade é automaticamente provada

Porque assumindo que temos dois morfismos t e s tal que t . i1 = f = s .i1 e t . i2 = g = s . i2

automaticamente t = s


t = [ t . i1, t . i2 ] = [f ,g] = [ s . i1 , s . i2] = s
  \end{comment}


 


 \end{proof}

 \begin{remark}
  Although in higher-order lambda calculus the distributive property follows directly from the fact that the syntactic category is closed, this is not the case for first-order lambda calculus, where the category is not required to be closed. Nevertheless, we will show that the distributive morphism is an isomorphism in $\catSyn(T)$, thereby establishing completeness for first-order lambda calculus. Note that our treatment of first-order $\lambda$-calculus implicitly assumes that the underlying categories of the models are distributive.
 \end{remark}

 \begin{proposition}
  Consider a first-oder linear $\lambda$-theory $T$. Then an equation 
$\Gamma \, \vljud \, v = w : \typeA$
is a theorem of $T$ if and only if it is satisfied by all first-order models of the theory.
 \end{proposition}

 \begin{proof}

  In the syntatic category the distributive property, $\dist$, corresponds to the class 
  \begin{align*}
    \big[z:(\typeA \oplus \mathbb{\typeB}) \otimes \mathbb{\typeD} \vljud \text{pm } z \text{ to } x \otimes d. \text{ case } x\,\{ &\inl_{\typeB}(a) \Rightarrow \inl_{\typeB \otimes \typeD}(a \otimes d); \\
    & \inr_{\typeA}(b) \Rightarrow \inr_{\typeA \otimes \typeD}(b \otimes d/z')\} \\
    & \hspace{45pt}: (\typeA \otimes \typeD) \oplus (\typeB \otimes \typeD) \big].
  \end{align*} 
  The fact that the distributivity morphism is an isomorphism follows by
  proving that the distributivity morphism  is a copairing (by reasoning similar to the previous case), together with the well-known categorical result that coproducts are unique up to isomorphism (\autoref{prop:cop_unique_iso}). We note that there is an additional step consisting of verifying that \(\dist\) satisfies the identities
\[
\dist \cdot \inl' = \inl \quad \text{and} \quad \dist \cdot \inr' = \inr,
\]
where $\inl', \inr'$ are the injections of this distributivity morphism. This is immediate once it is established that \(\dist\) a copairing.

This reduces to proving that the class
 \begin{align*}
    \big[z:(\typeA \otimes \mathbb{\typeB}) \otimes \mathbb{\typeD} \vljud \text{pm } z \text{ to } x \otimes d. \text{ case } x\,\{ &\inl_{\typeB}(a) \Rightarrow p[a \otimes d/z']; \\
    & \inr_{\typeA}(b) \Rightarrow q [b \otimes d/z']\}: \typeE \big],
  \end{align*} 
equipped with the injections
 \begin{align*}
    & \left[ x: \typeA \otimes \typeD \vljud\text{pm }x \text{ to } a\otimes d. \, \inl_{\typeB}(a) \otimes d : (\typeA \oplus \typeB) \otimes \typeD   \right],\\
    & \left[ x: \typeB \otimes \typeD \vljud\text{pm }x \text{ to } b\otimes d. \, \inr_{\typeA}(b) \otimes d : (\typeA \oplus \typeB) \otimes \typeD   \right]
  \end{align*}
is a copairing, $[p,q]$.

The proof of the coproduct diagram comutes follows  from equations $\eta_{\otimes_e}$, $\beta_{\otimes_e}$, $\beta_{\text{case}}^{\text{inl}}$, and  $\beta_{\text{case}}^{\text{inr}}$. Specifically, for the left triangle in the coproduct diagram, we have:
  \begin{align*}
    &  \hspace{-30pt} \left[\text{pm } z \text{ to } x \otimes d. \text{ case } x\, \left\{ \begin{aligned}
      & \inl(a) \Rightarrow p[a \otimes d/z'];\\
      & \inr(b) \Rightarrow q [b \otimes b/z']
    \end{aligned}
      \right\}
      \right] \comp \left[\text{pm }x \text{ to } a\otimes d. \, \inl(a) \otimes d \right]  \\
    &  \hspace{-30pt}  = \left[ \text{pm } (\text{pm }x \text{ to } a\otimes d. \, \inl(a) \otimes d ) \text{ to } x' \otimes d' . \, \text{ case } x'\, \left\{ \begin{aligned}
      & \inl(a') \Rightarrow p[a' \otimes d'/z'];\\
      & \inr(b) \Rightarrow q [b \otimes d'/z']
    \end{aligned} \right\} \right] \\
    &   \hspace{-30pt} = \left[ \text{pm } \inl(a) \otimes d  \text{ to } x' \otimes d' [x/ a \otimes d]. \, \text{ case } x'\, \left\{ \begin{aligned}
      & \inl(a') \Rightarrow p[a' \otimes d'/z'];\\
      & \inr(b) \Rightarrow q [b \otimes d'/z']
    \end{aligned} \right\} \right] & (\eta_{\otimes_e})  \\
     &   \hspace{-30pt} = \left[\text{ case } \inl(a)\, \left\{ \begin{aligned}
      & \inl(a') \Rightarrow p[a' \otimes d/z'];\\
      & \inr(b) \Rightarrow q [b \otimes d/z']
    \end{aligned} \right\} [x/ a \otimes d] \right] & (\beta_{\otimes_e})  \\
    &   \hspace{-30pt} = \left[ p \, [a \otimes d/z'] [x/ a \otimes d] \right]& (\beta_{\text{case}}^{\text{inl}}) \\
       & \hspace{-30pt} = \left[ p \right]
  \end{align*}
  The proof for the right triangle in the coproduct diagram is analogous.

  To prove unicity, we first establish that the following equality, known  as the \emph{syntactic fusion law}, holds:
  \begin{equation*}
   v \left[ \left(\text{case } a \,\{\inl_{\typeB}(x) \Rightarrow w ; \, \inr_{\typeA}(y) \Rightarrow u\}\right)  / z \right]  =  \text{case } a \,\{\inl_{\typeB}(x) \Rightarrow v[w/z] ; \, \inr_{\typeA}(y) \Rightarrow v[u/z]\}.
  \end{equation*}
This equality follows from the extensionality of the copairing and equations  $\beta_{\text{case}}^{\text{inl}}$, and $\beta_{\text{case}}^{\text{inr}}$. 
By the extensionality of the copairing, it suffices to prove the equility for  $\text{inl}(b)$ and $\text{inr}(c)$.
We present the explicit proof for $\text{inl}(b)$; the proof for  $\text{inr}(c)$ follows similar reasoning.
\begin{equation*}
\begin{split}
  & v \left[ \left(\text{case } \text{inl}(b) \,\{\inl(x) \Rightarrow w ; \, \inr(y) \Rightarrow u\}\right)  / z \right] & \\
  & =  v [w[b/x]\,/z] & (\beta_{\text{case}}^{\text{inl}})\\
  & =   v [[w/z]\,[b/x]]    \\
   & =  \text{case } \text{inl}(b) \,\{\inl(x) \Rightarrow v[w/z] ; \, \inr(y) \Rightarrow v[u/z]\}   & (\beta_{\text{case}}^{\text{inl}})
\end{split}
\end{equation*}

Next, to establish that this morphism is unique, we establish that given any morphism $m$ from $(\typeA \otimes \typeB) \otimes \typeD $ to $\typeE$, $z: (\typeA \otimes \typeB) \otimes \typeD \vljud m : \typeE $ , we have
  \begin{align*}
   & \hspace{-30pt} \left[\text{pm } z \text{ to } x \otimes d. \text{ case } a\, \left\{ \begin{aligned}
      & \inl(a) \Rightarrow m[\inl(a) \otimes d/z];\\
      & \inr(b) \Rightarrow m [\inr(b) \otimes d/z]
    \end{aligned}
      \right\}
      \right] \\
    & \hspace{-30pt} =   \left[ \text{pm } z \text{ to } x \otimes d. \, m \left[\text{case } x\, \left\{ \begin{aligned}
      & \inl(a) \Rightarrow \inl(a) \otimes d;\\
      & \inr(b) \Rightarrow \inr(b) \otimes d
    \end{aligned}  \right\}/z \right] \right]& (\text{syntactically fusion law}) \\
    & \hspace{-30pt} = \left[ \text{pm } z \text{ to } x \otimes d. \, m \left[\text{case } x\, \left\{ \begin{aligned}
      & \inl(a) \Rightarrow y \otimes d \, [\inl(a)/y];\\
      & \inr(b) \Rightarrow  y \otimes d \, [\inr(b)/y]
    \end{aligned}  \right\}/z \right] \right] &  \\
    & \hspace{-30pt} =  \left[\text{pm } z \text{ to } x \otimes d. \, m \left[ y \otimes d \, [x/y] \,/z \right]  \right]& (\eta_{\text{case}})  \\ 
    & \hspace{-30pt} = \left[ m[x \otimes d/z][z/x\otimes d]\right]  & (\eta_{\otimes_e}) \\
    & \hspace{-30pt} = [m]
  \end{align*}

  Finally, observe that,
  \begin{align*}
    & \left[m [\text{pm } x \text{ to } a\otimes d. \, \inl(a) \otimes d/ z]  \right] \\
    & =  \left[ \text{pm }x \text{ to } a\otimes d. m[\inl(a) \otimes d/ z] \right] & (c_{\otimes_e})\\
    & =  \left[ m[\inl(a) \otimes d/ z] [a \otimes d/ x] \right] & (\eta_{\otimes_e})
  \end{align*}
  Similarly we obtain $\left[m [\text{pm } x \text{ to } b\otimes d. \, \inr(b) \otimes d/ z]  \right] = \left[m [ \, \inr(b) \otimes d/ z] [b \otimes d/ x]  \right]. $

 

 \end{proof}

\begin{example}
  We now illustrate how the programs presented in Examples \ref{example:prog_swap} and \ref{example:prog_Dis2nd}, with slight modifications, are interpreted in $\catSet$.
   To this effect, we consider a type $N$ representing the set of natural numbers, along with a family of operations $\{n : \typeI \to \emph{N} \mid n \in \mathbb{N}\}$, each mapping the monoidal unit to a corresponding natural number $n$.
   We consider yet another operation $\text{dis}$ that marks elements of of type $N$ as discardable, $\text{dis}: N \to \typeI $.
  In $\catSet$ we have $\sem{\typeI}= \{*\}$, and define $\sem{N} = \mathbb{N}$,   $\sem{n} = \{*\} \to \mathbb{N}, \, * \mapsto n,$ and $\sem{\text{dis}}=!$, where $!$ denotes the terminal map. 
   Consider the following $\lambda$-term:
   \begin{equation*}
   \begin{split}
    &   x : N \otimes N \triangleright \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a : N \otimes N 
  \end{split}
  \end{equation*}
  
   Attending to \autoref{fig:denotational_sem} and the coherence theorem for symmetric monoidal categories this program is interpreted as follows:
   \begin{align*}
   &  \sem{ \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a } \\
   & \triangleq \sem{b \otimes a} \cdot \jn \cdot \alpha \cdot \sw \cdot \left( \sem{x} \otimes \id\right) \cdot \spt_{N \otimes N;-}   \\
   & = \sw \cdot \spt_{N;N} \cdot \jn_{\typeI; N; N} \cdot \alpha \cdot \sw_{N \otimes N;\typeI} \cdot \spt_{N \otimes N;-} \\
   & = \sw  & ( \text{coherence theorem}) 
   \end{align*}

   Next, consider the  $\lambda$-terms below.

   \begin{equation*}
    \begin{split}
    & \textbf{Dis2nd} \triangleq - \triangleright \lambda x: N \otimes \typeI. \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1 pt} b \hspace{3 pt} \text{ to } *.a: N \otimes N \multimap N \\
    & \textbf{Dis2nd} \,(1 (*) \otimes *) \\
    \end{split}
   \end{equation*}

   \begin{comment}
     &  \textbf{Dis1stOR2nd} \triangleq - \triangleright  \lambda y: N \otimes N . \, 
\text{case } \text{inl} (*) \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeB}(w) \Rightarrow  \textbf{Dis1st} \, x; \\
    &\inr_{\typeB}(z) \Rightarrow  \textbf{Dis2nd} \, x   \\ 
  \end{aligned}  
  \right\} : N \otimes N \multimap N  & \\
  & \textbf{Dis1stOR2nd} \, (1 (*) \otimes 2(*)) : N 
    \end{comment}

  In this case, we will coordinate the use of the equational system with the semantics, thus illustrating the synergy that both create. First, applying equations $\beta$ and $\beta_{\otimes_e}$ we have:
   \begin{equation*}
    \begin{split}
    \textbf{Dis2nd} \,(1 (*) \otimes 2(*)) & \triangleq \textbf{Dis2nd} \,[1 (*) \otimes 2(*)/x] \\
    & = \text{pm} \hspace{3 pt}  1 (*) \otimes 2(*) \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1 pt} \text{dis}(b) \hspace{3 pt} \text{ to } *.a \\
    & = \text{dis}(2(*)) \hspace{3 pt} \text{ to } *. 1 (*) 
\end{split}
   \end{equation*}
    The resulting program is interpreted as follows:
    \begin{align*}
   &  \sem{ \text{dis}(2(*)) \hspace{3 pt} \text{ to } *. 1 (*)} \\
    & = \llbracket 1(*) \rrbracket \cdot \lambda \cdot \sem{\text{dis}(2(*))} \otimes \id \cdot  \lambda^{-1}  \\
   &  =  \llbracket 1(*) \rrbracket \cdot \lambda \cdot (! \cdot \llbracket 2(*) \rrbracket \otimes \id) \cdot \lambda^{-1} \\
   &  =  \llbracket 1(*) \rrbracket \cdot \lambda \cdot (!  \otimes \id) \cdot \lambda^{-1} \\
   & = \llbracket 1(*) \rrbracket \cdot   \lambda \cdot  \id \cdot  \lambda^{-1} \\
   & =  \llbracket 1(*) \rrbracket 
   \end{align*}

   
\end{example}


\subsection{Semantics of metric equations} \label{subsec:semantics_metric}
 We will now turn our attention to the semantics of the metric equations. First, we recall the definitions of a metric space and of the category of metric spaces.

 \begin{definition} \label{def:metric_space}
    A \emph{metric space} is a pair $(X, d)$ where $X$ is a set and $d: X \times X \to [0, \infty]$ is a function known as \emph{distance} satisfying:
    \begin{enumerate}
        \item $0 \leq d(x, y)$, with equality if and only if $x = y$,
        \item $d(x, y) = d(y, x)$,
        \item $d(x, z) \leq d(x, y) + d(y, z)$ for all $x, y, z \in X$.
    \end{enumerate}
    A \emph{pseudometric space} satisfies the same axioms, except that the first condition condition is weakened: \(d(x, y) = 0\) may hold even when \(x \neq y\).
  \end{definition}

  \begin{definition}
    $\catMet$ denotes the category whose objects are metric spaces and whose morphisms are non-expansive maps, \ie, functions that do not increase the distance between points. More precisely, for two metric spaces $(X, d_X)$ and $(Y, d_Y)$, a morphism $f: (X, d_X) \to (Y, d_Y)$ is a function $f: X \to Y$ such that
$$
d_Y(f(x), f(y)) \leq d_X(x, y) \quad \text{for all } x, y \in X.
$$
  \end{definition}
 
 Here, we equip each hom-set $\catC(A, B)$ of a category $\catC$ with a metric $d_{A,B}$, and impose that both postcomposition and precomposition are non-expansive. That is, for all morphisms $f,f_1, f_2 \in \catC(A, B)$ and any $g, g_1, g_2 \in \catC(B, C)$, the following inequalities holds:
$$
d_{A,C}(g \circ f_1, g \circ f_2) \leq d_{A,B}(f_1, f_2) \hspace{40pt} d_{A,C}(g_1 \circ f, g_2 \circ f) \leq d_{B,C}(g_1, g_2).
$$
Note that, given the triangle inequality, we have:
\begin{equation*}
  d_{A,C}(g_1 \circ f_1, g_2 \circ f_2) \leq  d_{A,C}(g_1 \circ f_1, g_1 \circ f_2) + d_{A,C}(g_1 \circ f_2, g_2 \circ f_2) \leq  d_{A,B}(f_1, f_2) +   d_{B,C}(g_1, g_2).
\end{equation*}

This is known as \emph{enriching} the category $\catC$ \emph{over metric spaces}. Accordingly, we often refer to such a category as being \emph{enriched over metric spaces}, or simply as a $\catMet$-category.

Following the same principle, we require that the tensor product be non-expansive, \ie, 
\[
  d_{A \otimes C, B \otimes D}(f_1 \otimes g_1, f_2 \otimes g_2) \leq d_{A,B}(f_1, f_2) + d_{C,D}(g_1, g_2).
\]
In the literature, such a tensor product is typically referred to as a \emph{functor enriched over metric spaces}, or simply a $\catMet$\emph{-functor}.
Similarly, we require the currying functor $A \multimap (-) \colon \catC \to \catC$ to be non-expansive.


Coproducts are not discussed in this context (for now), as they relate to the interpretation of the metric equation for conditionals, which will be the subject of the next chapter.

\vspace{5pt}

In this context, \emph{soundness} and \emph{completeness} concepts are extended to encompass not only the classical equations but also the metric equations. Recall that classical equations $v=w$ can be written as $v=_0w$. As a result, in this metric setting, we define 

\begin{itemize}  
  \item \emph{Soundness} as the property 
\[
 M =_{\varepsilon} N \;\Rightarrow\; d (\llbracket N \rrbracket, \llbracket M \rrbracket) \leq \varepsilon
\quad \text{for all interpretations in the class.}
\]
 That is, if two terms are provably at a maximum distance ${\varepsilon}$, so are their respective interpretations
\item \emph{Completeness} as the property 
\[
 d (\llbracket N \rrbracket, \llbracket M \rrbracket) \leq \varepsilon
\;\Rightarrow\; M =_{\varepsilon} N 
\quad \text{for all interpretations in the class.}
\]
 That is, if ${\varepsilon}$ is the maximum distance between the interpretations of two programs, then they are provably at a maximum distance ${\varepsilon}$.
\end{itemize}


\begin{definition} \label{def:model_metric_no_cond}
 Consider a metric $\lambda$-theory $((G,\Sigma),Ax)$ and a symmetric monoidal closed $\catMet$-category $\catC$, in which both the tensor product and the internal hom-functor (currying) are non-expansive. Suppose that for each $X \in G$ we have an interpretation $\llbracket X \rrbracket$ as a $\catC$-object and analogously for the operation symbols. This interpretation structure is a model of the theory if all axioms in $Ax$ are satisfied by the interpretation.
\end{definition}



\begin{theorem} [Soundness] \cite[Theorem 3.14]{dahlqvist2023syntactic}  \label{thm:soundness_metric_no_cond}
  The rules in Figures \ref{fig:equations-linear-lambda} and \ref{fig:metric deductive system} are sound for a  symmetric monoidal closed $\catMet$-category  $\catC$, in which both the tensor product and the internal hom-functor (currying) are non-expansive. Specifically, if $\Gamma \vljud v =_{q} w : \typeA $ results from the rules in Figures \ref{fig:equations-linear-lambda} and \ref{fig:metric deductive system} then $q \geq d( \llbracket \Gamma  \vljud v : \typeA \rrbracket, \llbracket\Gamma \vljud w : \typeA \rrbracket)$.
\end{theorem}


Next, we will provide a proof sketch of the completeness result in \cite{dahlqvist2023syntactic} so the reader gets a general feeling of what it requires.

For two types $\typeA$ and $\typeB$ of a metric $\lambda$-theory $T$, consider the set $\texttt{Values}(\typeA,\typeB)$ of values $v$ such that $x : \typeA \vljud v : \typeB$. We equip $\texttt{Values}(\typeA,\typeB)$ with the function $d :\texttt{Values}(\typeA,\typeB) \times \texttt{Values}(\typeA,\typeB) \rightarrow [0, \infty]$ defined by,
$$d(v,w)=\inf{\{q \, \vert \, v=_q w \text{ is a theorem of } T \}}.$$


Given that the equations $\Gamma \vljud v = w : \typeA$ are abbreviations of $\Gamma \vljud v =_0 w : \typeA$, $\texttt{Values}((\typeA,\typeB), d)$ is a pseudometric space, \ie, it allows distinct terms to have distance zero. Consequently, we quotient this space by the relation $\sim$ (identifying elements at distance zero) to obtain a metric space, denoted by $(\texttt{Values}(\typeA, \typeB), d)/{\sim}$, which is a $\catMet$-category.

Completeness arises from constructing the syntactic category $\catSyn(T)$ of the underlying  theory
$T$ and then showing that provability of $\Gamma \, \vljud \, v =_q w : \typeA$
 in $T$ is equivalent to $d(\sem{v},\sem{w}) \geq q$ in the category
  $\catfont{Syn}(T)$. We use the category (\texttt{Values}$(\typeA,\typeB),d)$/$\sim$ to this end.  Note that the quotienting process identifies all terms $x : \typeA \triangleright v : \typeB$ and $x : \typeA \triangleright w : \typeB$ such that $v =_0 w$ and $w =_0 v$. This relation includes the equations-in-context from \autoref{fig:equations-linear-lambda}. Then, the next step is to prove that this quotienting procedure is compatible with the term formation rules of the extended calculus. To this effect, one generally uses the fact that $+$ distributes over suprema. This yields the desired category $\catfont{Syn}(T)$ which will correspond to a symmetric monoidal closed $\catMet$-category $\catC$, in which both the tensor product and the internal currying are non-expansive. The final step is to show that if an equation $\Gamma \vljud v =_q v' : \typeA$ with $q \in [0, \infty]$ is satisfied by Syn$(T)$, then it is a theorem of the linear metric $\lambda$-theory. Which follows from the strictly greater relation and rules (join), (weak), and (arch).




\begin{theorem}[Completeness] \cite[Theorem 3.16]{dahlqvist2023syntactic} \label{thm:completeness_metric_no_cond}
Consider a  metric $\lambda$-theory. A metric equation in context
$\Gamma \vljud v =_{q} w : \typeA$
is a theorem if and only if it holds in all models of the theory.
\end{theorem}

