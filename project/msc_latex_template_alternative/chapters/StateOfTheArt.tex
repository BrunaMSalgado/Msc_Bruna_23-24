







%It should be noted that the concept of a norm, as well as the properties of some norms, are relevant here,  as the existence of a metric system implies that operators have a well-defined norm. 



%\section{Functional Analysis} \label{sec:funal}



 %Functional analysis can be viewed as the  branch of mathematics that studies how the properties of finite-dimensional vector spaces generalize in infinite dimensional spaces.

%As a result, 
%p.26/28




%p.57->Bruce
%p97/100 -> conway




%p110, p.85


%\begin{lemma}
  %In the finite dimensional setting the norm topology and the weak topology coincide.
%\end{lemma}




%p.243 infinite dimensional analysis 

%IMPORTANTE:Since a finite dimensional space has only one Hausdorff linear topology, the norm topology and the weak topology must be the same.



%\section{Probabilistic Programming/Measure theory} \label{sec:pp}
 






%\subsection{Probabilistic Programming and Measure Theory}
% livro probabilistic programming 

% Probabilistic programs describe recipes for inferring statistical conclusions from a complex mixture of uncertain data and real-world observations. They can represent probabilistic graphical models are expected to have a major impact on machine intelligence. Probabilistic programs are ubiquitous. They steer autonomous robots and self-driving cars, are key to describe security mechanisms, naturally code up randomised algorithms for solving NP-hard or even unsolvable problems, and are rapidly encroaching on AI. Probabilistic programming aims to make probabilistic modelling and machine learning accessible to the programmer.


% INTRO Medidas e espaços mensuraveis 



  


   







%\todo[inline,size=\normalsize]{Normas} 

% def de valor abs p.133 (2.4) measure_theoritic_probability_by_athreya__lahiri.

%Lebesgue decomposition p.360 - measure theory


%\section{C* and W*-Algebras} \label{sec:c*_w*}

%Proposition 5.13. theory of op algebras 1 Takesaki -> f \otimes g é cp se f e g são cp





\todo[inline,size=\normalsize]{Colocar um frase bonitinha sobre como isto acaba por ser tb um resumo do que eu aprendi e portanto são apresentados os conceitos de raiz} 

\chapter{Metric Lambda Calculus} \label{ch:metriclambda}

\todo[inline,size=\normalsize]{Boolenos->Acrescentar reparo do dis(w)=*}

The Lambda Calculus, developed by Church and Curry in the 1930s, serves as a formal language capturing the key attribute of higher-order functional languages, treating functions as first-class citizens, allowing them to be passed as arguments \cite{barendregt1984lambda}.  Moreover, lambda calculus has been proven to be universal in the sense that any computable function can be represented as an expression within the language \cite{bernays1936alonzo} . Beyond its foundational aspects, this calculus incorporates extensions for modeling side effects, including probabilistic or non-deterministic behaviors and shared memory.  Higher-order functions form a pivotal abstraction in practical programming languages such as LISP, Scheme, ML, and Haskell.

%The metric lambda calculus incorporates a metric equational system, enabling reasoning about approximate program equivalence.

This chapter introduces the metric lambda calculus as presented in \cite{dahlqvist2022syntactic} drawing also from \cite{mackieLanguageAutonomous1993,croleCategoriesTypes1994,selinger2013lecture}. The metric lambda calculus integrates notions of
approximation into the equational system of affine lambda calculus, a variant of lambda calculus that restricts each variable to being used at most once.  This chapter offers a brief insight into lambda calculus and an overview of the syntax, metric equational system and interpretation of the metric lambda calculus. It also includes an exposition of the definitions and results from category theory used throughout the remainder of the thesis,  based on \cite{yanofskyMonoidalCategoryTheory2024,barrCategoryTheoryComputing1990,maclane13}.  These categorical notions play a central role, as programs are interpreted within distributive symmetric monoidal (closed) categories.
 It is worth noting that we explicitly prove certain results concerning conditionals; although these results are well known, their proofs appear to be absent from the existing literature. Additionally, we illustrate the usefulness of the ``traditional'' equational system by utilizing it to demonstrate that the terms $\inl(*)$ and $\inr(*)$ possess certain properties that are characteristic of booleans in classical logic.
For a more detailed study of lambda calculus theory, the reader is referred to \cite{barendregt1984lambda}.

 
%We then illustrate the use of the language for describing quantum and probabilistic programs.
%For structural reasons, soundness and completeness with respect to the ``traditional'' equational system are discussed in this chapter, while the corresponding notions for the metric $\lambda$-theory are addressed in the following chapter. 



\section{The Lambda Calculus}

The concept of a function takes a central role in the lambda calculus. But what exactly is a function?  In most mathematics, the “functions as graphs” paradigm is the most elegant and appropriate framework for understanding functions. Within this paradigm, each function $f$ has a fixed domain $X$ and a fixed codomain $Y$. The function $f$ is then a subset of $X \times Y$ that satisfies the property that for each $x \in X$ there is a unique $y \in Y$ such that $(x,y) \in f$. Two functions $f$ and $g$ are equal if they yield the same output on each input, that is if $f(x) = g(x)$ for all $x \in X$. This perspective is known as the extensional view of functions, as it emphasizes that the only observable property of a function is how it maps inputs to outputs.

On the other hand, the “functions as rules” paradigm is more appropriate within computer science. In this context, defining a function involves specifying a rule or procedure for computing the function. Such a rule is often expressed in the form of a formula, for example, \( f(x) = x^2 \). As with the mathematical paradigm, two functions are considered extensionally equal if they exhibit the same input-output behavior. However, this view also introduces the notion of intensional equality: two functions are intensionally equal if they are defined by (essentially) the same formula.


In the lambda calculus, functions are described explicitly as formulae. The function $f:x \mapsto f(x)$ is represented as $\lambda x.f(x)$.  Applying a function to an argument is done by juxtaposing the two expressions. For instance consider the function $f:x \mapsto x+1$, to compute $f(2)$ one writes $(\lambda x.x+1)(2)$.

The expression of higher‑order functions - functions whose inputs and/or outputs are themselves functions- in a simple manner is an essential feature of lambda calculus. For example, the composition operator $f,g \mapsto f \circ g$ is written as $\lambda f. \lambda g. \lambda x. f(g(x))$. Considering the functions $f:x \mapsto x^2$ and $g:x \mapsto x+1$, to compute $(f \circ g)(2)$ one writes $$(\lambda f. \lambda g. \lambda x. f(g(x)))(\lambda x.x^2)(\lambda x.x+1)(2).$$

As mentioned above,  within  the “functions as rules” paradigm, is not
always necessary to specify the domain and codomain of a function in advance. For instance, the identity function $f: x \mapsto x$, can have any set $X$ as its domain and codomain, provided that the domain and codomain are the same. In this case, one says that $f$ has type $X \rightarrow{} X$. In the case of the composition operator, $h=\lambda f. \lambda g. \lambda x. f(g(x))$, the domain and codomain of the functions $f$ and $g$ must match. Specifically, $f$ can have any set $X$ as its domain and any set  $Y$ as its codomain, provided that $Y$ is the domain of $g$. Similarly, $g$ can have any set $Z$ as its codomain.  Thus,  $h$ has type $$(X \rightarrow{} Y) \rightarrow{} (Y \rightarrow{} Z) \rightarrow{} (X \rightarrow{} Z).$$ 

This flexibility regarding domains and codomains enables operations on functions that are not possible in ordinary mathematics. For instance, if $f = \lambda x.x$ is the identity function, then one has that $f(x) = x$ for any $x$. In particular, by substituting $f$ for $x$, one obtains $f(f) = (\lambda x.x)(f) = f$. Note that the equation $f(f) = f$ is not valid in conventional mathematics, as it is not permissible, due to set-theoretic constraints, for a function to belong to its own domain.

Nevertheless this remarkable aspect of lambda calculus, this work focuses on a more restricted version of the lambda calculus, known as the simply-typed lambda calculus. Here, each expression is always assigned a type, which is very similar to the situation in mathematics. A function may only be applied to an argument if the argument's type aligns with the function's expected domain. Consequently, terms such as $f(f)$ are not allowed, even if $f$ represents the identity function.




%In order to be able to manipulate lambda-terms more easily, one can associate a type to each lambda-term


\section{Syntax}

\subsection{Type system}

As previously mentioned, this work focuses on the simply-typed lambda calculus, this work focuses on the simply-typed lambda calculus, where each lambda term is assigned a \emph{type}. Unlike sets, types are \emph{syntactic} objects, meaning they can be discussed independently of their elements. One can conceptualize types as names or labels for set. The definition of the grammar of types for affine lambda calculus is as follows, where $G$ represents a set of ground types, is given by the following \acrfull{bnf} \cite{backus1960report}.
\begin{equation} \label{eq:grammartypes}
\centering
\hspace{95pt} \mathbb{A} ::= X \in G \hspace{3 pt} \vert \hspace{3 pt} \mathbb{I}  \hspace{3 pt}  \vert \hspace{3 pt} \mathbb{A}  \otimes  \mathbb{A} \hspace{3 pt}  \vert \hspace{3 pt} \mathbb{A}  \oplus  \mathbb{A} \hspace{3 pt} \vert  \hspace{3 pt}  \mathbb{A} \multimap  \mathbb{A}
\end{equation}
Note that this is an inductive definition. Ground types are things such as booleans, integers, and so forth. The type $\mathbb{I}$ is the unit/empty type with only one element. The type $\mathbb{A} \otimes \mathbb{A}$ corresponds to the tensor of two types. The type $\mathbb{A} \oplus \mathbb{A}$ can be seen as the coproduct/sum of types. Finally, the type $\mathbb{A} \multimap \mathbb{B}$ is the type of linear maps one type to another.

 %It can be intuitively understood as the disjoint union of two types, analogous to the disjoint union in set theory: an element of $\mathbb{A} \oplus \mathbb{B}$ is either an element of $\mathbb{A}$ or an element of $\mathbb{B}$, together with an indication which of the two it belongs to. 

%The type $\mathbb{A} \oplus \mathbb{B}$ can be intuitively understood as the disjoint union of two types, analogous to the disjoint union in set theory: an element of $\mathbb{A} \oplus \mathbb{B}$ is either an element of $\mathbb{A}$ or an element of $\mathbb{B}$, together with a tag indicating which of the two it belongs to.


\subsection{(Raw)Terms}


The expressions of the lambda calculus are called lambda terms. In the simply-typed lambda calculus, each lambda term is assigned a type. The terms without the specification of a type are called \emph{raw typed lambda terms}. The grammar of \emph{raw typed lambda terms} is given by the \acrshort{bnf} below.
\begin{equation*} \label{eq:grammarlambda}
\begin{split}
 v,v_1, \ldots, v_n,w \hspace{10 pt} ::= \hspace{10 pt}& x \hspace{3 pt} \vert \hspace{3 pt} f(v_1, \ldots, v_n) \hspace{3 pt} \vert \hspace{3 pt} *  \hspace{3 pt} \vert \hspace{3 pt} (\lambda x: \mathbb{A}. v )\hspace{3 pt} \vert \hspace{3 pt} (v w) \hspace{3 pt}  \vert \hspace{3 pt} v \otimes w \hspace{3 pt} \vert
 \\&    \text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} x \otimes y. w  \hspace{3 pt}  \vert \hspace{3 pt} v \hspace{3 pt} \text{ to } *.w \hspace{3 pt} \vert \hspace{3 pt} \text{dis}(v) \hspace{3 pt} \vert \hspace{3 pt} \text{inl} (v) \hspace{3 pt} \vert \hspace{3 pt} \text{inr} (v) \hspace{3 pt} \vert
 \\& \text{ case } v \,   \{\text{inl}_{\typeB} (x) \Rightarrow w ; \, \text{inr}_{\typeA} (y) \Rightarrow u\}
\end{split}
\end{equation*}

Here $x$ ranges over an infinite set of variables. $f \in \Sigma$, where  $\Sigma$ corresponds to a class of sorted operation symbols, and $f(v_1, \ldots, v_n)$ corresponds to the aplication of the function $f$ to the arguments $v_1, \ldots, v_n$. The symbol $*$ is the unit element of the type $\mathbb{I}$. The term $(\lambda x: \mathbb{A}. v )$ is the lambda abstraction term, which represents a function that takes an argument of type $\mathbb{A}$ and returns the value of $v$. The term $(v w)$ is the application term, which applies the function $v$ to the argument $w$.  The term $v \otimes w$ is the tensor product of $v$ and $w$. The term $\text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} x \otimes y. w$ is the pattern-matching construct, which is used to deconstruct a tensor product into components $x$ and $y$. The term $v \text{ to } *.w$ is used to discard a variable $v$ of the unit type. The term $\text{dis}(v)$ is the discard term, which is used to discard a term $v$. The terms $\text{inl}_{\typeB}(v)$ and $\text{inr}_{\typeA}(v)$ represent the left and right injections of $v$, respectively. Intuitively, the case statement executes $w$ when $v$ is a left injection, and $u$ when $v$ is a right injection, and a ``mixture'' of both otherwise.

%evaluates $v$ and stores the value of the right and left components in variables $x$ and $y$, respectively, and then 


%Then, based on (the left and right injections of) these values, 


%These serve a similar role to the traditional boolean values ``true'' and ``false'', as illustrated by the case statement, which evaluates $v$ and stores the value of the right and left components in variables $x$ and $y$, respectively. Then, we use the left and right injections of  $x$ and $y$, respectively, as if they were ``true'' and ``false'', respectively, in the sense that we evaluate 

 %to $w$ when $v$ is a left injection, and to $u$ when $v$ is a right injection. 

%? The term in1M is simply an elementof the left component of A + B.
%The term (caseM ofxA ⇒ N |yB ⇒ P) is a case distinction: evaluate M of type A + B. The answer is either an element o 
%the left component A or of the right component B. In the first case, assign the answer to the variable x and evaluate N. In the second case, assign the answer to the variable y and evaluate P. Since both N and P are of type C, we get a final result of type C. 

\todo[inline,size=\normalsize]{Ver o que por antes do ::= porque v1,..., vn tb são termos } 

\subsection{Free and Bound Variables}
An occurrence of a variable $x$ within a term of the form $\lambda x.v$ is referred to as \emph{bound}.  Similarly, the variables $x$ and $y$ in the term $\text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} x \otimes y. w$ are also bound. A variable occurrence that is not bound is said to be \emph{free}. For example, in the term $\lambda x.xy$, the variable $y$ is free, whereas the variable $x$ is bound.  

The set of free variables of a term $v$ is denoted by \gls{fv}, and is defined inductively as follows:

\begin{comment}
\begin{equation*}
\begin{split}
FV(x) &= \{x\}, &  FV(*) &= \emptyset,  \\
FV(f(v_1, \ldots, v_n)), &= FV(v_1) \cup \ldots \cup FV(v_n)& FV(\lambda x: \mathbb{A}. v) &= FV(v) \backslash \{x\}, \\
FV(v w) &= FV(v) \cup FV(w), & FV(v \otimes w) &= FV(v) \cup FV(w), \\
FV(\text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} x \otimes y. w), &= FV(v) \cup (FV(w)  \backslash \{x,y\}) & FV(\text{dis}(v)) &= FV(v),\\
FV(v \text{ to } *.w) &= FV(v) \cup FV(w)  &  FV(\text{inl}_\typeB) &=  FV(v). \\
\end{split}
\end{equation*}
\end{comment}

\begin{equation*}
\begin{split}
&FV(x) = \{x\}, &&  FV(*) = \emptyset,  \\
&FV(f(v_1, \ldots, v_n)), = FV(v_1) \cup \ldots \cup FV(v_n)&& FV(\lambda x: \mathbb{A}. v) = FV(v) \backslash \{x\}, \\
&FV(v w) = FV(v) \cup FV(w), && FV(v \otimes w) = FV(v) \cup FV(w), \\
& FV(\text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} x \otimes y. w), = FV(v) \cup (FV(w)  \backslash \{x,y\}) && FV(\text{dis}(v)) = FV(v),\\
&FV(v \text{ to } *.w) = FV(v) \cup FV(w)  &&  FV(\text{inl}_\typeB (v)) =  FV(\text{inr}_\typeA (v)) = FV(v). \\
\end{split}
\end{equation*}
\vspace{-20pt}
\begin{equation*}
  \begin{split}
& \hspace{-30pt} FV(\text{case } v \,   \{\text{inl}_{\typeB} (x) \Rightarrow w ; \, \text{inr}_{\typeA} (y) \Rightarrow u\}) = FV(v) \cup  (FV(w) \backslash \{x\}) \cup (FV(u)\backslash \{y\})
\end{split}
\end{equation*}


\subsection{Term formation rules}

To prevent the formation of nonsensical terms within the context of lambda calculus, such as $(v \otimes w) (u)$, the \emph{typing rules} are imposed.

A \emph{typed} term is a pair consisting of a term and its corresponding type. The notation \gls{typed-term} denotes that the term $v$ has type $\mathbb{A}$. Typing rules are formulated using \emph{typing judgments}. A typing judgment is an expression of the form $x_{1}: \mathbb{A}_{1}, \ldots, x_{n}: \mathbb{A}_{n} \hspace{1pt} \triangleright \hspace{1pt} v: \mathbb{A}$ (where $n \geq 1$), which asserts that the term $v$ is a well-typed term of type $\mathbb{A}$ under the assumption that each variable variable $x_{i}$ has type $\mathbb{A}_{i}$, for $1 \leq i \leq n$. The list $x_{1}: \mathbb{A}_{1}, \ldots, x_{n}: \mathbb{A}_{n}$ of typed variables is called the \emph{typing context} of the judgment, and it might be empty.  Each variable $x_i$ (where $1 \leq i \leq n$) must occur at most once in $x_1, \ldots, x_n$. The typing contexts are denoted by Greek letters \gls{typingcontexts}, and from now on, when referring to an abstract judgment, the notation \gls{judgement} will be employed.
 The empty context is denoted by $-$. Note that in the affine lambda calculus, different contexts do not share variables. For example, if $\Gamma = x:\mathbb{A},y:\mathbb{B}$ none of these variables can appear in any other context. 


The concept of \emph{shuffling} is employed to construct a linear typing system that ensures the admissibility of the exchange rule and enables unambiguous reference to judgment's denotation $[\![ \Gamma \triangleright v: \mathbb{A} ]\!]$. An admissible rule is not explicitly included in the formal definition of type theory, but its validity can be proven by demonstrating that whenever the premises can be derived, it is possible to construct a derivation of its conclusion. Shuffling is defined as a permutation of typed variables in a sequence of contexts, $\Gamma_1, \ldots, \Gamma_n$, preserving the relative order of variables within each $\Gamma_i$ \cite{shulman2019practical}. For instance, if $\Gamma_1=x:\mathbb{A}, y:\mathbb{B}$ and $\Gamma_2=z:\mathbb{D}$, then $z:\mathbb{D}, x:\mathbb{A}, y:\mathbb{B}$ is a valid shuffle of $\Gamma_1, \Gamma_2$. On the other hand, $y:\mathbb{B}, x:\mathbb{A}, z:\mathbb{D}$ is not a shuffle because it alters the occurrence order of $x$ and $y$ in $\Gamma_1$. The set of shuffles in $\Gamma_1, \ldots, \Gamma_n$ is denoted as $\text{Sf} (\Gamma_1, \ldots, \Gamma_n)$. A valid typing derivation is constructed using the inductive rules shown in \autoref{fig:typing_rules_linear}.
%An admissible rule is not explicitly included in the formal definition of the type theory, but it can be proven valid using the fact that whenever one has derivations of its premises it is possible to construct a derivation of its conclusion.
%an admissible rule is one that is not asserted as part of the specification of the type theory, but for which we can prove after the fact that whenever we have derivations of its premises we can construct a derivation of its conclusion — usually by inductively traversing and modifying the given derivations of its premises. 
\begin{figure} [H]
  \small{
\begin{equation*}
\begin{split}
\begin{aligned}
& \hspace{-20pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \Gamma_{i} \triangleright v_{i}: \mathbb{A}_{i} \quad f: \mathbb{A}_{1}, \ldots, \mathbb{A}_{n} \xrightarrow{} \mathbb{A} \in \Sigma \quad E \in \text{Sf}(\Gamma_{1}; \ldots; \Gamma_{n})\\
    \hline
   E \triangleright f( v_{1},\ldots,v_{n}): \mathbb{A}
\end{array}
$
\end{minipage}
\hspace{148pt}
\text{(ax)} 
 \hspace{30pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
      \\
    \hline
   x:\mathbb{A} \triangleright x:\mathbb{A}
\end{array}
$ \end{minipage}
\hspace{-68pt} \text{(hyp)} \\
& \hspace{-20pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    \\
    \hline
   - \triangleright *: \mathbb{I}
\end{array}
$
\end{minipage}
\hspace{-87pt}
\text{($\mathbb{I}_{i}$)} 
 \hspace{90pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \Gamma \triangleright v: \mathbb{A} \otimes \mathbb{B} \quad  \Delta,x: \mathbb{A}, y: \mathbb{B}  \triangleright w: \mathbb{D}  \quad E \in \text{Sf}(\Gamma;\Delta)\\
    \hline
   E\triangleright \text{pm } v \text{ to } x \otimes y. w :\mathbb{D}
\end{array}
$ \end{minipage}
\hspace{117pt} (\otimes_{e}) \\
& \hspace{-20pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \Gamma \triangleright v: \mathbb{A} \quad  \Delta \triangleright w: \mathbb{B}  \quad E \in \text{Sf}(\Gamma;\Delta) \\
    \hline
   E \triangleright v \otimes w: \mathbb{A} \otimes \mathbb{B} 
\end{array}
$
\end{minipage}
\hspace{41pt} (\otimes_{i}) 
 \hspace{35pt}
 \begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \Gamma \triangleright v: \mathbb{I} \quad  \Delta \triangleright w: \mathbb{A}  \quad E \in \text{Sf}(\Gamma;\Delta)  \\
    \hline
   E \triangleright v \text { to } *.w: \mathbb{A}  
\end{array}
$ \end{minipage}
\hspace{38pt} (\mathbb{I}_{e}) \\
& \hspace{-20pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \Gamma,x:\mathbb{A} \triangleright v: \mathbb{B} \\
    \hline
   \Gamma \triangleright \lambda x:\mathbb{A} . v: \mathbb{A} \multimap \mathbb{B} 
\end{array}
$
\end{minipage}
\hspace{-27pt} (\multimap_{i}) 
 \hspace{64pt}
 \begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \Gamma \triangleright v: \mathbb{A} \multimap \mathbb{B} \quad  \Delta \triangleright w: \mathbb{A}  \quad E \in \text{Sf}(\Gamma;\Delta)  \\
    \hline
   E \triangleright v w: \mathbb{B}  
\end{array}
$ \end{minipage}
\hspace{67pt} (\multimap_{e}) \\
& \hspace{-20pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \Gamma \triangleright v: \mathbb{A}  \\
    \hline
   \Gamma \triangleright \text{dis}(v):  \mathbb{I} 
\end{array}
$
\end{minipage}
\hspace{-67pt} (\text{dis})
\hspace{55pt}
    %
    \begin{prooftree}
        \hypo{\Gamma \vljud v: \typeA}
        \infer1[(inl)]{\Gamma \vljud \inl_{\typeB}(v): \typeA \oplus \typeB}
    \end{prooftree}
    %
    \hspace{55pt}
    %
    \begin{prooftree}
        \hypo{\Gamma \vljud v: \typeB}
        \infer1[(inr)]{\Gamma \vljud \inr_{\typeA}(v): \typeA \oplus \typeB}
    \end{prooftree} 
    %
    \\[10pt]
    &\hspace{20pt}
    %
    \begin{prooftree}
        \hypo{\Gamma \vljud v: \typeA \oplus \typeB}
        \hypo{\Delta, x: \typeA \vljud w: \typeD}
        \hypo{\Delta, y: \typeB \vljud u: \typeD}
        \hypo{E \in \Shuff(\Gamma; \Delta)}
        \infer4[(case)]{E \vljud \text{case } v\,
        \{\inl_{\typeB}(x) 
            \Rightarrow w ; \,
          \inr_{\typeA}(y) \Rightarrow u
        \}: \typeD}
    \end{prooftree}
\end{aligned}
\end{split}
\end{equation*}
  }
\caption{Term formation rules of affine lambda calculus.}
\label{fig:typing_rules_linear}
\end{figure}
The rule (ax) states that if there is a function $f \in \Sigma$ that has type $\mathbb{A}_1, \ldots, \mathbb{A}_n \rightarrow \mathbb{A}$ and a set of variables $v_1,\ldots, v_n$ whose types match the type of the arguments of $f$, then if that function is applied to $v_1,…,v_n$ the respective result is of type $\mathbb{A}$.
The rule (hyp) is a tautology: under the assumption that $x$ has type $\mathbb{A}$, $x$ has type $\mathbb{A}$. 
The rule ($\mathbb{I}_{i}$) asserts that the unit element $*$ always has type $\mathbb{I}$. 
The rule ($\multimap_i$) expresses that if $v$ is a term of type $\mathbb{B}$ with a variable $x$ of type $\mathbb{A}$, then $\lambda x:\mathbb{A} . v$ is a function of type $\mathbb{A} \multimap \mathbb{B} $. 
The rule $(\multimap_e)$ states that a function of type $\mathbb{A} \multimap \mathbb{B}$  can be applied to an argument of type $\mathbb{A}$  to produce a result of type $\mathbb{B}$. 
The rule $(\mathbb{\otimes}_i)$  asserts that if there is a term $v$ of type $\mathbb{A}$ and a term $w$ of type $\mathbb{B}$,  then the tensor of these terms is of type $\mathbb{A} \otimes \mathbb{B}$.
The rule $(\mathbb{\otimes}_e)$ expresses if there is a term $w$ of type $\mathbb{D}$ with variables $x$ and $y$ of types $\mathbb{A}$ and $\mathbb{B}$, respectively, and a term $v$ of type $\mathbb{A} \otimes \mathbb{B}$, then $v$ can be deconstructed into $x \otimes y$. 
The rule $(\mathbb{I}_e)$ states that if there is a term $w$ of type $\mathbb{A}$ and a term $v$ of type $\mathbb{I}$, then $v$ can be discarded, and only the term $w$ remains. 
The rule $(\text{dis})$ asserts that a term $v$ of type $\mathbb{A}$ can be discarded, resulting in a term of type $\mathbb{I}$. The rules $(\text{inl}_\typeB)$ (resp. $(\text{inr}_\typeA)$) states that if we inject a term $v$ of type $\typeA$ (resp. $\typeB$) into $\typeA \oplus \typeB$ we obtain a term of type $\typeA \oplus \typeB$. 
Finally, the rule $\text{(case)}$ states that if there are two programs of type $\typeD$ to be executed depending on the value of a term $v$ of type $\typeA \oplus \typeB$ (whose right and left components are assigned to variables $x$ and $y$, respectively), then the resulting program also has type $\typeD$.


For a better understanding of the rules, a few straightforward programming examples are provided.  

\begin{example} \label{example:prog_swap}

For instance, the program that swaps the elements of a tensor product can be written as follows:
\begin{comment}
\begin{equation*}
\begin{split}
 \textbf{SwapTensor} \triangleq & - \triangleright \lambda x: \mathbb{A \otimes B}. \text{pm} \hspace{3 pt} x\hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a : \mathbb{B} \otimes \mathbb{A}
\end{split}
\end{equation*}
Now, to prove that this program is well-typed one can write the following typing derivation:
\begin{equation*}
\begin{split}
1 \hspace{10 pt} & x : \mathbb{A} \triangleright x : \mathbb{A} \otimes \mathbb{B}   \hspace{10 pt} & {(\text{hyp})} \\
2 \hspace{10 pt} &  b : \mathbb{B} \triangleright   b : \mathbb{B} \hspace{10 pt}&{(\text{hyp})} \\
3 \hspace{10 pt} &   a : \mathbb{A} \triangleright  a : \mathbb{A} \hspace{10 pt}&{(\text{hyp})} \\
4 \hspace{10 pt} &   b : \mathbb{B},a : \mathbb{A} \triangleright b \otimes a : \mathbb{B} \otimes \mathbb{A} \hspace{10pt} &\text{($2,3,\otimes_i$)} \\
5\hspace{10 pt}& x : \mathbb{A} \otimes \typeB \triangleright \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a : \mathbb{B} \otimes \mathbb{A}& \hspace{10pt} \text{($4,\otimes_e$)} \\
6 \hspace{10 pt}& - \triangleright \lambda x: \mathbb{A \otimes B}. \text{pm} \hspace{3 pt} x\hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a : \mathbb{B} \otimes \mathbb{A} & \text{($5,\multimap_i$)}
\end{split}
\end{equation*}
\end{comment}
\begin{equation*}
\begin{split}
&  \textbf{SwapTensor} \triangleq x : \mathbb{A} \otimes  \mathbb{B}  \triangleright \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a : \mathbb{B} \otimes \mathbb{A}
\end{split}
\end{equation*}
Now, to prove that this program is well-typed one can write the following typing derivation:
\begin{equation*}
\begin{split}
1 \hspace{10 pt} & x : \mathbb{A} \otimes \mathbb{B} \triangleright   y : \mathbb{A} \otimes \mathbb{B} \hspace{10 pt} & {(\text{hyp})} \\
2 \hspace{10 pt} &  b : \mathbb{B} \triangleright   b : \mathbb{B} \hspace{10 pt}&{(\text{hyp})} \\
3 \hspace{10 pt} &   a : \mathbb{A} \triangleright  a : \mathbb{A} \hspace{10 pt}&{(\text{hyp})} \\
4 \hspace{10 pt} &   b : \mathbb{B},a : \mathbb{A} \triangleright b \otimes a : \mathbb{B} \otimes \mathbb{A} \hspace{10pt} &\text{($2,3,\otimes_i$)} \\
5\hspace{10 pt}& x : \mathbb{A},  y : \mathbb{B} \triangleright \text{pm} \hspace{3 pt} x \otimes y\hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a : \mathbb{B} \otimes \mathbb{A}& \hspace{10pt} \text{($1,4,\otimes_e$)}
\end{split}
\end{equation*}


Observe that in the notation of the third column, the numbers correspond to the premises utilized in the application of the rule.
\end{example}

\begin{example} \label{example:prog_Dis2nd}
Another example is the function that recieves a tensor product and returns first element, discarding the second:
\begin{equation*}
\begin{split}
& \textbf{Dis2nd} \triangleq - \triangleright \lambda x: \mathbb{A \otimes A}. \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1 pt} \text{dis}(b) \hspace{3 pt} \text{ to } *.a: \mathbb{A}
\end{split}
\end{equation*}
To prove that this program is well-typed one can write the following typing derivation:
\begin{equation*}
\begin{split}
1  \hspace{10 pt} & b : \mathbb{A} \triangleright b : \mathbb{A}  \hspace{10 pt} & {(\text{hyp})} \\
2 \hspace{10 pt} & b : \mathbb{A} \triangleright \text{dis}(b): \mathbb{I} \hspace{10 pt} & {(1,\text{dis})} \\
3 \hspace{10 pt} & a : \mathbb{A} \triangleright a : \mathbb{A}  \hspace{10 pt} & {(\text{hyp})} \\
4 \hspace{10 pt} &  a : \mathbb{A}, b : \mathbb{A}  \triangleright \text{dis}(b) \hspace{3 pt} \text{ to } *.a  & {(2,3,\mathbb{I}_{e})} \\
5 \hspace{10 pt} & x : \mathbb{A \otimes A} \triangleright x : \mathbb{A \otimes A}  \hspace{10 pt} & {(\text{hyp})} \\
6 \hspace{10 pt} & x : \mathbb{A \otimes A} \triangleright \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1 pt} \text{dis}(b) \hspace{3 pt} \text{ to } *.a : \mathbb{A} \hspace{10pt} & \text{($4,5,\otimes_{e}$)} \\
7 \hspace{10 pt} & - \triangleright \lambda x: \mathbb{A \otimes A}. \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1 pt} \text{dis}(b) \hspace{3 pt} \text{ to } *.a: \mathbb{A} \hspace{10pt} & \text{($6,\multimap_i$)}
\end{split}
\end{equation*}
\end{example}

\begin{example} \label{example:prog_Dis1stor2nd}
  Consider an analogous program to the previous example that that recieves a tensor product and returns
  second element, discarding the first: 
  \begin{equation*}
\begin{split}
& \textbf{Dis1st} \triangleq - \triangleright \lambda x: \mathbb{A \otimes A}. \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1 pt} \text{dis}(a) \hspace{3 pt} \text{ to } *.b: \mathbb{A \otimes A} \multimap \mathbb{A}
\end{split}
\end{equation*}
Following a similar line of reasoning as in the previous example, it is straightforward to verify that this $\lambda$-term is also well-typed.  
Now, let us examine a program that receives both a coproduct/sum and a tensor product, and applies either $\textbf{Dis1st}$ or $\textbf{Dis2nd}$ to the tensor product (or a mixture of both), depending on the value of the coproduct:
\begin{equation*}
\begin{split}
&  \textbf{Dis1stOR2nd} \triangleq - \triangleright  \lambda x: \mathbb{\typeB \oplus \typeB}. \, \lambda y: \mathbb{A \otimes A}. \, 
\text{case } x \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeB}(w) \Rightarrow \text{dis}(w) \text{ to} * . \, \textbf{Dis1st} \, y; \\
    &\inr_{\typeB}(z) \Rightarrow  \text{dis}(z) \text{ to} *. \textbf{Dis2nd} \, y   \\ 
  \end{aligned}  
  \right\}  & \\
\end{split}
\end{equation*}


To prove that this program is well-typed one can write the following typing derivation (the derivations regarding \textbf{Dis1st} and \textbf{Dis2nd} will be omitted):
\begin{equation*}
\begin{split}
1  \hspace{10 pt} & y : \mathbb{A} \otimes \mathbb{A}  \triangleright y : \mathbb{A}  \otimes \mathbb{A} \hspace{10 pt} & {(\text{hyp})} \\
2  \hspace{10 pt} & y : \mathbb{A}  \otimes \mathbb{A} \triangleright \textbf{Dis1st}   \, y : \mathbb{A} & {(\multimap_e)} \\
3  \hspace{10 pt} & y : \mathbb{A}  \otimes \mathbb{A}  \triangleright \textbf{Dis2nd}  \, y : \mathbb{A}  & {(\multimap_e)} \\
4  \hspace{10 pt} & w : \mathbb{B} \triangleright w : \mathbb{B}  \hspace{10 pt} & {(\text{hyp})} \\
5 \hspace{10 pt} & w : \mathbb{B} \triangleright \text{dis}(w): \mathbb{I} \hspace{10 pt} & {(4,\text{dis})} \\
6  \hspace{10 pt} & z : \mathbb{B} \triangleright z : \mathbb{B}  \hspace{10 pt} & {(\text{hyp})} \\
7 \hspace{10 pt} & z : \mathbb{B} \triangleright \text{dis}(z): \mathbb{I} \hspace{10 pt} & {(6,\text{dis})} \\
8 \hspace{10 pt} &  y : \mathbb{A}  \otimes \mathbb{A},  w : \mathbb{B}   \triangleright \text{dis}(w) \text{ to} * . \, \textbf{Dis1st} \, y : \typeA  & {(2,5,\mathbb{I}_{e})} \\
9 \hspace{10 pt} &   y : \mathbb{A}  \otimes \mathbb{A}, z : \mathbb{B}   \triangleright \text{dis}(z) \text{ to} * . \, \textbf{Dis2nd} \, y : \typeA & {(3,7,\mathbb{I}_{e})} \\
10 \hspace{10 pt} & x : \mathbb{B \oplus B} \triangleright x : \mathbb{B \oplus B}  \hspace{10 pt} & {(\text{hyp})} \\
11 \hspace{10 pt} &    x: \mathbb{\typeB \oplus \typeB},  y: \mathbb{A \otimes A} \triangleright \, 
\text{case } x \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeB}(w) \Rightarrow \text{dis}(w) \text{ to} * . \, \textbf{Dis1st} \, y; \\
    &\inr_{\typeB}(z) \Rightarrow  \text{dis}(z) \text{ to} *. \textbf{Dis2nd} \, y   \\ 
  \end{aligned}  
  \right\}  & \text{($8,9,10,\text{case}$)} \\
12 \hspace{10 pt} & x: \mathbb{\typeB \oplus \typeB} \triangleright \lambda y: \mathbb{A \otimes A}. \, 
\text{case } x \,  
  \left\{ \ldots 
  \right\}   & \text{($11,\multimap_i$)} \\
13 \hspace{10 pt} & - \triangleright  \lambda x: \mathbb{\typeB \oplus \typeB}. \, \lambda y: \mathbb{A \otimes A}. \, 
\text{case } x \,  
  \left\{ \ldots  
  \right\}   & \text{($12,\multimap_i$)}
\end{split}
\end{equation*}
\end{example}


\subsection{$\alpha$-equivalence}
 
A natural notion of equivalence definition stems from the fact that terms that differ only in the names of their bound variables represent the same program. For instance, the functions $\lambda x:\mathbb{A}.x $ and $\lambda y:\mathbb{A}.y$ have the same input-output behavior, despite being represented by different lambda terms. This equivalence is called $\alpha$\emph{-equivalence}.

\begin{definition}
  The $\alpha$-equivalence is an equivalence relation on lambda terms that is used to rename bound variables. To rename a variable $x$ as $y$ in a term $v$, denoted by $v\{y/x\}$, is to replace all occurrences of $x$ in $v$ by $y$. Two terms $v$ and $w$ are $\alpha$-equivalent, written $=_{\alpha}$, if one can be derived from the other by a series of
  changes of bound variables
\end{definition}

\begin{convention}
  Terms are considered up to $\alpha$-equivalence from now on.
\end{convention}

\subsection{Substitution}
The substitution of a variable $x$ for a term $w$ in a term $v$ is denoted by \gls{substitution}. It is only  permitted  to replace free variables. For instance, $\lambda x. \hspace{1pt} x  \hspace{2pt} [v/x]$ is $\lambda x. \hspace{1pt} x  \hspace{2pt}$ and not  $\lambda x. \hspace{1pt} v  \hspace{2pt}$. Moreover, it is necessary to avoid the unintended binding of free variables. For example, $$(\text{pm} \hspace{3 pt} x \otimes y\hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a \otimes z) \hspace{2 pt}  [z/\text{pm} \hspace{3 pt} c \otimes d\hspace{3 pt} \text{to} \hspace{3 pt} e \otimes f. \hspace{1pt} f \otimes e \otimes a]$$ is not the same as $$\text{pm} \hspace{3 pt} x \otimes y\hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a \otimes (\text{pm} \hspace{3 pt} c \otimes d\hspace{3 pt} \text{to} \hspace{3 pt} e \otimes f. \hspace{1pt} f \otimes e \otimes a ). $$ Instead, the bounded variable $a$ must be renamed before the substitution, and in this case, the proper substitution is $$(\text{pm} \hspace{3 pt} x \otimes y\hspace{3 pt} \text{to} \hspace{3 pt} t \otimes b. \hspace{1pt} b \otimes t \otimes z) \hspace{2 pt}  [z/\text{pm} \hspace{3 pt} c \otimes d\hspace{3 pt} \text{to} \hspace{3 pt} e \otimes f. \hspace{1pt} f \otimes e \otimes a]$$ which is equal to $$\text{pm} \hspace{3 pt} x \otimes y\hspace{3 pt} \text{to} \hspace{3 pt} t \otimes b. \hspace{1pt} b \otimes t \otimes (\text{pm} \hspace{3 pt} c \otimes d\hspace{3 pt} \text{to} \hspace{3 pt} e \otimes f. \hspace{1pt} f \otimes e \otimes a) .$$

Note that a simple way of ensuring these restrictions are satisfied is not allowing the variable $x$ to occur in the context of $w$ in $v[w/x]$. Since $x$ is in the context of $v$, this is always the case in the affine lambda calculus.

\begin{definition}
Given the typings judgments $\Gamma, x: \mathbb{A} \triangleright v:\mathbb{B}$ and $\Delta \triangleright w: \mathbb{A}$, the substitution $\Gamma, \Delta \, \triangleright \,  v[w/x]:\mathbb{B}$ is defined below. The types of judgments are omitted as no ambiguity arises.
\begin{align*}
  \hspace{-23pt}\Gamma,  \Delta \triangleright y[w/x] &=  \Gamma, \Delta \triangleright y, \\
  \hspace{-23pt}  \Delta \triangleright *[w/x] &=   \Delta \triangleright * ,\\
  \hspace{-23pt} \Gamma,  \Delta \triangleright (\lambda y: \mathbb{B}. v)[w/x] &= \Gamma,  \Delta \triangleright  \lambda y: \mathbb{B}. v[w/x],  \\
  \hspace{-23pt}(\text{dis}(v))[w/x] &= \text{dis}(v[w/x]),\\
  \hspace{-23pt}(\text{inl}(v))[w/x] &= \text{inl}(v[w/x]),\\
  \hspace{-23pt}(\text{inr}(v))[w/x] &= \text{inr}(v[w/x]),\\
  \hspace{-23pt}\text{In the next three cases, } \Gamma,x: \mathbb{A} \in & \text{Sf}(\Gamma_1,\ldots,\Gamma_i,\ldots,\Gamma_n) \text{ and } \Gamma_{i}  \triangleright v_{i}\\
  \hspace{-23pt} \Gamma, \Delta  \triangleright(f(v_1, \ldots, v_n))[w/x] &= \Gamma, \Delta  \triangleright f(v_1[w/x], \ldots, v_n),& (\text{ if } x: \mathbb{A} \in \Gamma_1)  \\
  \hspace{-23pt} \Gamma, \Delta  \triangleright(f(v_1, \ldots,v_i,\ldots, v_n))[w/x] &= \Gamma, \Delta  \triangleright f(v_1, \ldots,v_i[w/x],\ldots, v_n),& (\text{ if } x: \mathbb{A} \in T_i)  \\
  \hspace{-23pt}\Gamma, \Delta  \triangleright(f(v_1, \ldots, v_n))[w/x] &= \Gamma, \Delta  \triangleright f(v_1, \ldots, v_n[w/x]),& (\text{ if } x: \mathbb{A} \in \Gamma_n)  \\
  \hspace{-23pt}\text{In the next two cases, }\Gamma,x: \mathbb{A} & \in \text{Sf}(\Gamma_1,\Gamma_2), \Gamma_1 \triangleright v \text{, and } \Gamma_2 \triangleright u  \\
  \hspace{-23pt}\Gamma, \Delta  \triangleright (v \hspace{1pt}  u)[w/x] &= \Gamma, \Delta  \triangleright (v[w/x] \hspace{1pt} u), & (\text{ if } x: \mathbb{A} \in \Gamma_1)\\
  \hspace{-23pt}\Gamma, \Delta  \triangleright (v \hspace{1pt}  u)[w/x] &= \Gamma, \Delta  \triangleright (v \hspace{1pt} u[w/x]), & (\text{ if } x: \mathbb{A} \in \Gamma_2)\\
  \hspace{-23pt}\text{In the next two cases, }\Gamma,x: \mathbb{A} & \in \text{Sf}(\Gamma_1,\Gamma_2), \Gamma_1 \triangleright v \text{, and } \Gamma_2 \triangleright u  \\
  \hspace{-23pt}\Gamma, \Delta  \triangleright (v \otimes u)[w/x] &= \Gamma, \Delta  \triangleright v[w/x] \otimes u, & (\text{ if } x: \mathbb{A} \in \Gamma_1)\\ 
  \hspace{-23pt}\Gamma, \Delta  \triangleright (v \otimes u)[w/x] &= \Gamma, \Delta  \triangleright v \otimes u[w/x], & (\text{ if } x: \mathbb{A} \in \Gamma_2)\\
  \hspace{-23pt}\text{In the next two cases, }\Gamma,x: \mathbb{A}  \in \text{Sf}(&\Gamma_1,\Gamma_2), \Gamma_1 \triangleright v \text{, and } \Gamma_2, y: \mathbb{D}, z:\mathbb{E} \triangleright u  \\
  \hspace{-23pt} \Gamma, \Delta  \triangleright (\text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} y \otimes z. u)[w/x] &= \Gamma, \Delta  \triangleright \text{pm} \hspace{3 pt} v[w/x] \hspace{3 pt} \text{to} \hspace{3 pt} y \otimes z. u,  &  (\text{ if } x: \mathbb{A} \in \Gamma_1) \\
  \hspace{-23pt} \Gamma, \Delta  \triangleright (\text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} y \otimes z. u)[w/x] &= \Gamma, \Delta  \triangleright \text{pm} \hspace{3 pt} v \hspace{3 pt} \text{to} \hspace{3 pt} y \otimes z. u[w/x],  &  (\text{ if } x: \mathbb{A} \in \Gamma_2) \\
  \hspace{-23pt}\text{In the next two cases, }\Gamma,x: \mathbb{A} &  \in \text{Sf}(\Gamma_1,\Gamma_2), \Gamma_1 \triangleright v \text{, and } \Gamma_2 \triangleright u  \\
  \hspace{-23pt}\Gamma, \Delta  \triangleright(v \hspace{3 pt} \text{to} \hspace{3 pt} *.u)[w/x] &= \Gamma, \Delta  \triangleright v[w/x] \hspace{3 pt} \text{to} \hspace{3 pt} *.u&  (\text{ if } x: \mathbb{A} \in \Gamma_1),\\
  \hspace{-23pt}\Gamma, \Delta  \triangleright(v \hspace{3 pt} \text{to} \hspace{3 pt} *.u)[w/x] &= \Gamma, \Delta  \triangleright v \hspace{3 pt} \text{to} \hspace{3 pt} *.u[w/x] &  (\text{ if } x: \mathbb{A} \in \Gamma_2).\\
  \hspace{-23pt}\text{In the next two cases, }\Gamma,x: \mathbb{A}  \in \text{Sf}(&\Gamma_1,\Gamma_2), \Gamma_1 \triangleright v \text{, } \Gamma_2, y: \mathbb{D}\triangleright a   \text{, and } \\
   & \hspace{-66pt} \Gamma_2, z: \mathbb{E}\triangleright b  \\
   \hspace{-23pt} \text{case } v \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeI}(y) \Rightarrow a; \\
    &\inr_{\typeI}(z) \Rightarrow  b  \\ 
  \end{aligned}  
  \right\}[w/x] &=  \text{case } v [w/x]  \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeI}(y) \Rightarrow a; \\
    &\inr_{\typeI}(z) \Rightarrow  b  \\ 
  \end{aligned}  
  \right\}  &  (\text{ if } x: \mathbb{A} \in \Gamma_1) \\
  \text{case } v \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeE}(y) \Rightarrow a; \\
    &\inr_{\typeD}(z) \Rightarrow  b  \\ 
  \end{aligned}  
  \right\}[w/x] &=  \text{case } v   \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeE}(y) \Rightarrow a [w/x]; \\
    &\inr_{\typeD}(z) \Rightarrow  b [w/x]  \\ 
  \end{aligned}  
  \right\}  &  (\text{ if } x: \mathbb{A} \in \Gamma_2) \\
\end{align*}
\end{definition}




The sequential substitutions $M[M_i/x_i] \ldots [M_n/x_n]$ are writen as $M[M_i/x_i, \ldots ,M_n/x_n]$.


\subsection{Properties}


The calculus defined in \autoref{fig:typing_rules_linear} possesses several desirable properties, which are listed below. Before proceeding, it is necessary to introduce some auxiliary notation. Given a context $\Gamma$, $te(\Gamma)$ denotes context $\Gamma$ with all types erased. The expression $\Gamma \simeq_{\pi} \Gamma'$ denotes that the contexts $\Gamma$ is a permutation of context $\Gamma'$. This notation also applies to non-repetitive lists of untyped variables $te(\Gamma)$. Additionally, a judgment $\Gamma \triangleright v: \mathbb{A}$ will often be abbreviated into $\Gamma \triangleright v $ or even just $v$ when no ambiguities arise.


\begin{theorem} \label {theorem:unique_der}
   The lambda calculus defined by the rules of \autoref{fig:typing_rules_linear} has the following properties:
   \begin{enumerate}
     \item\label{perm} for all judgements $\Gamma \vljud v$ and $\Gamma'
             \vljud v$, te($\Gamma$) $\simeq_{\pi}$  te($\Gamma'$); 
     %
     \item\label{type} additionally if $\Gamma \vljud v: \typeA,
       \Gamma' \vljud v: \typeA'$, and $\Gamma \simeq_{\pi}
       \Gamma'$, then $\typeA$ must be equal to $\typeA'$;
     %
     \item\label{der} all judgements $\Gamma \vljud v:\typeA$ have a unique derivation.
\end{enumerate}
\end{theorem}

\begin{proof}
Since these properties are established in \cite[Theorem 2.3]{dahlqvist2023complete} for the lambda calculus without conditionals,  it suffices to consider the cases involving conditionals.

  It follows in all three cases from induction over the length of derivation
trees. 

Let us focus first on Property~\eqref{perm}. The case of the rules concerning
injections is direct. As for rule~$(\text{case})$ take two contexts $E$ and
$E'$ for the same conditional. According to this rule we obtain contexts
$\Gamma$, $\Gamma'$, $\Delta$, $\Delta'$ such that $E \in
\Shuff(\Gamma;\Delta)$ and $E' \in \Shuff(\Gamma';\Delta')$. It follows from
induction that  $\Gamma \simeq_\pi \Gamma'$ and $\Delta \simeq_\pi \Delta'$,
and the proof is then obtained from the sequence of equivalences,
\begin{align*}
        \text{te}(E) & \simeq_\pi \text{te}(\Gamma, \Delta) 
        \\
        & \simeq_\pi \text{te}(\Gamma', \Delta')
        \\
        & \simeq_\pi \text{te}(E')
\end{align*}
Concerning Property~\eqref{type}, the case of the rules concerning injections
is direct and the case of rule~$(\text{case})$ is a corollary of
Property~\eqref{perm}. Finally let us consider Property~\eqref{der}. Again the
case concerning injections is direct and we thus focus only on
rule~$(\text{case})$. According to this rule we obtain contexts $\Gamma$,
$\Gamma'$, $\Delta$, $\Delta'$ such that $E \in \Shuff(\Gamma;\Delta)$ and $E
\in \Shuff(\Gamma';\Delta')$. By an appeal to Property~\eqref{perm} we also
obtain $\Gamma \simeq_\pi \Gamma'$ and $\Delta \simeq_\pi \Delta'$, and thus
since shuffling preserves relative orders we obtain $\Gamma = \Gamma'$ and
$\Delta = \Delta'$. The proof then follows by induction.
\end{proof}


\begin{lemma}[Exchange and Substitution]
\label{lem:exh_and_sub} 
For every judgement $\Gamma,x: \typeA, y: \typeB, \Delta \vljud v: \typeD$ the
judgement $\Gamma, y:\typeB, x:\typeA, \Delta \vljud v:
\typeD$ is derivable. Not only this, given judgements  $\Gamma,x:\typeA \vljud
v: \typeB$ and $\Delta \vljud w: \typeA$ the judgement $\Gamma, \Delta \vljud
v[w/x]: \typeB$ is also derivable.
\end{lemma}

\begin{proof}
  Once again, these properties are established in \cite[Theorem 2.1]{dahlqvist2022syntactic} for the lambda calculus without conditionals, so it suffices to consider the cases involving conditionals.

  We start with the exchange property which follows by induction over the length
of derivation trees. The rules that involve injections are direct.  The rule
$(\text{case})$ calls for case distinction, more specifically we  distinguish
between the cases in which both variables ($x : \typeA$ and $y : \typeB$) are
in $\Gamma$, both are in $\Delta$, and otherwise. The first two cases follow
straightforwardly by induction and the definition of a shuffle. For the third
case consider a judgement $E_1,x : \typeA, y : \typeB, E_2 \vljud \text{case }
v\, \{\inl_{\typeF}(a) \Rightarrow w ; \, \inr_{\typeE}(b) \Rightarrow u \}:
\typeD$, and assume with no loss of generality that $\Gamma$ is of the form
$\Gamma_1, x : \typeA, \Gamma_2$ and $\Delta$ of the form $\Delta_1, y :
\typeB, \Delta_2$. The proof now follows directly from the implication,
\begin{align*}
        & E_1, x : \typeA, y : \typeB, E_2 \in \Shuff(\Gamma_1, x : \typeA, \Gamma_2 ; \,
        \Delta_1, y : \typeB, \Delta_2) \\
        \Longrightarrow &
        E_1, y : \typeB, x : \typeA, E_2 \in \Shuff(\Gamma_1, x : \typeA, \Gamma_2 ; \,
        \Delta_1, y : \typeB, \Delta_2)
\end{align*}
(which holds by the definition of a shuffle).

Finally we now focus on the substitution rule which also follows by induction over the
length of derivation trees. Again the cases involving the injections are direct,
and we thus only detail the proof of rule $(\text{case})$. Consider then
judgements $E,x : \typeA \vljud \text{case } v\, \{\inl_{\typeD}(a) \Rightarrow
w ; \, \inr_{\typeE}(b) \Rightarrow u \}: \typeB$ and
$Z \vljud t : \typeA$ with $E \in \Shuff(\Gamma; \Delta)$. According to the definition
of a shuffle either $\Gamma$ is of the form $\Gamma_1, x: \typeA$ or $\Delta$ is
of the form $\Delta_1, x : \typeA$. The first case follows directly and the second case
is a corollary of the exchange rule.
\end{proof}
 


\subsection{Equations-in-context}
The simply typed lambda calculus is a formal language that captures operations like the application of a function to an argument and the elimination of variables. To express these operations there is a set of rules known as reduction rules. These rules fall into two primary categories: the $\beta$\emph{-reductions}, which perform operations and enforce the implicit meaning of the term, and $\eta$\emph{-reductions}, which simplify terms by exploiting the extensionality of functions. 
There is also a secondary class of reductions known as \emph{commuting conversions}, which serve to disambiguate terms that, while equivalent, have different representations.
As a result, affine $\lambda$-calculus comes equipped with the so-called equations-in-context \gls{equation-in-context}, depicted in \autoref{fig:equations-linear-lambda}.
\begin{figure}[H]
  \centering
  \begin{tabular}{ |ccccc| }
    \hline
$(\beta)$ &  $\Gamma, \Delta \triangleright (\lambda x : \mathbb{A}.$ $v) \hspace{1pt}  w = v[w/x]: \mathbb{B} $ & &$(\eta)$ &  $\Gamma  \triangleright \lambda x : \mathbb{A} .(v \hspace{1pt} x) = v: \mathbb{A} \multimap \mathbb{B} $ \\
$(\beta_{\mathbb{I}_{e}})$ &   $ \Gamma \triangleright * \text { to } *.$ $v = v:\mathbb{A}$ && $(\eta_{\mathbb{I}_{e}})$ & $ \Delta, \Gamma \triangleright v$ to $*$ . $w[* / z] = w[v / z]: \mathbb{A}$  \\
$(\beta_{\otimes_{e}})$   &\multicolumn{4}{c|}{$ \hspace{-10pt}  E, \Gamma, \Delta \hspace{1pt}\triangleright \text{pm } v \otimes w$ to $x \otimes y.$ $u = u[v/x,w/y]:\mathbb{A}$ }\\
$(\eta_{\otimes_{e}})$   &\multicolumn{4}{c|}{$\Delta , \Gamma\hspace{1pt}\triangleright \text{pm } v$ to $x \otimes y.$ $u[x \otimes y/z] = u[v/z] :\mathbb{A} $}\\
 $(c_{\mathbb{I}_{e}})$  &\multicolumn{4}{c|}{ $\Delta,\Gamma, E \triangleright u[v \text{ to } \ast . w/z] = v \text{ to } \ast . u[w/z]:\mathbb{A}$ }\\
$(c_{\otimes_{e}})$ & \multicolumn{4}{c|}{ $\Delta,\Gamma, E  \triangleright u[$pm $v$ to $x \otimes y.$ $w/z] =$ pm $v$ to $x \otimes y.$ $u[w/z]: \mathbb{A} $ }\\
$(\eta_{\text{dis}})$ & \multicolumn{4}{c|}{ $x_1:\mathbb{A}_1, \ldots,x_n:\mathbb{A}_n\triangleright v = \text{dis}(x_1) \text{ to } \ast.$ $\ldots$ $\text{dis}(x_{n-1}) \text{ to } \ast \text{ dis}(x_{n}): \mathbb{ I}$ }\\
$(\beta_{case}^{inl})$ &\multicolumn{4}{c|} {$\text{case } 
          \inl_{\typeB}(v)\, \{ \inl_{\typeB} (x) \Rightarrow w 
          ;\, \inr_{\typeA} (y) 
          \Rightarrow u\}= w[v/x]$}\\
$(\beta_{case}^{inr})$ &\multicolumn{4}{c|} {$\text{case } 
          \inl_{\typeB}(v)\, \{ \inl_{\typeB} (x) \Rightarrow w 
          ;\, \inr_{\typeA} (y) 
          \Rightarrow u\}= u[v/y]$}\\
$(\eta_{case})$ &\multicolumn{4}{c|} { $\text{case } v\, \{\text{inl}_{\typeB} (y) \Rightarrow w [ \text{inl}_{\typeB}(y)/x] ;\, \text{inr}_{\typeA} (z) \Rightarrow w [ \text{inr}_{\typeA}(z)/x]\} = w[v/x]$} \\
\hline
  \end{tabular}
\caption{Equations-in-context for affine lambda calculus}
\label{fig:equations-linear-lambda}
\end{figure}

It is evident that, for example, equation $(\beta)$ enforces the meaning of  $(\lambda x : \mathbb{A}.$ $v) \hspace{1pt} w $, which is interpreted as ``$v$ with $w$ in place of $x$". The equation $(\eta)$, on the other hand, is a simplification rule that states that a function that applies another function $v$ to an argument $x$ can be simplified to the function $v$ itself. The remaining $\beta$ e $\eta$ equations follow similar reasoning. The commuting conversion $(c_{\mathbb{I}_{e}})$ expresses that substituting a variable $z$ by a term that maps a term $v$ to the unit element $*$ in a term $w$ is equivalent to mapping a term $v$ to the unit element $*$ and then replacing $z$ by $w$. The other commuting conversion has a similar interpretation.
%\begin{figure}[H]
  %\centering
  %\begin{tabular}{ |c|c| }
      %\hline 
      %Monoidal structure & Higher-order structure \\
      %\hline
      %pm $v \otimes w$ to $x \otimes y.$ $u = u[v/x,w/y]$& \\
      %pm $v$ to $x \otimes y.$ $u[x \otimes y/z] = u[v/z]$ & $(\lambda x : A.$ $v) w = v[w/x]$\\
      %$* \text { to } *.$ $v = v$ & $\lambda x : A.(v x) = v$ \\
      %$v$ to $*$ . $w[* / z] = w[v / z]$ & \\
      %\hline
      %\multicolumn{2}{|c|}{Commuting conversions} \\
      %\hline
      %\multicolumn{2}{|c|}{$u[v \text{ to } \ast . w/z] = v \text{ to } \ast . u[w/z]$}  \\
      %\multicolumn{2}{|c|}{$u[$pm $v$ to $x \otimes y.$ $w/z] =$ pm $v$ to $x \otimes y.$ $u[w/z]$} \\
      %\hline
      %\multicolumn{2}{|c|}{Discard} \\
      %\hline
      %\multicolumn{2}{|c|}{$v: \mathbb{I} = \text{dis}(x_1) \text{ to } \ast.$ $\ldots$ $\text{dis}(x_{n-1}) \text{ to } \ast \text{ dis}(x_{n})$} \\
      %\hline
  %\end{tabular}
  %\caption{Equations-in-context for affine lambda calculus}
  %\label{fig:equations-linear-lambda}
%\end{figure}



\begin{example} \label{ex:eq_contex_gen}
  For instance, consider the $\lambda$-term 
  $$ - \triangleright \big(\lambda z : \typeI \otimes \typeA. \, \text{pm } z \text{ to} * \otimes y. \, y\big) \, (v \otimes w): \typeA$$
Applying the $\beta$ reduction, we have:
\begin{align*}
 - \triangleright \big(\lambda z : \typeI \otimes \typeA. \, \text{pm } z \text{ to } * \otimes y. \, y\big) \, (v \otimes w) 
&= \text{pm }  v \otimes w \text{ to} * \otimes y. \, y : \typeA. 
\end{align*}
Next, applying the $\beta_{\otimes_e}$ reduction, it follows:
\begin{align*}
\text{pm }  v \otimes w \text{ to} * \otimes y. \, y : \typeA = w : \typeA. 
\end{align*}
\end{example}



\todo[inline,size=\normalsize]{Não sei se este é o lugar mais indicado para estas definições, mas tb não estou a ver outro dado que as categorias só vêm a seguir. Ou então passar isto para a interpretação.} 


\begin{definition}
  Let \( S \) be a set. A \emph{relation} on \( S \) is a subset \( R \subseteq S \times S \). An ordered pair \( (s_1, s_2) \in R \) means that \( s_1 \) is related to \( s_2 \).
\end{definition}


\begin{definition}
  A relation on a set \( S \) is an \emph{equivalence relation} if it is
\begin{itemize}
  \item reflexive: for all \( x \in S \), \( x \sim x \),
  \item symmetric: for all \( x, y \in S \), if \( x \sim y \) then \( y \sim x \), and
  \item transitive: for all \( x, y, z \in S \), if \( x \sim y \) and \( y \sim z \), then \( x \sim z \).
\end{itemize} 
We denote such a relation by \( \sim \,\subseteq S \times S \), and write \( x \sim y \) to mean that \( (x, y) \in \, \sim \).
\end{definition}

\begin{definition}
  Given an equivalence relation on a set \( S \), we can describe disjoint subsets of \( S \) called \emph{equivalence classes}. If \( s \in S \), then the \emph{equivalence class} of \( s \) is the set of all elements related to it:
\[
[s] = \{ r \in S \mid r \sim s \}.
\]
That is, \( [s] \) is the set of all elements that are considered “the same” as \( s \) under the relation \( \sim \). For a given set \( S \) and an equivalence relation \( \sim \) on \( S \), we define the \emph{quotient set}, denoted \( S / \sim \), whose elements are all the equivalence classes of elements in \( S \). There is an obvious \emph{quotient function} from \( S \) to \( S / \sim \) that maps each element \( s \in S \) to its equivalence class \( [s] \).
\end{definition}

%For instance, consider a set of cars \( S \). We can define an equivalence relation on \( S \) by grouping cars according to their colour. This results in subsets such as the set of blue cars, the set of red cars, the set of green cars, and so on — these subsets are the \emph{equivalence classes}. Moreover, the collection of all such equivalence classes forms a new set, called the \emph{quotient set}.



\begin{definition} \label{def:linear_lambda_theory}
  Consider a pair $(G, \Sigma)$, where $G$ is a class of ground types and $\Sigma$ is a class of sorted operation symbols. A \emph{$\lambda$-theory} is a triple $((G, \Sigma), Ax)$, where $Ax$ is a class of equations-in-context over $\lambda$-terms constructed from $(G, \Sigma)$. The elements of $Ax$ are called the \emph{axioms} of the theory.
\end{definition}


 Let $Th(Ax)$ denote the smallest class containing $Ax$, the equations presented in \autoref{fig:equations-linear-lambda}, and closed under exchange and substitution (\autoref{lem:exh_and_sub}). The elements of $Th(Ax)$ are called the \emph{theorems} of the theory.

 For instance recall \autoref{ex:eq_contex_gen}: 
 $$- \triangleright \big(\lambda z : \typeI \otimes \typeA. \, \text{pm } z \text{ to} * \otimes y. \, y\big) \, (v \otimes w): \typeA =  w : \typeA $$
 is a theorem.

\subsection{Interlude: Booleans} \label{subsec:interlude_bool}
Booleans can be represented as the type $\typeI \oplus \typeI$, in which case $\text{True} = \inl(*)$, $\text{False} = \inr(*)$ \cite{selinger2013lecture}. We will use the equations in \autoref{fig:equations-linear-lambda} to demonstrate that they possess certain properties that are characteristic of booleans in classical logic. The remaining properties can be proven through similar reasoning. 

Given variables $v : \typeI \oplus \typeI$ and $w : \typeI \oplus \typeI$, their conjunction and disjunction correspond to the following programs:

\begin{align*}
  & \hspace{-25pt}\textbf{Conjunction} \triangleq v:\typeI \oplus \typeI, w:\typeI \oplus \typeI \vljud \text{case } v\,
\left\{
    \begin{aligned} 
    &\inl_{\typeI}(x) \Rightarrow x \text{ to} *. \, w ; \\
    &\inr_{\typeI}(y) \Rightarrow   \text{ to} *. \,  \text{dis}(w) \text{ to} *. \inr_{\typeI}(*)  \\ 
  \end{aligned}  
  \right\}
\end{align*}

\begin{align*}
  & \hspace{-25pt}\textbf{Disjunction} \triangleq v:\typeI \oplus \typeI, w:\typeI \oplus \typeI \vljud \text{case } v\,
\left\{
    \begin{aligned} 
    &\inl_{\typeI}(x) \Rightarrow  \text{ to} *. \,  \text{dis}(w) \text{ to} *. \inl_{\typeI}(*) ; \\
    &\inr_{\typeI}(y) \Rightarrow  y \text{ to} *. \,  w  \\ 
  \end{aligned}  
  \right\}
\end{align*}

Moreover, negation can be expressed by the following program:
  \begin{align*}
  & \hspace{-25pt}\textbf{Negation} \triangleq v:\typeI \oplus \typeI \vljud \text{case } v\,
\left\{
    \begin{aligned} 
    &\inl_{\typeI}(x) \Rightarrow \inr_{\typeI}(x) ; \\
    &\inr_{\typeI}(y) \Rightarrow  \inl_{\typeI}(y)  \\ 
  \end{aligned}  
  \right\}
\end{align*}

  \begin{lemma} \label{lemma:inl_neutral}
     $\inl(*)$ acts as the neutral element for conjunction, whereas $\inr(*)$ serves as the absorbing element, \ie
     $$ \textbf{Conjunction} \, [\inl(*)/v,w'/w ] = w'  \quad \text{and} \quad \textbf{Conjunction} \, [\inr(*)/v,w'/w ] = \inr(*)$$.
  \end{lemma}

  \begin{proof}
    These properties follow from the equations $\beta_{case}^{inl}$, $\beta_{case}^{inr}$, $\beta_{\mathbb{I}_{e}}$, and $\eta_{\text{dis}}$.

    \begin{equation*}
    \begin{split}
      & \textbf{Conjunction} \, [\inl(*)/v,w'/w ] \\
      & = \text{case } \inl(*)\,
  \{\inl_{\typeI}(x) \Rightarrow x \text{ to} *. \, w' ;
  \, \inr_{\typeI}(y) \Rightarrow y \text{ to} *. \,  \text{dis}(w') \text{ to} *. \inr_{\typeI}(*)
  \} \\
  & = * \text{ to} *. \, w' & (\beta_{case}^{inl}) \\
  & =  * \text{ to} *. \, w' & (\beta_{\mathbb{I}_{e}}) \\
  & = w'
    \end{split}
    \end{equation*}

    For the second equality to be well-defined, we observe that the context of $ w'$ must be empty. Moreover, we remark on the following:
    \begin{align*}
       & x:\typeI \vljud \, x \text{ to} *. \, \text{dis}(w) = x \text{ to} *. \, x : \typeI    & (\eta_{\text{dis}}) \\
       & \implies - \vljud \,x \text{ to} *. \, \text{dis}(w)[*/x] = x \text{ to} *. \, x  [*/x]  : \typeI \\
       & \implies - \vljud \, * \text{ to} *. \, \text{dis}(w) = * \text{ to} *. \, *  : \typeI \\
       & \implies - \vljud \,  \text{dis}(w) = * : \typeI & (\beta_{\mathbb{I}_{e}})
    \end{align*}



    With the equation above in hand, we have:
    \begin{align*}
      & \hspace{-22pt} \textbf{Conjunction} \, [\inr(*)/v,w'/w ] \\
      & \hspace{-22pt}  = \text{case } \inr(*)\,
  \{\inl_{\typeI}(x) \Rightarrow x \text{ to} *. \, w' ;
  \, \inr_{\typeI}(y) \Rightarrow y \text{ to} *. \,  \text{dis}(w') \text{ to} *. \inr_{\typeI}(*)
  \} \\
  & \hspace{-22pt}  =  * \, y \text{ to} *. \,  \text{dis}(w') \text{ to} *. \inr_{\typeI}(*) & (\beta_{case}^{inr}) \\
  & \hspace{-22pt}  = \text{dis}(w') \text{ to} *. \inr_{\typeI}(*) & (\beta_{\mathbb{I}_{e}}) \\
  & \hspace{-22pt}  = * \text{ to} *. \inr_{\typeI}(*) &  (\eta_\text{dis} \text{ and } \beta_{\mathbb{I}_{e}}) \\
  & \hspace{-22pt}  =  \inr_{\typeI}(*)
    \end{align*}

     \todo[inline,size=\normalsize]{Duvida na aplicação da regra do discard}
  \end{proof}

 
  Note that the idempotency property of conjunction (for $\inl(*)$ and $\inr(*)$) follows directly from the equalities above.


\begin{lemma} \label{lem:comm}
  %The conjunction, disjunction, and negation of Boolean values satisfy the standard properties of classical logic. In particular, conjunction and disjunction are commutative, associative, idempotent, and distributive with respect to each other. Negation follows the double negation property and De Morgan's laws.
  The conjunction of two terms is comutative , \ie,
 $$ \textbf{Conjunction} \, [v'/v,w'/w ] = \textbf{Conjunction} \, [w'/v,v'/w ]  $$ 
\end{lemma}




\begin{proof}
  This property follows from the equations $\eta_{\text{case}}$,  $c_{\mathbb{I}_{e}}$, $c_{\mathbb{I}_{e}}$, $\eta_{\mathbb{I}_{e}}$, and $\eta_{\text{dis}}$, and the $\alpha$-equivalence.
  
  \begin{align*}
     &  \hspace{-20pt} \textbf{Conjunction} \, [v'/v,w'/w ] \\
     &  \hspace{-20pt} =\text{case } v\,
  \{\inl_{\typeI}(x) \Rightarrow x \text{ to} *. \, w ;
  \, \inr_{\typeI}(y) \Rightarrow y \text{ to} *. \,  \text{dis}(w) \text{ to} *. \inr_{\typeI}(*)
  \} [w'/w] \\
  & \hspace{-20pt} =   \text{case } w' \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeI}(a) \Rightarrow \text{ case } v' \, \Bigg\{ 
      \begin{aligned}
      & \inl_{\typeI}(x) \Rightarrow x \text{ to} *. \, \inl_{\typeI}(a);\\
      & \inr_{\typeI}(y) \Rightarrow y \text{ to} *. \,  \text{dis}(\inl_{\typeI}(a)) \text{ to} *. \inr_{\typeI}(*)
      \end{aligned} \Bigg\} \\
    &\inr_{\typeI}(b) \Rightarrow \text{ case } v' \, \Bigg\{ 
      \begin{aligned}
      & \inl_{\typeI}(x) \Rightarrow x \text{ to} *. \, \inr_{\typeI}(b);\\
      & \inr_{\typeI}(y) \Rightarrow y \text{ to} *. \,  \text{dis}( \inr_{\typeI}(b)) \text{ to} *. \inr_{\typeI}(*)
      \end{aligned} \Bigg\}    \\ 
  \end{aligned}  
  \right\} &  (\eta_{\text{case}}) \\
  & \hspace{-20pt} =   \text{case } w' \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeI}(a) \Rightarrow \text{ case } v' \, \Bigg\{ 
      \begin{aligned}
      & \inl_{\typeI}(x) \Rightarrow x \text{ to} *. \,  \inl_{\typeI}(a);\\
      & \inr_{\typeI}(y) \Rightarrow y \text{ to} *. \,  a \text{ to} *. \inr_{\typeI}(*)
      \end{aligned} \Bigg\} \\
    &\inr_{\typeI}(b) \Rightarrow \text{ case } v' \, \Bigg\{ 
      \begin{aligned}
      & \inl_{\typeI}(x) \Rightarrow x \text{ to} *. \, \inr_{\typeI}(b);\\
      & \inr_{\typeI}(y) \Rightarrow y \text{ to} *. \,  b \text{ to} *.  \inr_{\typeI}(*)
      \end{aligned} \Bigg\}    \\ 
  \end{aligned}  
  \right\} &  (\eta_{\text{dis}}) \\
  &  \hspace{-20pt} =   \text{case } w' \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeI}(a) \Rightarrow \text{ case } v' \, \Bigg\{ 
      \begin{aligned}
      & \inl_{\typeI}(x) \Rightarrow x \text{ to} *. \,  a \text{ to} *. \inl_{\typeI}(*) \, ;\\
      & \inr_{\typeI}(y) \Rightarrow y \text{ to} *. \,  a \text{ to} *. \inl_{\typeI}(*)
      \end{aligned} \Bigg\} \\
    &\inr_{\typeI}(b) \Rightarrow \text{ case } v' \, \Bigg\{ 
      \begin{aligned}
      & \inl_{\typeI}(x) \Rightarrow x \text{ to} *. \, b \text{ to} *. \, \inr_{\typeI}(*);\\
      & \inr_{\typeI}(y) \Rightarrow y \text{ to} *. \,  b \text{ to} *.  \inr_{\typeI}(*)
      \end{aligned} \Bigg\}    \\ 
  \end{aligned}  
  \right\} &  (\eta_{\mathbb{I}_{e}}) \\
   &  \hspace{-20pt} =   \text{case } w' \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeI}(a) \Rightarrow \text{ case } v' \, \Bigg\{ 
      \begin{aligned}
      & \inl_{\typeI}(x) \Rightarrow a \text{ to} *. \,  \inl_{\typeI}(*) [x \text{ to} *. \, * /* ] ;\\
      & \inr_{\typeI}(y) \Rightarrow a \text{ to} *. \,  \inl_{\typeI}(*) [y \text{ to} *. \, * /* ]
      \end{aligned} \Bigg\} \\
    &\inr_{\typeI}(b) \Rightarrow \text{ case } v' \, \Bigg\{ 
      \begin{aligned}
      & \inl_{\typeI}(x) \Rightarrow b \text{ to} *. \, \inl_{\typeI}(*) [x \text{ to} *. \, * /* ];\\
      & \inr_{\typeI}(y) \Rightarrow b \text{ to} *. \,  \inl_{\typeI}(*) [y \text{ to} *. \, * /* ]
      \end{aligned} \Bigg\}    \\ 
  \end{aligned}  
  \right\} &  (c_{\mathbb{I}_{e}}) \\
  & \hspace{-20pt} =   \text{case } w' \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeI}(a) \Rightarrow \text{ case } v' \, \Bigg\{ 
      \begin{aligned}
      & \inl_{\typeI}(x) \Rightarrow a \text{ to} *. \,  \inl_{\typeI}(x);\\
      & \inr_{\typeI}(y) \Rightarrow a \text{ to} *. \,  \inr_{\typeI}(y)
      \end{aligned} \Bigg\} \\
    &\inr_{\typeI}(b) \Rightarrow \text{ case } v' \, \Bigg\{ 
      \begin{aligned}
      & \inl_{\typeI}(x) \Rightarrow b \text{ to} *. \, x \text{ to} *. \, \inr_{\typeI}(*);\\
      & \inr_{\typeI}(y) \Rightarrow b \text{ to} *. \,  y \text{ to} *.  \inr_{\typeI}(*)
      \end{aligned} \Bigg\}    \\ 
  \end{aligned}  
  \right\} &  (\eta_{\mathbb{I}_{e}}) \\
&\hspace{-20pt} =  \text{case } w' \,  
\left\{
  \begin{aligned} 
  &\inl_{\typeI}(a) \Rightarrow \text{ case } v' \, \Bigg\{ 
    \begin{aligned}
    & \inl_{\typeI}(x) \Rightarrow a \text{ to} *. \, \inl_{\typeI}(x);\\
    & \inr_{\typeI}(y) \Rightarrow a \text{ to} *. \inr_{\typeI}(y)
    \end{aligned} \Bigg\} \\
  &\inr_{\typeI}(b) \Rightarrow \text{ case } v' \, \Bigg\{ 
    \begin{aligned}
    & \inl_{\typeI}(x) \Rightarrow b \text{ to} *. \, \text{dis}(\inr_{\typeI}(x)) \text{ to} *. \, \inr_{\typeI}(*);\\
    & \inr_{\typeI}(y) \Rightarrow b \text{ to} *. \,  \text{dis}(\inr_{\typeI}(y)) \text{ to} *. \inr_{\typeI}(*)
    \end{aligned} \Bigg\}    \\ 
\end{aligned}  
\right\} &  (\eta_{\text{dis}}) \\
& \hspace{-20pt} = \text{case } w'\,
\{\inl_{\typeI}(a) \Rightarrow a \text{ to} *. \, v' ;
\, \inr_{\typeI}(b) \Rightarrow b \text{ to} *. \,  \text{dis}(v') \text{ to} *. \inr_{\typeI}(*)
\} &  (\eta_{case}) \\
& \hspace{-20pt} = \textbf{Conjunction} \, [w'/v,v'/w ] 
  \end{align*}

\end{proof}

To address the double negation property and De~Morgan's laws, we begin by establishing a key equality known as the \emph{syntactic fusion law}.

\begin{lemma} [\emph{Syntactic fusion law}]
  The following equality holds:
  \begin{equation*}
   v \left[ \left(\text{case } a \,\{\inl_{\typeB}(x) \Rightarrow w ; \, \inr_{\typeA}(y) \Rightarrow u\}\right)  / z \right]  =  \text{case } a \,\{\inl_{\typeB}(x) \Rightarrow v[w/z] ; \, \inr_{\typeA}(y) \Rightarrow v[u/z]\}.
  \end{equation*}
\end{lemma}

  \begin{proof}
    
This equality follows from the $\alpha$-equivalence and the equations $\eta_{\text{case}}$, $\beta_{\text{case}}^{\text{inl}}$, and $\beta_{\text{case}}^{\text{inr}}$ . 

\begin{equation*}
\begin{split}
  & v \left[ \left(\text{case } a \,\{\inl_{\typeB}(x) \Rightarrow w ; \, \inr_{\typeA}(y) \Rightarrow u\}\right)  / z \right] & \\
  & =v  \left[ \left(\text{case } a \,\{\inl_{\typeB}(x') \Rightarrow w[x'/x] ; \, \inr_{\typeA}(y') \Rightarrow u[y'/y]\}\right)  / z \right] & (\alpha) \\
  & = \text{case } a\,
\left\{
    \begin{aligned} 
    &\inl_{\typeI}(x) \Rightarrow v \left[ \text{case } \inl_{\typeI}(x)\,
\left\{
    \begin{aligned} 
    &\inl_{\typeI}(x') \Rightarrow w[x'/x] ; \\
    &\inr_{\typeI}(y') \Rightarrow u[y'/y]  \\ 
  \end{aligned}  
  \right\} / z \right] ; \\
    &\inr_{\typeI}(y) \Rightarrow   v \left[ \text{case } \inr_{\typeI}(y)\,
\left\{
    \begin{aligned} 
    &\inl_{\typeI}(x') \Rightarrow w[x'/x] ; \\
    &\inr_{\typeI}(y') \Rightarrow u[y'/y]  \\ 
  \end{aligned}  
  \right\} / z \right] \\
  \end{aligned}  
  \right\} & (\eta_{\text{case}}) \\
   & = \text{case } a \,\{\inl_{\typeB}(x) \Rightarrow v[w[x'/x][x/x']/z] ; \, \inr_{\typeA}(y) \Rightarrow v[u[y'/y][y/y']/z]\}   & (\beta_{\text{case}}^{\text{inl}} \text{ and } \beta_{\text{case}}^{\text{inr}}  ) \\
   & =  \text{case } a \,\{\inl_{\typeB}(x) \Rightarrow v[w/z] ; \, \inr_{\typeA}(y) \Rightarrow v[u/z]\}   
\end{split}
\end{equation*}
  \end{proof}


\begin{lemma} \label{lem:dneg}
  The double negation of a term $w$ is equivalent to that term, \ie, 
  $$\textbf{Negation} [\textbf{Negation} \, [w/v] / v ] = w.$$
\end{lemma}

\begin{proof}

  This property follows from the syntactic fusion law and equations $\beta_{case}^{inl}$, $\beta_{case}^{inr}$, and  $\eta_{\text{case}}.$
  
    \begin{equation*}
    \begin{split}
       &\textbf{Negation} [\textbf{Negation} \, [w/v] / v ]  \\
        & \triangleq   \textbf{Negation} [ \text{case } w\,
    \{\inl_{\typeI}(x) 
        \Rightarrow \inr_{\typeI}(x) ; \,
      \inr_{\typeI}(y) \Rightarrow \inl_{\typeI}(y)
    \}/v] \\
    & = \text{case } w\,
\left\{
    \begin{aligned} 
    &\inl_{\typeI}(x) \Rightarrow \textbf{Negation}[\inr_\typeI(x)/v]; \\
    &\inr_{\typeI}(y) \Rightarrow  \textbf{Negation}[\inl_\typeI(y)/v]  \\ 
  \end{aligned}  
  \right\} & (\text{syntactic fusion law}) \\
    & =  \text{case } w\,
    \{\inl_{\typeI}(x) 
        \Rightarrow \inr_{\typeI}(x) ; \,
      \inl_{\typeI}(x) \Rightarrow \inr_{\typeI}(y)
    \} & (\beta_{case}^{inl} \text{ and }\beta_{case}^{inr}) \\
    & = w & (\eta_{case})
    \end{split} 
    \end{equation*}
\end{proof}

\begin{lemma} \label{lem:dmorgan} 
  De Morgan's laws hold for terms  $\Gamma \vljud v': \typeI \oplus \typeI$ and $\Delta \vljud w: \typeI \oplus \typeI$, \ie, 
  $$\textbf{Disjunction} [\textbf{Negation} \, [v'/v] / v, \textbf{Negation} \, [w'/v] / w ] =  \textbf{Negation}[\textbf{Conjunction} [v'/v, w'/w]  /v] $$ 
  and 
  $$\textbf{Conjunction} [\textbf{Negation} \, [v'/v] / v, \textbf{Negation} \, [w'/v] / w ] =  \textbf{Negation}[\textbf{Disjunction} [v'/v, w'/w]  /v].$$
\end{lemma}

\begin{proof}
  Both De Morgan's laws follow from the equations $\eta_{case}, \beta_{case}^{inl}$, $\beta_{case}^{inr}, c_{\mathbb{I}_{e}}, \eta_{\text{dis}} $, the syntatic fusion law (\text{SFL}), and the $\alpha$-equivalence.

  \begin{align*}
  & \hspace{-27pt} \textbf{Disjunction} [\textbf{Negation} \, [v'/v] / v, \textbf{Negation} \, [w'/v] / w ] \\
  & \hspace{-27pt} \triangleq \text{case }  \, 
    \text{case } v  
    \left\{ \begin{aligned}
    & \inl_{\typeI}(a) \Rightarrow \inr_{\typeI}(a); \\
    & \inl_{\typeI}(b) \Rightarrow \inl_{\typeI}(b) 
  \end{aligned}  \right\}  
   \left\{ \begin{aligned}
    & \inl_{\typeI}(x) \Rightarrow  \ldots ; \\
    & \inl_{\typeI}(y) \Rightarrow  \ldots
   \end{aligned} \right\} [v/v']\\
   & \hspace{-27pt} = \text{case }  v'
    \left\{ \begin{aligned}
      &\inl_{\typeI}(c) \Rightarrow \text{case } \, \text{case } \inl_{\typeI}(c)
        \left\{ \begin{aligned}  
          \inl_{\typeI}(a) \Rightarrow \inr_{\typeI}(a) ; \\
          \inr_{\typeI}(b) \Rightarrow \inl_{\typeI}(b)
          \end{aligned} \right\} 
          \, \ldots; \\
      & \inl_{\typeI}(d) \Rightarrow \text{case } \, \text{case } \inr_{\typeI}(d)
        \left\{ \begin{aligned}  
          \inl_{\typeI}(a) \Rightarrow \inr_{\typeI}(a) ; \\
          \inr_{\typeI}(b) \Rightarrow \inl_{\typeI}(b)
          \end{aligned} \right\} 
           \, \ldots 
    \end{aligned} \right\} 
   \, & (\eta_{case}) \\
    & \hspace{-27pt} = \text{case }  v'
    \left\{ \begin{aligned}
      &\inl_{\typeI}(c) \Rightarrow \text{case } \inr_{\typeI}(c)
           \left\{ \begin{aligned}
            & \inl_{\typeI}(x) \Rightarrow  \ldots; \\
            & \inr_{\typeI}(y) \Rightarrow  \ldots
            \end{aligned} \right\}; \\
      & \inl_{\typeI}(d) \Rightarrow \text{case } \inl_{\typeI}(d) 
           \left\{ \begin{aligned}
            & \inl_{\typeI}(x) \Rightarrow  \ldots; \\
            & \inr_{\typeI}(y) \Rightarrow  \ldots
            \end{aligned} \right\} 
    \end{aligned} \right\} 
   \, & \begin{aligned} & ( \beta_{case}^{inl} \\ & \text{and} \\  &\beta_{case}^{inr}) \end{aligned} \\
   & \hspace{-27pt} = \text{case }  v'
    \left\{ \begin{aligned}
      &\inl_{\typeI}(c) \Rightarrow  c \text{ to} *. \,   \textbf{Negation}[w'/v];  \\
      & \inl_{\typeI}(d) \Rightarrow  d \text{ to} *. \,  \text{dis}\left(\text{case } w'
        \left\{ \begin{aligned}  
          \inl_{\typeI}(a) \Rightarrow \inr_{\typeI}(a) ; \\
          \inr_{\typeI}(b) \Rightarrow \inl_{\typeI}(b)
          \end{aligned} \right\}\right) \ldots
    \end{aligned} \right\} 
   \, &  \begin{aligned} & ( \beta_{case}^{inl} \\ & \text{and} \\  &\beta_{case}^{inr}) \end{aligned} \\
      & \hspace{-27pt} = \text{case }  v'
    \left\{ \begin{aligned}
      &\inl_{\typeI}(c) \Rightarrow c \text{ to} *. \,   \textbf{Negation}[w'/v];  \\
      & \inl_{\typeI}(d) \Rightarrow   d \text{ to} *. \, \text{dis}\left( w'\right) \text{ to} *. \,  \inl_{\typeI}(*)
    \end{aligned} \right\}  
   \, & (\eta_{\text{dis}}) \\
      & \hspace{-27pt} = \text{case }  v'
    \left\{ \begin{aligned}
      &\inl_{\typeI}(c) \Rightarrow c \text{ to} *. \,   \textbf{Negation}[w'/v];  \\
      & \inl_{\typeI}(d) \Rightarrow   d \text{ to} *. \, \text{dis}\left( w'\right) \text{ to} *. \,  \inr_{\typeI}(*) \left\{ \begin{aligned}  
          \inl_{\typeI}(a) \Rightarrow \inr_{\typeI}(a) ; \\
          \inr_{\typeI}(b) \Rightarrow \inl_{\typeI}(b)
          \end{aligned} \right\}
    \end{aligned} \right\}  
   \, & (\beta_{case}^{inr}) \\
   & \hspace{-27pt} = \text{case }  v'
    \left\{ \begin{aligned}
      &\inl_{\typeI}(c) \Rightarrow  c \text{ to} *. \, \textbf{Negation}[ w'/v];  \\
      & \inl_{\typeI}(d) \Rightarrow   d \text{ to} *. \, \text{dis}\left( w'\right) \text{ to} *. \textbf{Negation}[\inr_{\typeI}(*)/v]
    \end{aligned} \right\}  
   \, &  \\
   & \hspace{-27pt} = \text{case }  v'
    \left\{ \begin{aligned}
      &\inl_{\typeI}(c) \Rightarrow  \textbf{Negation}[c \text{ to} *. \, w'/v];  \\
      & \inl_{\typeI}(d) \Rightarrow  \textbf{Negation}[ d \text{ to} *. \, \text{dis}\left( w'\right) \text{ to} *.\inr_{\typeI}(*)/v]
    \end{aligned} \right\}  
   \, & (c_{\mathbb{I}_{e}}) \\
   & \hspace{-27pt} = \textbf{Negation}\left[ \text{case } v'\,
\{\inl_{\typeI}(c) \Rightarrow c \text{ to} *. \, w ; 
\, \inr_{\typeI}(d) \Rightarrow d \text{ to} *. \,  \text{dis}(w) \text{ to} *. \inr_{\typeI}(*)
\} / v\right] 
   \, & (\text{SFL})  \\
   & \hspace{-27pt} = \textbf{Negation}[\textbf{Conjunction} [v'/v, w',w]  /v] & (\alpha)
  \end{align*}
\end{proof}


\subsection{Metric equational system}

\emph{Metric equations} \cite{mardare2016quantitative,mardare2017axiomatizability} are a strong candidate for reasoning about approximate program equivalence. These equations take the form of \gls{metric-equation}, where  $\epsilon$ is a non-negative rational representing the ``maximum distance" between the two terms $t$ and $s$. The metric equational system for affine lambda calculus is depicted in \autoref{fig:metric deductive system}. 
Strictly speaking, the equations $\Gamma \triangleright v = w : A$ in \autoref{fig:equations-linear-lambda}, which in this setting abbreviate $\Gamma \triangleright w =_0 v : A$, are also part of the metric equational system.


\begin{figure} [H]
\begin{equation*}
\begin{split}
\begin{aligned}
&
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \\
    \hline
   v=_{0}v
\end{array}
$
\end{minipage}
\hspace{-90pt}
\text{(refl)} 
 \hspace{55pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    v=_{q}w \quad w=_{r}u  \\
    \hline
   v=_{q + r} u
\end{array}
$ \end{minipage}
\hspace{-40pt} \text{(trans)} 
\hspace{55pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    v=_{q}w \quad r\geq q  \\
    \hline
   v=_{r} w
\end{array}
$ \end{minipage}
\hspace{-50pt} \text{(weak)} \\
&
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    \forall r > q . \hspace{4pt} v=_{r} w \\
    \hline
   v=_{q}w
\end{array}
$
\end{minipage}
\hspace{-45pt}
\text{(arch)} 
 \hspace{50pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    \forall i \leq n. \hspace{4pt} v=_{q_i} w\\
    \hline
   v=_{\wedge q_i} w
\end{array}
$ \end{minipage}
\hspace{-40pt} \text{(join)} 
\hspace{58pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    v=_{q} w \\
    \hline
   w =_{q } v
\end{array}
$ \end{minipage}
\hspace{-88pt}\text{(sym)}   \\
&
\begin{minipage}[t]{0.3\textwidth}
  $\begin{array}{c}
      v=_{q} w \quad v'=_{r} w' \\
      \hline
     v \otimes v' =_{q + r} w \otimes w'
  \end{array}
  $ \end{minipage}
  \hspace{-14pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
   \forall i \leq n. \hspace{4pt} v_{i}=_{q_i} w_{i}\\
    \hline
   f(v_{1},...,v_{n})=_{\Sigma q_i} f(w_{1},...,,w_{n}) 
\end{array}
$
\end{minipage}
 \hspace{47pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    v=_{q} w  \\
    \hline
  \lambda x : \mathbb{A}. \hspace{4pt} v=_{q} \lambda x:\mathbb{A}. \hspace{4pt} w
\end{array}
$ \end{minipage}
\hspace{20pt}  \\
&
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    v=_{q} w \quad  v'=_{r} w'  \\
    \hline
   \text{pm} \hspace{4pt} v \hspace{4pt} \text{to} \hspace{4pt} x \otimes y. \hspace{4pt} v'=_{q + r}\text{pm} \hspace{4pt} w \hspace{4pt} \text{to} \hspace{4pt} x \otimes y .  \hspace{4pt} w'
\end{array}
$
\end{minipage}
\hspace{95pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    v =_{q} w    \\
    \hline
  \text{dis}(v) =_{q} \text{dis}(w)
\end{array}
$ \end{minipage}
\hspace{-35pt}
\begin{minipage}[t]{0.3\textwidth}
  $\begin{array}{c}
      v=_{q} w \quad v'=_{r} w' \\
      \hline
     v \hspace{1pt} v' =_{q + r} w \hspace{1pt}  w'
  \end{array}
  $ \end{minipage}
 \\
 &
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
  \Gamma \triangleright v =_{q} w: \mathbb{A} \quad \Delta \in \text{perm}(\Gamma)\\
    \hline
   \Delta \triangleright v =_{q} w: \mathbb{A}
\end{array}
$
\end{minipage}
\hspace{36pt}
\begin{minipage}[t]{0.3\textwidth}
  $\begin{array}{c}
     v=_{q}w  \quad v'=_{r}w'\\
      \hline
      v \text { to } *.v' =_{q+r} w \text { to } *.w'
  \end{array}
  $ \end{minipage}
  \hspace{14pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
    v =_{q} w \quad v'=_{r} w'    \\
    \hline
  v[v'/x]=_{q + r} w[w'/x]
\end{array}
$ \end{minipage}
\hspace{10pt}
\end{aligned}
\end{split}
\end{equation*}
\caption{Metric equational system}
\label{fig:metric deductive system}
\end{figure}

Here, $\text{perm} (\Gamma)$ denotes the set of possible permutions of context $\Gamma$. The rules (refl), (trans), and (sym)  generalize the properties of reflexivity, transitivity, and symmetry of equality.  Rule (weak) asserts that if two terms are at a maximum distance $q$ from each other, then they are also separated by any $r \geq q$. Rule (arch) states that if $v =_r w$ for all approximations $r$ of $q$, then it necessarily follows that $v =_q w$. The rule  (join) expresses that if several maximum distances between two terms are known, the actual maximum distance between them is the minimum of these distances. The rule that follows conveys that if the maximum distance between two terms $v$ and $w$ is $q$, and the maximum distance between terms $v'$ and $w'$ is $r$, then the maximum distance between the tensor products $v \otimes v'$ and $w \otimes w'$ is $q + r$. The remaining rules follow similar reasoning.

% dizer o que é perm

\todo[inline,size=\normalsize]{Não sei se dá para postular para f sem argumento, não dá para usar nenhuma das regras, a da substuição não dá para aplicar porque f sozinho não é termo} 

\begin{example} \label{ex:metric_eqs}
  To ilustrate the usefulness of these equations, consider the program $P$ that recieves a tensor product, swaps its elements and then applies a function $f: \typeA \to \typeD \in \Sigma $ to the new second element of the tensor  pair:
\begin{equation*}
\begin{split}
& P = x : \mathbb{A},  y : \mathbb{B} \triangleright \text{pm} \hspace{3 pt} x \otimes y\hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes f(a) : \mathbb{B} \otimes \mathbb{D} &
\end{split}
\end{equation*}
Let $ f^{\epsilon} $ be an erroneous implementation of $ f $. The program above is thus rewritten as:
\begin{equation*}
  \begin{split}
  & P^{\epsilon} = x : \mathbb{A},  y : \mathbb{B} \triangleright \text{pm} \hspace{3 pt} x \otimes y\hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes f(a)^{\epsilon}  : \mathbb{B} \otimes \mathbb{D} &
  \end{split}
  \end{equation*}
Consider we can postulate the axiom $f^{\epsilon}(a) =_{\epsilon} f(a)$. Then, it is possible to show that $P^{\epsilon} =_{\epsilon} P $ using our metric equational system. The prove is as follows. The types and contexts are omitted for brevity as no ambiguity arises.
\begin{equation*}
  \begin{split}
  1 \hspace{10 pt} & f(a)^{\epsilon} =_{\epsilon} f(a) \\
  2 \hspace{10 pt} &  b =_{0} b & {(\text{refl})} \\
  3 \hspace{10 pt} & b \otimes f(a)^{\epsilon}  =_{\epsilon} b \otimes f(a)  & \text{($1,2,\otimes_i$)} \\
  4 \hspace{10 pt} &   x \otimes y =_{0}  x \otimes y  &{(\text{refl})}  \\
  5\hspace{10 pt}& \text{pm} \hspace{3 pt} x \otimes y\hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes f(a)^{\epsilon} =_{\epsilon} \text{pm} \hspace{3 pt} x \otimes y\hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes f(a) & \hspace{10pt} \text{($3,4,\otimes_e$)}
  \end{split}
  \end{equation*}
\end{example}


Note that without a metric equation for conditionals, we cannot, for instance, reason about approximate equivalence in programs such as  $\textbf{Conjunction}$, discussed in the previous subsubsection. For example, it would be interesting to know whether, given that \( v' =_{\varepsilon} \inl_{\typeI}(*) \), it follows that
\[
\textbf{Conjunction}[v/v', w/w'] =_{\varepsilon} \textbf{Conjunction}[\inl_{\typeI}(*)/v', w/w'].
\]


\begin{definition} \label{def:metric_lambda_theory}
  Consider a tuple \( (G, \Sigma) \), where \( G \) is a class of ground types and \( \Sigma \) is a class of sorted operation symbols of the form \( f : A_1, \ldots, A_n \to A \) with \( n \geq 1 \). A \emph{metric $\lambda$-theory} is a tuple \( ((G, \Sigma), Ax) \), where \( Ax \) is a class of \emph{metric equations-in-context} over  $\lambda$-terms constructed from \( (G, \Sigma) \).
\end{definition}

The elements of \( Ax \) are called the \emph{axioms} of the theory. Let \( Th(Ax) \) denote the smallest class that contains \( Ax \) and is closed under the rules presented in \autoref{fig:equations-linear-lambda} (i.e., the classical equational system) and \autoref{fig:metric deductive system}. The elements of \( Th(Ax) \) are called the \emph{theorems} of the theory.

For instance, in \autoref{ex:metric_eqs}, $\textbf{Conjunction}[v/v', w/w'] =_{\varepsilon} \textbf{Conjunction}[\inl_{\typeI}(*)/v', w/w']$ is a theorem.



\section{Category theory} \label{sec:catgories}


\todo[inline,size=\normalsize]{Cenas para a descrição da secção: ref awodeyCategoryTheory2010 e nota sobre a maioria dos exemplos serem retirados do noson} 

Category theory originated as an effort to connect and unify two distinct areas of mathematics.  The goal was to study and classify specific geometric structures—such as topological spaces, manifolds, and bundles—by associating them with corresponding algebraic structures like groups, rings, and abelian groups. It became clear that a language was needed to connect geometric and algebraic objects—one not explicitly tailored to geometry or algebra. Only a language of such generality could allow meaningful discussion across both fields.  This is the birth of category theory.
Described as "a language about nothing, and therefore about everything," category theory provides a highly general way of discussing mathematical concepts.  It was invented by Samuel Eilenberg and Saunders MacLane \cite{eilenbergGeneralTheoryNatural1945}. They organized various mathematical structures into categories called geometric others algebraic. To connect these categories, they defined functors, which map objects and morphisms from one category to another, much like functions do. They further introduced natural transformations, which provide a way to compare functors, translating the results of one functor into those of another. \cite{yanofskyMonoidalCategoryTheory2024}


\subsection{Categories}

\begin{definition}
   A \emph{category} $\catC$ consists of
   \begin{itemize}
    \item a collection of objects $A, B, C, \ldots$, denoted $|\catC|$ or $\text{Obj}(\catC)$;
    \item a collection of morphisms $f, g, \ldots $, usually denoted $\catC(A,B)$, $\mathrm{Hom}_{\catC}(A,B)$, or $\mathrm{Hom}(A,B)$ if there is no ambiguity. 
   \end{itemize}
    The collection for morphisms has the following structure:
    \begin{itemize}
      \item Each morphism has a specified domain and codomain; the notation $ f : A \to B $ indicates that $ f $ is a morphism from object $ A $ to object $ B $.  
       \item Every object $ A $ has an identity morphism $ \id_A : A \to A $.  
      \item For any pair of morphisms $ f : A \to B $ and $ g : B \to C $, where the codomain of $ f $ matches the domain of $ g $, there exists a composite morphism $ g \circ f : X \to Z $. We will also write $g \circ f$ as $f \cdot g$ or simply $f g$.
    \end{itemize}

     The composition is required to satisfy the two following laws: if $f : A \to B, g : B \to C,$ and $h:C \to D$ are morphisms, then
     \begin{itemize}
      \item  $f \circ \id_a = f = \id_b \circ f$;
      \item  $  (f \circ g) \circ h = f \circ (g \circ h) $.
     \end{itemize}
\end{definition}




%Sets

\begin{example}
 $\catSet$ is the category whose objects are sets and whose  morphisms are functions between them. Given a function \(f: A \to B\), it assigns to each element \(a \in A\) a unique image \(f(a) \in B\). 
 For any two functions \(f: A \to B\) and \(g: B \to C\), their composition is defined by
$$
(g \circ f)(a) = g(f(a)) \quad \text{for all } a \in A.
$$
This composition is \emph{associative}. That is, for any further function \(h: C \to D\), we have
\[
(h \circ g) \circ f = h \circ (g \circ f),
\]
since for every \(a \in A\),
\[
((h \circ g) \circ f)(a) = h(g(f(a))) = (h \circ (g \circ f))(a).
\]

Moreover, for every set \(A\), there exists an \emph{identity function}
\[
\id_A : A \to A, \quad \text{defined by } \id_A(a) = a,
\]
which satisfies the unit laws for composition:
\[
f \circ \id_A = f \quad \text{and} \quad \id_B \circ f = f
\]
for any function \(f: A \to B\).

Therefore, \(\mathbf{Set}\), with sets as objects and functions as morphisms, satisfies the axioms of a category.
\end{example}

Another common type of example consists of categories of sets equipped with additional structure, along with functions that preserve that structure.


\begin{definition}
A \emph{partially ordered set} or \emph{partial order} is a set $A$ equipped with a binary relation $\leq_A$ satisfying the following properties for all $a, b, c \in A$:
\begin{itemize}
    \item Reflexivity: $a \leq_A a$;
    \item Transitivity: If $a \leq_A b$ and $b \leq_A c$, then $a \leq_A c$;
    \item Antisymmetry: If $a \leq_A b$ and $b \leq_A a$, then $a = b$.
\end{itemize}
\end{definition}

\begin{example}
The set of real numbers \(\mathbb{R}\), equipped with the usual ordering \(\leq\), forms a poset. Moreover, it is \emph{linearly ordered} (or \emph{totally ordered}), since for any \(x, y \in \mathbb{R}\), either \(x \leq y\) or \(y \leq x\) holds.
\end{example}

\begin{definition}
Given two partial orders \((A, \leq_A)\) and \((B,\leq_B)\), a function \(m: A \to B\) is called a \emph{monotone map} (or \emph{order-preserving map}) if for all \(a, a' \in A\),
\[
a \leq_A a' \quad \Rightarrow \quad m(a) \leq_B m(a').
\]
\end{definition}



\begin{example}
  $\catPO$ is the category of all partial orders and all monotone maps. First, for any poset \(A\), the identity function \(\id_A : A \to A\) is monotone. Indeed, for all \(a \in A\),
\[
a \leq_A a \quad \Rightarrow \quad \id_A(a) \leq_A \id_A(a).
\]

Next, given monotone maps \(f : A \to B\) and \(g : B \to C\), their composition \(g \circ f : A \to C\) is also monotone. For all \(a, a' \in A\), if \(a \leq_A a'\), then
\[
f(a) \leq_B f(a') \quad \text{and} \quad g(f(a)) \leq_C g(f(a')),
\]
so it follows that
\[
(g \circ f)(a) \leq_C (g \circ f)(a').
\]

\end{example}

\begin{example}
  Each partially ordered set naturally defines a category. Let \( (P, \leq) \) be a poset. We define a category \( \catfont{B(P, \leq)} \), often denoted simply by \( \catfont{B(P)} \) or even \( \catfont{P} \), where the objects are the elements of \( P \), and there is a unique morphism \( p \to q \) if and only if \( p \leq q \). The reflexivity of the order \( \leq \) ensures the existence of identity morphisms, while transitivity guarantees that morphisms compose appropriately. Moreover, since there is at most one morphism between any two objects, composition is trivially associative. 
\end{example}

\begin{example}
 $\catVect$ is the category of finite complex vector spaces and linear mappings.
\end{example}

\begin{example}
Given a functional programming language $L$, we can define a category $\catCompFunc$.  In this category, the objects represent the data types of the language \( L \), and the morphisms correspond to computable functions or programs. A function is considered \emph{computable} if a computer program is capable of executing that function.

The composition of morphisms is defined as follows: given two morphisms \( f \colon X \to Y \) and \( g \colon Y \to Z \), the composition \( g \circ f \colon X \to Z \) is defined by applying \( g \) to the output of \( f \). This composition is often written as \( f;g \).

Additionally, the identity morphism \( \id_X \colon X \to X \) represents the ``identity program," which returns its input without making any changes (i.e., it "does nothing").
\end{example}


\begin{definition} 
 A morphism $f : A \to B$  in a category $\catC$ be a category is called an \emph{isomorphism} if there exists a morphism $f^{-1} : B \to A$ such that
\[
f^{-1} \circ f = \id_A \quad \text{and} \quad f \circ g = \id_B.
\]
In this case, $f^{-1}$ is called the \emph{inverse} of $f$, and it is unique. If such an isomorphism exists, we say that $A$ and $B$ are \emph{isomorphic}, written
$A \cong B.$
\end{definition}

One of the central ideas in category theory is \emph{duality}. Simply put, for a given definition of a structure, there is often a corresponding dual concept obtained by reversing the directions of all the arrows. 

\begin{definition} 
Let \(\catC\) be a category. The \emph{opposite category}, denoted \(\catC^{\mathrm{op}}\), is defined as follows:
\begin{itemize}
  \item The objects of \(\catC^{\mathrm{op}}\) are the same as those of \(\catC\).
  \item For any pair of objects \(A, B\), the hom-set in \(\catC^{\mathrm{op}}\) is defined by
  \[
  \textit{Hom}_{\catC^{\mathrm{op}}}(A, B) = \textit{Hom}_{\catC}(B, A),
  \]
  that is, each morphism \(f: A \to B\) in \(\catC^{\mathrm{op}}\) corresponds to a morphism \(f: B \to A\) in \(\catC\).
  \item Composition in \(\catC^{\mathrm{op}}\) is defined using the composition in \(\catC\), but in reverse order. That is, if
  \[
  A \xrightarrow{ \quad f \quad } B \xrightarrow{\quad g \quad } C
  \]
  are morphisms in \(\catC^{\mathrm{op}}\), corresponding to morphisms
  \[
  C \xrightarrow{ \quad g \quad} B \xrightarrow{ \quad f \quad} A
  \]
  in \(\catC\), then the composition in \(\catC^{\mathrm{op}}\) is defined by
  \[
  g \circ f := f \circ_{\catC} g.
  \]
\end{itemize}

Thus, \(\catC^{\mathrm{op}}\) reverses the direction of morphisms and composition while retaining the same collection of objects.
\end{definition}

\begin{definition}
  A subcategory \( \catD \) of a category \( \catC \) is a category such that 
  \begin{itemize}
    \item All the objects of \( \catD \) are objects of \( \catC \), and all the morphisms of \( \catD \) are morphisms of \( \catC \) (that is, \( \catD_0 \subseteq \catC_0 \) and \( \catD_1 \subseteq \catC_1 \)).
    \item The domain and codomain of any morphism in \( \catD \) are the same as in \( \catC \) (in other words, the domain and codomain maps for \( \catD \) are the restrictions of those for \( \catC \)).
    \item  It follows that for any objects \( A \) and \( B \) in \( \catD \), we have \( \mathrm{Hom}_{\catD}(A, B) \subseteq \mathrm{Hom}_{\catC}(A, B) \). If \( A \) is an object of \( \catD \), then its identity morphism \( \mathrm{id}_A \) in \( \catC \) also belongs to \( \catD \).
    \item If \( f: A \to B \) and \( g: B \to C \) are morphisms in \( \catD \), then the composite \( g \circ f \), as defined in \( \catC \), is also in \( \catD \), and coincides with the composite in \( \catD \).
  \end{itemize}
  
\end{definition}

\begin{example}
  The category $\catFinSet$, whose objects are finite sets and whose morphisms are functions between them, forms a subcategory of the category $\catSet$.
\end{example}


\begin{definition}
  A category is called \emph{small} if both its collection of objects and its collection of morphisms form sets.
A category is called \emph{locally small} if, for every pair of objects, the corresponding hom-set is a set.
\end{definition}


% Partial order
%Vect
%Banach spaces and short maps
%CPTP
%Linguagens de programação


%We say that a diagram commutes when for every two vertices X,Y in the diagram, all the paths from X to Y (following arrows) yield equal morphisms. 

%c_op
%iso

\subsection{Produts and coproducts}

 A category frequently possesses more intricate structure than a mere collection of objects and their morphisms. The existence of particular relationships among certain objects and morphisms can can make some objects have important properties.

It should be noted that a diagram is said to commute if, for every pair of objects $X$ and $Y$ in the diagram, all directed paths from 
$X$ to $Y$ yield equal morphisms.

\begin{definition}
  An object \( 0 \) in a category \( \catC \) is called an \emph{initial object} if for every object \( A \in \catC  \), there exists a unique morphism  $f: 0 \to A $.

\end{definition}

\begin{definition}
  An object \( T \) in a category \( \catC  \) is called a \emph{terminal object} if for every object \( A \in \catC  \), there exists a unique morphism $ f: A \to T $ .
\end{definition}

\begin{example}
In the category \( \catSet \), the empty set \( \emptyset \) is an initial object, since for any set \( S \), there exists a unique function$f : \emptyset \to S.$
This function is unique because there are no elements in \( \emptyset \) to map.

Any singleton set, such as \( \{*\} \) or \( \{a\} \), is a terminal object in this category. For any set \( S \), there exists a unique function $f : S \to \{*\}$,
which maps every element of \( S \) to the sole element of the singleton set
\end{example}

\begin{example}
  Let $(P, \leq)$ be a partial order and $\catfont{P}$ be its associated
 category.
  Here, the initial object is the \emph{bottom element}—an element that is less than or equal to every other element in $P$. The terminal object in $\catP$ is the \emph{top element}—an element that is greater than or equal to every other element in $P$.
\end{example}



\begin{definition} [\emph{Product}]
  Consider a category $\catC$.  We say that it has (binary) products if for any
objects $A$ and $B$ in $\catC$ there also exists an object $A \times B$
$\catC$ with morphisms $\pi_A : A \times B \to A$ and $\pi_B :  A \times B \to  B$
that satisfy a certain universal property: specifically for every two morphisms
$f  : C \to A$ and $g : C \to B$ there exists a \emph{unique} morphism $\langle f,g \rangle :
C \to A \times B $ called \emph{pairing} that makes the diagram below commute.
\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=4em,column sep=7em,minimum width=2em]
  {
   & C &  \\
    A  & A \times B & B\\
  };
  \path[-stealth]
    (m-1-2) edge  node [above] {$f$} (m-2-1)
    (m-1-2) edge  node [above] {$g$} (m-2-3)
    (m-2-2) edge  node [below] {$\pi_A$} (m-2-1)
    (m-2-2) edge  node [below] {$\pi_B$} (m-2-3)
    (m-1-2) edge [dotted]  node [right] {$\langle f,g \rangle$} (m-2-2);
    ;
\end{tikzpicture}
\]
\end{definition}

  \begin{definition}
Let \( A \times B \) be a product of objects \( A \) and \( B \), and let \( A' \times B' \) be a product of objects \( A' \) and \( A' \) in a category $\catC$. Suppose we are given morphisms \( f : A \to A' \) and \( g : B \to B' \). 

Then there exists a unique morphism
\[
f \times g : a \times b \to a' \times b'
\]
such that the following diagram commutes.
\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=4em,column sep=7em,minimum width=2em]
  {
   A & A \times B & B \\
    A'  & A' \times B' & B'\\
  };
  \path[-stealth]
    (m-1-2) edge  node [below] {$\pi_A$} (m-1-1)
    (m-1-2) edge  node [below] {$\pi_B$} (m-1-3)
    (m-1-1) edge  node [left] {$f$} (m-2-1)
    (m-1-3) edge  node [right] {$g$} (m-2-3)
    (m-2-2) edge  node [below] {$\pi_A'$} (m-2-1)
    (m-2-2) edge  node [below] {$\pi_B'$} (m-2-3)
    (m-1-2) edge [dotted]  node [right] {$ f\times g$} (m-2-2);
    ;
\end{tikzpicture}
\]
This induced morphism \( f \times g \) is called the \emph{product of the morphisms} \( f \) and \( g \), and it is given explicitly by
\[
f \times g = \langle f \circ \pi_A,\, g \circ \pi_B \rangle.
\]
\end{definition}




\begin{theorem} 
  Let \( A \times B \) be the product of objects \( A \) and \( B \) in a category $\catC$. For any object $C$ and morphisms \( f : C \to A \) and \( g : C \to B \) are morphisms, it holds that:
\[
\langle f \circ h,\, g \circ h \rangle = \langle f, g \rangle \circ h.
\]
\end{theorem}

\begin{proof}

  The universal property of the product induces a unique morphism \( \langle f, g \rangle : C \to A \times B \) such that
$\pi_A \circ \langle f, g \rangle = f \quad \text{and} \quad \pi_B \circ \langle f, g \rangle = g.$

Now, let \( h : D \to C \) be another morphism. Then the compositions \( f \circ h : D \to A \) and \( g \circ h : D \to B \) also induce a unique morphism \( \langle f \circ h,\, g \circ h \rangle : D \to A \times B \) by the universal property of the product. As a result, by the universal property of the produt the following diagram commutes.

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=8em]
  {
    & D &  \\
    & C &  \\
     & A \times B &\\
    A &  & B\\
  };
  \path[-stealth]
    (m-1-2) edge  node [right] {$h$} (m-2-2)
    (m-1-2) edge  node [left] {$f \circ h$} (m-4-1)
    (m-1-2) edge  node [right] {$g \circ h$} (m-4-3)
    (m-1-2) edge [bend left=-20] node [left] {$\langle f \circ h,g \circ h \rangle$} (m-3-2)
    (m-2-2) edge  node [right] {$\langle f,g \rangle$} (m-3-2)
    (m-2-2) edge  node [right] {$f$} (m-4-1)
    (m-2-2) edge  node [right] {$g$} (m-4-3)
    (m-3-2) edge  node [below=0.1cm] {$\pi_A$} (m-4-1)
    (m-3-2) edge  node [below=0.1cm] {$\pi_B$} (m-4-3)
    %(m-1-2) edge  node [above] {$f$} (m-2-1)
    %(m-1-2) edge  node [above] {$g$} (m-2-3)   
    %(m-1-2) edge [dotted]  node [right] {$\langle f,g \rangle$} (m-2-2);
    ;
\end{tikzpicture}
\]

\end{proof}


% exemplos

\begin{example}
In the category $\catSet$, the product of two sets $A$ and $B$ is given by their Cartesian product, denoted
\[
A \times B = \{(a, b) \mid a \in A,\ b \in B\}.
\]
The projection maps are defined by
\[
\pi_A(a, b) = a \quad \text{and} \quad \pi_B(a, b) = b.
\]
Given a set $C$ and morphisms $f: C \to A$ and $g: C \to B$, their pairing is the map
\[
\langle f, g \rangle(c) = (f(c), g(c)).
\]
\end{example}

\begin{example}
  Let $(P, \leq)$ be a partial order and $\catfont{P}$ be its associated category.  Consider a product of elements  \( p \times q \in P\). Then, by definition, there must exist projections satisfying
\[
p \times q \leq p \quad \text{and} \quad p \times q \leq q.
\]
Furthermore, for any element \( x \in P \), if
\[
x \leq p \quad \text{and} \quad x \leq q,
\]
then it follows that
\[
x \leq p \times q.
\]
This operation \( p \times q \) corresponds to what is commonly known as the \emph{greatest lower bound} or \emph{meet}, and is typically denoted by \( p \wedge q \).
\end{example}

\todo[inline,size=\normalsize]{Maybe tirar este exemplo} 

\begin{example}
  In the category $\catVect$, the product of two vector spaces $V$ and $W$ corresponds to their direct sum, denoted by $V \oplus W$.
The projection maps are the linear maps
\[
\pi_V : V \oplus W \to V, \quad \pi_V(v, w) = v,
\]
\[
\pi_W : V \oplus W \to W, \quad \pi_W(v, w) = w.
\]
Given any vector space $U$ and linear maps $f: U \to V$ and $g: U \to W$, the unique map $\langle f, g\rangle : U \to V \oplus W$
is defined by
\[
\langle f, g\rangle (u) = (f(u), g(u)).
\]
\end{example}


The \emph{coproduct} is the dual of the \emph{product}—it is obtained by reversing all the morphisms in the definition of a product. Consequently, a product in a category $\catC$ corresponds to a coproduct in the opposite category $\catCop$.

  
\begin{definition}
Consider a category $\catC$.  We say that it has (binary) coproducts if for any
objects $A$ and $B$ in $\catC$ there also exists an object $A \oplus B$ in
$\catC$ with morphisms $\inl : A \to A \oplus B$ and $\inr : B \to A \oplus B$
that satisfy a certain universal property: specifically for every two morphisms
$f  : A \to C$ and $g : B \to C$ there exists a \emph{unique} morphism $[f,g] :
A \oplus B \to C$ known as \emph{co-pairing} that makes the diagram below commute.
\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=4em,column sep=7em,minimum width=2em]
  {
   & C &  \\
    A  & A \oplus B & B\\
  };
  \path[-stealth]
    (m-2-1) edge  node [above] {$f$} (m-1-2)
    (m-2-3) edge  node [above] {$g$} (m-1-2)
    (m-2-1) edge  node [below] {$\inl$} (m-2-2)
    (m-2-3) edge  node [below] {$\inr$} (m-2-2)
    (m-2-2) edge [dotted]  node [right] {$[f,g]$} (m-1-2);
    ;
\end{tikzpicture}
\]
\end{definition}

  \begin{definition}
Let \( A \oplus B \) be a coproduct of objects \( A \) and \( B \), and let \( A' \times B' \) be a coproduct of objects \( A' \) and \( A' \) in a category $\catC$. Suppose we are given morphisms \( f : A \to A' \) and \( g : B \to B' \). 

Then there exists a unique morphism
\[
f \times g : a \times b \to a' \times b'
\]
such that the following diagram commutes.
\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=4em,column sep=7em,minimum width=2em]
  {
   A  & A \times B & B \\
    A'  & A' \times B' & B' \\
  };
  \path[-stealth]
    (m-1-1) edge  node [below] {$\inl$} (m-1-2)
    (m-1-3) edge  node [below] {$\inr$} (m-1-2)
    (m-1-1) edge  node [left] {$f$} (m-2-1)
    (m-1-3) edge  node [right] {$g$} (m-2-3)
    (m-2-1) edge  node [below] {$\inl$} (m-2-2)
    (m-2-3) edge  node [below] {$\inr$} (m-2-2)
    (m-1-2) edge [dotted]  node [right] {$ f\oplus g$} (m-2-2);
    ;
\end{tikzpicture}
\]
This induced morphism \( f \oplus g \) is called the \emph{coproduct of the morphisms} \( f \) and \( g \), and it is given explicitly by
\[
f \oplus g = [\inl \circ f,\, \inr \circ g].
\]
\end{definition}

\begin{theorem} 
  Let \( A \oplus B \) be the product of objects \( A \) and \( B \) in a category $\catC$. For any object $C$ and morphisms \( f : A \to C \) and \( g : B \to C \) are morphisms, it holds that:
\[
[h \circ f,\, h \circ g]  =  h \circ [f,g].
\]
\end{theorem}

\begin{proof}
 By the universal property of the coprodut the following diagram commutes.

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=8em]
  {
    & D &  \\
    & C &  \\
     & A \oplus B &\\
    A &  & B\\
  };
  \path[-stealth]
    (m-2-2) edge  node [right] {$h$} (m-1-2)
    (m-4-1) edge  node [left] {$h \circ f$} (m-1-2)
    (m-4-3) edge  node [right] {$h \circ c$} (m-1-2)
    (m-3-2) edge [bend left=20] node [left] {$[h \circ f,h \circ g]$} (m-1-2)
    (m-3-2) edge  node [right] {$[f,g]$} (m-2-2)
    (m-4-1) edge  node [right] {$f$} (m-2-2)
    (m-4-3) edge  node [right] {$g$} (m-2-2)
    (m-4-1) edge  node [below=0.1cm] {$\inl$} (m-3-2)
    (m-4-3) edge  node [below=0.1cm] {$\inr$} (m-3-2)
    ;
\end{tikzpicture}
\]

\end{proof}

\begin{example}
  In the category $\catSet$, the coproduct \( A \oplus B \) of two sets is their disjoint union, which can be constructed as
\[
A \oplus B = \{(a, 1) \mid a \in A\} \cup \{(b, 2) \mid b \in B\}.
\]
The canonical coproduct injections are defined by
\[
i_1(a) = (a, 1), \quad i_2(b) = (b, 2).
\]
Given any set \(C\) and functions \(f: A \to C\) and \(g: B \to C\), the copairing \([f, g]: A \oplus B \to C\) is defined by
\[
[f, g](x, \delta) = 
\begin{cases}
f(x) & \text{if } \delta = 1, \\
g(x) & \text{if } \delta = 2.
\end{cases}
\]
\end{example}

\begin{example}
  Let $(P, \leq)$ be a partial order and $\catfont{P}$ be its associated category.  
Consider a coproduct of elements \( p \oplus q \in P \). Then, by definition, there must exist injections satisfying
\[
p \leq p + q \quad \text{and} \quad q \leq p + q.
\]
Furthermore, for any element \( z \in P \), if
\[
p \leq z \quad \text{and} \quad q \leq z,
\]
then it follows that
\[
p + q \leq z.
\]
This operation \( p + q \) corresponds to what is commonly known as the \emph{least upper bound} or \emph{join}, and is typically denoted by \( p \vee q \).
\end{example}

\todo[inline,size=\normalsize]{Maybe tirar este exemplo} 

\begin{example}
  In $\catVect$, the coproduct coincides with the product. In such cases, this structure is called a \emph{biproduct}. 
  In both  $\catVect$. the injection maps are the linear maps
\[
\inl: V \to  V \oplus W, \quad  \inl(v)= (v,0),
\]
\[
\inr : W \to V \oplus W, \quad \inr(w) = (0,w).
\]
Given any vector space $U$ and linear maps $f: V \to U$ and $g: W \to U$, the unique map $\langle f, g\rangle : V \oplus W \to U$
is defined by
\[
[f, g] (v,w) = f(v)+ g(w).
\]
\end{example}



\subsection{Functors}
  

Although categories are interesting on their own, the real strength of category theory lies in understanding how categories relate to one another. Just as functions express relationships between sets, functors play a similar role for categories. A functor maps each object in one category to an object in another category, and it does the same for morphisms, preserving the structure of the relationships.

\begin{definition}
  Let $\catC$ and $\catD$ be two categories. A \emph{functor} $F: \catC \to \catD$ consists of a mapping that assigns to each object $A$ in $\catC$ an object $FA$ in $\catD$, and to each morphism $f \in \mathrm{Hom}_{\catC}(A, B)$ a morphism $Ff \in \mathrm{Hom}_{\catD}(FA, FB)$, in such a way that the following two conditions are satisfied for all objects $A, B, C$ in $\catC$ and all morphisms $f \in \mathrm{Hom}_{\catC}(A,B)$ and $g \in \mathrm{Hom}_{\catC}(B,C)$:
\[
F(\id_A) = \id_{FA}, \qquad F(g \circ f) = F(g) \circ F(f).
\]

A functor $F: \catC \to \catD$ is said to be \emph{full} if, for all objects $A$ and $B$ in $\catC$, the induced map
\[
F_{A,B}: \mathrm{Hom}_{\catC}(A, B) \longrightarrow \mathrm{Hom}_{\catD}(FA, FB), \quad f \mapsto Ff,
\]
is surjective. The functor is called \emph{faithful} if each $F_{A,B}$ is injective, and \emph{fully faithful} if each $F_{A,B}$ is bijective.

A \emph{full embedding} is a functor that is fully faithful and, in addition, injective on objects.
\end{definition}

\begin {example}
Let $\catC$ be a category. Then there exists an \emph{identity functor} $\id_{\catC} : \catC \to \catC,$
which is defined on objects by $\id_{\catC}(A) = A$ for every object $A$ in $\catC$, and analogously on morphisms, that is, $\id_{\catC}(f) = f$ for every morphism $f$ in $\catC$.
\end{example}

\begin{example}
  Consider the natural numbers $\mathbb{N}$ as partial order category. There is a functor $(-) + 5 : \mathbb{N} \to \mathbb{N}$
that maps each object $m \in \mathbb{N}$ to $m + 5$. This defines a functor because it preserves morphisms: if $m \leq m'$, then $m + 5 \leq m' + 5$. Moreover, the identity morphisms are preserved, since $(\id_m) + 5 = \id_{m+5}$.
\end{example}

\begin{example}
  Consider the set of real numbers $\mathbb{R}$ and the set of integers $\mathbb{Z}$, each regarded as a partial order category. In this context, there exists a functor $\mathrm{Floor} : \mathbb{R} \to \mathbb{Z} $ that assigns to each real number $r \in \mathbb{R}$ the greatest integer less than or equal to $r$, denoted $\lfloor r \rfloor$. For instance, $\lfloor 6.2 \rfloor = 6$ and $\lfloor -1.66 \rfloor = -2$. 

Similarly, there exists a \emph{ceiling functor} $\mathrm{Ceil} : \mathbb{R} \to \mathbb{Z}$ that maps each real number $r$ to the least integer greater than or equal to $r$, denoted $\lceil r \rceil$.


\end{example}


\begin{definition}
  Given categories $\catC$, $\catD$, and $\catE$, a \emph{bifunctor}
$F : \catC \times \catD \to \catE$
is simply a functor from the product category $\catC \times \catD$ to $\catE$. In particular, $F$ is a rule that assigns:
\begin{itemize}
    \item to every object $A \in \catC$ and $B \in \catD$, an object $F(A, B) \in \catE$;
    \item to every morphism $f : A \to A'$ in $\catC$ and $g : B \to B'$ in $\catD$, a morphism
    $F(f, g) : F(A, B) \to F(A', B') \in \catE.$
\end{itemize}

These assignments must satisfy the following two requirements:

\begin{itemize}
    \item Respect for composition:  
    For morphisms $f : A \to A'$, $f' : A' \to A''$ in $\catC$ and $g : B \to B'$, $g' : B' \to B''$ in $\catD$, it should hold that
    \[
    F(f' \circ f,\, g' \circ g) = F(f', g') \circ F(f, g),
    \]
    where the $\circ$ on the right-hand side is composition in $\catE$.

    \item Respect for identities:  
    For all objects $A \in \catC$ and $B \in \catD$, it should hold that
    \[
    F(\id_A, \id_B) = \id_{F(A, B)},
    \]
    where $\id_A$ and $\id_B$ are the identity morphisms in $\catC$ and $\catD$, respectively, and $\id_{F(A, B)}$ is the identity morphism in $\catE$.
\end{itemize}

Many times, rather than writing the name of the bifunctor before the input, like $F(A, B)$, we write the bifunctor as an operation between the inputs, for example, $a \mathbin{\Box} b$. If we use this notation, the condition
\[
F(f' \circ f,\, g' \circ g) = F(f', g') \circ F(f, g)
\]
becomes
\[
(f' \circ f) \mathbin{\Box} (g' \circ g) = (f' \mathbin{\Box} g') \circ (f \mathbin{\Box} g).
\]
\end{definition}

\subsection{Natural Tranformations}
A \emph{natural transformation} is a morphism between functors. It provides a way of relating two functors that have the same domain and codomain. Intuitively, if we consider two functors $F, G : \catC \to \catD$ as different ways of assigning images of the category $\catC$ into the category $\catD$, then a natural transformation $\eta : F \Rightarrow G$ is a coherent way of transforming the image of $F$ into the image of $G$.

\begin{definition}
  Let $\catC$ and $\catD$ be categories, and let $F, G : \catC \to \catD$ be functors. A \emph{natural transformation} $\eta : F \Rightarrow G$ is a family of morphisms in $\catD$,
\[
\left( \eta_A : FA \to GA \right)_{A \in \mathrm{Ob}(\catC)},
\]
indexed by the objects of $\catC$, such that for every morphism $f : A \to A'$ in $\catC$, the following diagram commutes. 

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=4em,column sep=7em,minimum width=2em]
  {
   FA  & GA  \\
    FA'  & GA' \times B'  \\
  };
  \path[-stealth]
    (m-1-1) edge  node [above] {$\eta_A$} (m-1-2)
    (m-2-1) edge  node [below] {$\eta_{A'}$} (m-2-2)
    (m-1-1) edge  node [left] {$Ff$} (m-2-1)
    (m-1-2) edge  node [right] {$Gf$} (m-2-2)
    ;
\end{tikzpicture}
\]
Given a natural transformation \(\eta : F \Rightarrow G\), the morphism \(\eta_A : F(A) \to G(A)\) in \(\catD\) is called the \emph{component} of \(\eta\) at \(A\).
A natural transformation $\eta : F \Rightarrow G$ is represented diagrammatically as 
\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=0.4em,column sep=2em,minimum width=1em]
  {
   \catC   & \big\Downarrow_{\eta} & \catD \\
  };
  \path[-stealth]
    (m-1-1) edge [bend left=40] node [above] {$F$} (m-1-3)
    (m-1-1) edge [bend left=-40] node [below] {$G$} (m-1-3)
    ;
\end{tikzpicture}
\]

\end{definition}


\begin{example}
  For every functor $F : \catC \to \catD$, there exists a natural transformation
  \[
    \iota_F : F \Rightarrow F
  \]
  known as  \emph{identity natural transformation},  such that for each object $A \in \catC$, each component of $\iota_F$ is the identity morphism:
  \[
    (\iota_F)_A = \id_{F(A)} : F(A) \to F(A).
    \] 
\end{example}

\begin{example}
The \emph{list functor}
\[
\mathrm{List} : \catSet \to \catSet
\]
assigns to each set \( S \) the set of all finite sequences (or lists) of its elements. 

For instance, if \( S = \{a, b, c\} \), then
\[
\mathrm{List}(S) = \{\varepsilon, a, b, c, aa, ab, ac, ba, \ldots, abc, cba, \ldots\},
\]
where \( \varepsilon \) denotes the empty list.

Given a function \( f : S \to T \), where $T=\{1,2\} $, the functor maps it to \(\mathrm{List}(f) : \mathrm{List}(S) \to \mathrm{List}(T)\), which applies \( f \) to each element of a list. For example, if
\[
f(a) = 2, \quad f(b) = 1, \quad f(c) = 2,
\]
then \(\mathrm{List}(f)(aabccba) = 2212212\).

There exists a natural transformation
\[
\mathrm{Reverse} : \mathrm{List} \Rightarrow \mathrm{List},
\]
whose component at a set \(S\), \(\mathrm{Reverse}_S\), maps each list to its reversal. For example:
\[
\mathrm{Reverse}_S(accbab) = babcca.
\]

\end{example}


\begin{definition}
  A natural transformation $\eta : F \Rightarrow G$ between functors $F, G : \catC \to \catD$ is called a \emph{natural isomorphism} if, for every object $A \in \catC$,  $\eta_A : F(A) \to G(A)$ is an isomorphism in $\catD$.
\end{definition}


\subsection{Equivalence of Categories}

In category theory, the concept of isomorphism between categories can be quite strict - is is not often is arises in a non-trivial manner. So, we weaken the notion of isomorphism and come to the concept of an \emph{equivalence of categories}.

If \( F: \catC \to \catD \) is an isomorphism of categories, then for every object \( B \in \catD \), there exists a \emph{unique} object \( A \in \catC \) such that \( F(A) = B \). This expresses the idea that \( \catC \) and \( \catD \) are structurally identical.

An \emph{equivalence of categories} relaxes this requirement. For every object \( B \in \catD \), there exists an object \( A \in \catC \) such that \( F(A) \) is not necessarily equal to \( B \), but is \emph{isomorphic} to \( B \). 

\begin{definition}
  Categories \( \catC \) and \( \catD \) are  said to be \emph{equivalent} if there exist functors 
\( F: \catC \to \catD \) and \( G: \catD\to \catC \) such that 
\( G \circ F \cong \id_{\catC} \) and \( F \circ G \cong \id_{\catD} \). 
The functors \( F \) and \( G \) are called \emph{quasi-inverses}, and we write \( \catC \simeq \catD \).
This means that for every \( A \in \catC \), there is a \( B \in \catD \) with \( G(B) \cong A \), 
and for every \( B \in \catD \), there is an \( A \in \catC \) with \( F(A) \cong B \).
\end{definition}

\begin{example}
  One of the simplest examples of an equivalence of categories is the relationship between the one-object category \( \mathbf{1} \) and the category \( \mathbf{2}_I \), which has two objects and a single isomorphism between them. We can visualize this as:
$$
\ast \;\; \quad \simeq \quad a \xlongrightarrow{\;\cong\;} b \;\; 
$$

More precisely, there is a unique functor \( ! : \mathbf{2}_I \to \mathbf{1} \), and a functor \( L : \mathbf{1} \to \mathbf{2}_I \) defined by \( L(\ast) = a \). Clearly, the composition \( ! \circ L \) is equal to \( \id_{\mathbf{1}} \), and \( L \circ ! \cong \id_{\mathbf{2}_I} \), since both objects \( a \) and \( b \) in \( \mathbf{2}_I \) are isomorphic. Thus, \( \mathbf{1} \simeq \mathbf{2}_I \).
\end{example}

\subsection{Adjoints}
If we further weaken the notion of an equivalence of categories, we we arrive at the concept of an \emph{adjunction}. A central idea in category theory is that \emph{the weaker the assumptions we impose, the more mathematical phenomena we can capture}. This principle explains why adjunctions are among category theory's most important structures.

\begin{definition}
  An adjunction between categories $\catC$ and $\catD$ is given by a quadruple 
$(L, R, \eta, \epsilon)$, where $L : \catC \to \catD$ and 
$R : \catD \to \catC$ are functors, and 
$\eta : \id_{\catC} \Rightarrow R L$ and 
$\epsilon : L R \Rightarrow \id_{\catD}$ are natural transformations such that
\[
(\eta R) \circ (R \epsilon) = \id_R \quad \text{and} \quad (L \eta) \circ (\epsilon L) = \id_L.
\]
One says that $R$ is right adjoint to $L$, or that $L$ is left adjoint to $R$, and one calls 
$\eta$ the \emph{unit} and $\epsilon$ the \emph{counit} of the adjunction.  Such an adjunction is denoted by $L \dashv R$, where the turn of the symbol $\dashv$ always points to the left adjoint.

An adjunction between categories can be characterized in multiple equivalent ways. It follows one of the alternative formulations. 

Given categories \(\catC\) and \(\catD\), a pair of functors \(L: \catC \to \catD\) and \(R: \catD \to \catC \) form an \emph{adjunction} \(L \dashv R\) if there exists a natural isomorphism:
\[
\mathrm{Hom}_{\catD}(L(A), B) 
\xlongrightarrow{\quad \Phi_{A,B} \quad}
\mathrm{Hom}_{\catC}(A, R(B)).
\]

\end{definition}

\begin{example}
  Consider the set of real numbers $\mathbb{R}$ and the set of integers $\mathbb{Z}$, each viewed as partial order categories. There is an inclusion functor $\mathrm{inc} : \mathbb{Z} \hookrightarrow \mathbb{R}$ which simply maps each integer to itself.  This inclusion has a left adjoint $L : \mathbb{R} \to \mathbb{Z}$. 

To determine this left adjoint $L$, we use the definition of an adjunction:  for all $N \in \mathbb{Z}$ and $R \in \mathbb{R}$, we have a natural isomorphism:
\[
\mathrm{Hom}_{\mathbb{Z}}(L(R), N) \cong \mathrm{Hom}_{\mathbb{R}}(R, \mathrm{inc}(N)).
\]

Since both $\mathbb{Z}$ and $\mathbb{R}$ are partial orders, the hom-sets contain at most one morphism. Hence, this isomorphism reduces to the logical equivalence:
\[
 L(R) \leq N \text{ if and only if } R \leq \mathrm{inc}(N) = N.
\]

Take $R = 7.27$ as an example. Then the inequality $R \leq N$ holds precisely when $N$ is an integer greater than or equal to $7.27$. That is:
\[
7.27 \nleq 5,\quad 7.27 \nleq 6,\quad 7.27 \nleq 7,\quad 7.27 \leq 8,\quad 7.27 \leq 9, \quad \ldots
\]
By the condition above, we must then have:
\[
L(7.27) \nleq 5,\quad L(7.27) \nleq 6,\quad L(7.27) \nleq 7,\quad L(7.27) \leq 8,\quad L(7.27) \leq 9, \quad \ldots
\]
From this, we conclude that $L(7.27) = 8$. In general, $L(R)$ is the least integer greater than or equal to $R$, \ie, the ceiling function:
\[
L(r) = \lceil r \rceil.
\]

Thus, the inclusion functor $\mathrm{inc}$ has $\lceil \; \rceil$ as a left adjoint, \ie, $\lceil  \; \rceil \dashv \mathrm{inc}$.
The unit of this adjunction is the natural transformation
$\eta : \id_{\mathbb{R}} \Rightarrow \mathrm{inc} \circ \lceil  \;  \rceil,$
which expresses the inequality $r \leq \lceil r \rceil$ for all $r \in \mathbb{R}$. The counit of the adjunction is the identity, since for any integer $n$, it holds that $\lceil N \rceil = N$.
\end{example}



\begin{definition}
  Let \( F: \catC \to \catD \) and \( G:  \catD \to  \catE \) be functors. It is said that \( G \)  \emph{preserves coproducts} if  whenever $ L $ is a coproduct of \( F \), then \( G(L) \) is a coproduct of \( G \circ F \).
\end{definition}

\begin{theorem}
 Left adjoints preserve coproducts.
\end{theorem}

% usar a def principal e a ds homsets

%adjoints
% example com o floor
%Adjoints preserve coproducts

\subsection{Monoidal categories}
\begin{definition}
A \textbf{monoid} is a triple $(M, \cdot, u)$, where $M$ is a set equipped with 
a binary operation $\cdot \colon M \times M \to M$ and a distinguished element 
$u \in M$ called the \emph{unit}, satisfying the following axioms for all 
$x, y, z \in M$:
\begin{align*}
    \text{(Associativity)} & & x \cdot (y \cdot z) &= (x \cdot y) \cdot z, \\
    \text{(Unit laws)}     & & u \cdot x &= x = x \cdot u.
\end{align*}
\end{definition}

Monoidal categories are named so because they are categories equipped with an additional structure that resembles the structure of monoids.

\begin{comment}
  
\begin{definition}
 A \emph{strict monoidal category} is a category $\catC$ equipped with a bifunctor $\otimes : \catC \times \catC \to \catC$
called the \emph{tensor product}, which is associative, meaning that for all objects $A, B, C \in \catC$,
\[
A \otimes (B \otimes C) = (A \otimes B) \otimes C. 
\]
In addition, there is a distinguished object $I \in \catC$, called the \emph{unit}, that acts as a unit for $\otimes$, satisfying
\[
I \otimes C = C = C \otimes I \quad \text{for all } C \in \catC. 
\]
\end{definition}



\begin{example}
The partially ordered set of natural numbers $\mathbb{N}$ forms a strict monoidal category when equipped with addition, denoted $(\mathbb{N}, +, 0)$.  The tensor product is given by ordinary addition, and the unit object is $0$, since for any $n \in \mathbb{N}$ we have $0 + n = n = n + 0$, and addition is strictly associative.
Moreover, the tensor product is monotonic with respect to the order: if $m \leq m'$ and $n \leq n'$, then $m + n \leq m' + n'$. This order preservation ensures functoriality of the monoidal structure.
A similar argument applies to multiplication: if $m \leq m'$ and $n \leq n'$, then $m \cdot n \leq m' \cdot n'$. Therefore, $(\mathbb{N}, \cdot, 1)$ also forms a strict monoidal category under the same ordered structure.
\end{example}

\end{comment}

\begin{definition}
  A symmetric monoidal category consists of a category $\catC$ equipped with a bifunctor $\otimes : \catC \times \catC \to \catC$
called \emph{tensor product} and a distinguished object $I \in \catC$, called \emph{unit} together with natural isomorphisms
\[
\alpha_{A,B,C} : A \otimes (B \otimes C) \rightarrow (A \otimes B) \otimes C,
\]
\[
\lambda_A :\mathbb{I}\otimes A \rightarrow A, \quad \rho_A : A \otimes\mathbb{I}\rightarrow A,
\]
known as \emph{reassociator}, \emph{left unitor},and \emph{right unitor}, respectively.

Moreover, these natural isomorphisms are required to make the following coherence diagrams commute.
\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=3em,column sep=2em,minimum width=1em]
  {
    & (A \otimes B) \otimes (C \otimes D) & \\
   A \otimes (B \otimes (C \otimes D)) &  & ((A \otimes B) \otimes C) \otimes D  \\
    A \otimes ((B \otimes C) \otimes D)  & &  (A \otimes (B \otimes C)) \otimes D \\
  };
  \path[-stealth]
    (m-1-2) edge  node [above] {$\alpha$} (m-2-3)
    (m-3-3) edge  node [right] {$\alpha \otimes \id$} (m-2-3)
    (m-3-1) edge  node [below] {$\alpha$} (m-3-3)
    (m-2-1) edge  node [left] {$\id \otimes \alpha$} (m-3-1)
    (m-2-1) edge  node [above] {$\alpha$} (m-1-2)
    ;
\end{tikzpicture}
\]


\[
\begin{minipage}{0.45\textwidth}
\centering
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=3em,column sep=2em,minimum width=1em]
  {
    A \otimes (\mathbb{I} \otimes A) &   & (A \otimes \mathbb{I}) \otimes A \\
     & A \otimes A & \\
  };
  \path[-stealth]
    (m-1-1) edge  node [above] {$\alpha$} (m-1-3)
    (m-1-1) edge  node [left] {$\id \otimes \lambda_A$} (m-2-2)
    (m-1-3) edge  node [right] {$\rho_A \otimes \id$} (m-2-2);
\end{tikzpicture}
\end{minipage}
\quad \quad
\begin{minipage}{0.45\textwidth}
\centering
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=3em,column sep=3em,minimum width=1em]
  {
   \mathbb{I}\otimes\mathbb{I}&   &\mathbb{I}\otimes\mathbb{I} \\
     &\mathbb{I}& \\
  };
  \path[-stealth]
    (m-1-1) edge  node [left] {$\lambda_I$} (m-2-2)
    (m-1-3) edge  node [right] {$\rho_I$} (m-2-2);
  \draw[double equal sign distance] (m-1-1) -- (m-1-3);
\end{tikzpicture}
\end{minipage}
\]

\end{definition}






\begin{definition}
  A monoidal category is said to be \emph{symmetric} when it is
equipped with a natural isomorphism $\sw : A \otimes B \rightarrow B \otimes A$ known as \emph{braiding} such that the following diagrams commute.


\[
\begin{minipage}{0.45\textwidth}
\centering
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=3em,column sep=3em,minimum width=1em]
  {
    A \otimes B &   & B \otimes A  \\
     & A \otimes B & \\
  };
  \path[-stealth]
    (m-1-1) edge  node [above] {$\sw_{A,B}$} (m-1-3)
    (m-1-3) edge  node [right=0.15cm] {$\sw_{B,A}$} (m-2-2);
  \draw[double equal sign distance] (m-1-1) -- (m-2-2);
\end{tikzpicture}
\end{minipage}
\quad \quad \
\begin{minipage}{0.45\textwidth}
\centering
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=3em,column sep=3em,minimum width=1em]
  {
    A \otimes\mathbb{I}&   & \mathbb{I}\otimes A \\
     & A & \\
  };
  \path[-stealth]
    (m-1-1) edge  node [above] {$\sw_{A,I}$} (m-1-3)
    (m-1-1) edge  node [left] {$\rho_A$} (m-2-2)
    (m-1-3) edge  node [right=0.15cm] {$\lambda_A$} (m-2-2);
\end{tikzpicture}
\end{minipage}
\]

$$
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=1em]
  {
  A \otimes (B \otimes C)  & (A \otimes B) \otimes C & C \otimes (A \otimes B) \\
  A \otimes (C \otimes B) & (A \otimes C) \otimes B & (C \otimes A) \otimes B\\
  };
  \path[-stealth]
    (m-1-1) edge  node [above] {$\alpha$} (m-1-2)
    (m-1-2) edge  node [above] {$\sw$} (m-1-3)
    (m-1-3) edge  node [right] {$\alpha$} (m-2-3)
    (m-2-2) edge  node [below] {$\sw \otimes \id$} (m-2-3)
    (m-2-1) edge  node [below] {$\alpha$} (m-2-2)
    (m-1-1) edge  node [right] {$\id \otimes \sw$} (m-2-1)
    ;
\end{tikzpicture}
$$

\end{definition}

\begin{definition}
  A monoidal category $\catC$ is  said to be \emph{closed} if for each object
  $A$ in $\catC$ the functor $- \otimes A$ has a right adjoint,
  denoted by $A \multimap -$. 
\end{definition}

\begin{definition}
        A monoidal category $\catC$ with coproducts is called
        \emph{distributive} if for every object $A$ in $\catC$ the
        functor $- \otimes A$ preserves coproducts. Explicitly
        this means that the morphism,
        \[
                [\inl \otimes \id, \inr \otimes \id] : B \otimes A \oplus C \otimes                     A \to (B \oplus C) \otimes A
        \]
        is actually an isomorphism. We will denote the respective inverse
        by $\dist$. Note that if $\catC$ is monoidal closed then it is automatically
        distributive as left adjoints preserve all colimits.
\end{definition}


\begin{definition}
        The \emph{terminal map to the unit object} (of a monoidal category) is the unique mapping form each object to the monoidal unit and is represented by $!$.
\end{definition}


\begin{example}
Examples of monoidal closed categories with coproducts include $\catSet$ and $\catVect$. In $\catSet$, the tensor product is the cartesian product, the monoidal unit is the singleton set, the coproduct is the disjoint union, and the internal hom consists of all functions between sets. For $\catVect$, the tensor product is the standard tensor product of complex vector spaces, the field of complex number $\mathbb{C}$, the coproduct is the direct sum, and the internal hom is the space of complex linear maps. %Similarly, in $\catBan$ the tensor product is the projective tensor product, the monoidal unit is $\mathbb{R}$, the coproduct is the direct sum equipped with the $L_1$-norm, and the internal hom consists of all functions between sets corresponds to space of short (linear) maps equiped with the operator norm.
\end{example}


\begin{theorem}[\emph{Coherence Theorem for Symmetric Monoidal Categories}]
Any diagram in a symmetric monoidal category constructed only from associators $\alpha$, unitors $\lambda$, $\rho$, the symmetry $\sw$, and inverses and their composition and tensor product necessarily commutes.
\end{theorem}


 

 










\section{Interpretation} \label{sec: Lambda Calculus:Interpretation}

\todo[inline,size=\normalsize]{Sets, sub, exchange, soundness and completeness, conds, diagrama a explicar}

Up to this point, we have discussed $\lambda$-calculus in abstract terms: we explored which programs can be written, but we have not yet assigned them any meaning. This process—assigning meaning to syntactic expressions—is known as the \emph{interpretation} or \emph{semantics} of the language. In fact, the word ``semantics'' comes from the Greek word for ``meaning''.

There are different kinds of semantics, in particular, \emph{denotational semantics} interprets terms as mathematical objects. This is done by defining a function that maps syntactic entities (such as types and terms) to semantic entities (such as sets and elements). This mapping is called the \emph{interpretation function}, typically denoted by $\sem{-}$. Thus, given a term $v$, we write $\sem{v}$ to denote its meaning under a specific interpretation.

Naturally, this raises important questions: what guarantees do we have that the interpretation of terms respects the classical equations of the calculus? This leads us to the notions of \emph{soundness} and \emph{completeness}.

With respect to a given class of interpretations:
\begin{itemize}
  \item \emph{Soundness} is the property 
  \[
  M = N \;\Rightarrow\; \llbracket M \rrbracket = \llbracket N \rrbracket
  \quad \text{for all interpretations in the class.}
  \]
  That is, if two terms are provably equal, then they are interpreted as equal.
  
  \item \emph{Completeness} is the property 
  \[
  \llbracket M \rrbracket = \llbracket N \rrbracket
  \;\Rightarrow\; M = N
  \quad \text{for all interpretations in the class.}
  \]
  That is, if two terms are interpreted as equal, then they are provably equal.
\end{itemize}

Soundness ensures that our equations are \emph{correct}—all derivable equations are semantically valid. Completeness ensures that our equations are \emph{sufficient}—we can derive all semantically valid equations.

We note that, in the case of the metric equations, the underlying idea is similar, although soundness and completeness are defined differently.

\vspace{20pt}

In order to define the interpretation of judgments $\Gamma \triangleright v: \mathbb{A}$, it is necessary to establish some notation first. Let $\catC$ be a symmetric monoidal category and $A$, $B$ and $C$ be objects of this category. For all morphisms $f: A \otimes B \xrightarrow{} C$, the morphism $\overline{f} : A \xrightarrow{} (B \multimap C)$ denotes the corresponding curried version. Moreover, there is the application morphism, $\text{app}: (A \multimap B) \otimes A \rightarrow B$.  

For all ground types $X \in G$  the interpretation of $[\![X]\!]$  is postulated  to be an object of $\catC$. Types are interpreted inductively using the unit $\mathbb{I}$, the tensor $\otimes$, the coproduct $\oplus$, and the linear map $\multimap$. Given a non-empty context $\Gamma=\Gamma',x: \mathbb{A}$, its interpretation is defined by $[\![\Gamma',x: \mathbb{A}]\!] = [\![\Gamma']\!] \otimes [\![\mathbb{A}]\!]$ if $\Gamma'$ is non-empty and $[\![\Gamma',x: \mathbb{A}]\!] = [\![\mathbb{A}]\!]$ otherwise. The empty context $-$ is interpreted as $[\![-]\!] = \mathbb{I}$. Given $A_{1}, . . . ,A_{n} \in \catC$, the $n$-tensor $(\ldots (A_1 \otimes A_2) \otimes \ldots ) \otimes A_{n}$ is denoted as $X_1 \otimes \ldots \otimes A_{n}$, and similarly for morphisms. 


\subsection{Classical Semantics}

``Housekeeping" morphisms are employed to handle interactions between context interpretation and the vectorial model. Given $\Gamma_{1}, \ldots, \Gamma_{n}$, the morphism that splits $[\![\Gamma_{1}, \ldots, \Gamma_{n}]\!]$ into $[\![\Gamma_{1}]\!] \otimes \ldots \otimes [\![\Gamma_{n}]\!]  $ is denoted by $\text{sp}_{\Gamma_1;\ldots;\Gamma_n}: [\![\Gamma_{1}, \ldots, \Gamma_{n}]\!] \xrightarrow{} [\![\Gamma_{1}]\!] \otimes \ldots \otimes [\![\Gamma_{n}]\!] $. For $n=1$, $\text{sp}_{\Gamma_1} = \text{id}$.
Let $\Gamma_1$ and $\Gamma_2$ be two contexts, $\text{sp}_{\Gamma_1, \Gamma_2} \rightarrow \Gamma_1\otimes \Gamma_2$ is defined as:
\begin{equation*}
  \text{sp}_{-; \Gamma_2} = \lambda^{-1} \hspace{1cm} \text{sp}_{\Gamma_1; -} = \rho^{-1} \hspace{1cm} \text{sp}_{\Gamma_1;x:\mathbb{A}} = \id \hspace{1cm} \text{sp}_{\Gamma_1; \Delta, x: \mathbb{A}} = \alpha \cdot (\text{sp}_{\Gamma_1; \Delta} \otimes \id)
\end{equation*}
For $n>2$, $\text{sp}_{\Gamma_1;\ldots;\Gamma_n}$ is is defined recursively based on the previous definition, using induction on $n$:
\begin{equation*}
  \text{sp}_{\Gamma_1;\ldots;\Gamma_n} = (\text{sp}_{\Gamma_1;\ldots;\Gamma_{n-1}} \otimes \id )\cdot \text{sp}_{\Gamma_1, \ldots, \Gamma_{n-1} ;\Gamma_n}
\end{equation*}
On the other hand, $\text{jn}_{\Gamma_1;\ldots;\Gamma_n}$ denotes the inverse of $\text{sp}_{\Gamma_1;\ldots;\Gamma_n}$. Next, given $\Gamma, x : \mathbb{A}, y : \mathbb{B},\Delta$, the morphism permuting $x$ and $y$ is denoted by $\text{exch}_{\Gamma, x : \mathbb{A}, y : \mathbb{B},\Delta}: [\![\Gamma,\underline{ x : \mathbb{A}, y : \mathbb{B}},\Delta]\!] \xrightarrow{} [\![\Gamma, y : \mathbb{B}, x : \mathbb{A}, \Delta]\!] $ and defined as:
\begin{equation*}
  \text{exch}_{\Gamma, \underline{ x : \mathbb{A}, y : \mathbb{B}},\Delta} = \text{jn}_{\Gamma; y:\mathbb{B}, x:\mathbb{A};\Delta} \cdot (\id \otimes \text{sw} \otimes \id ) \cdot \text{sp}_{\Gamma; x:\mathbb{A}, y:\mathbb{B};\Delta}
\end{equation*} 


The shuffling morphism $\text{sh}_{E}: [\![E]\!] \xrightarrow{} [\![\Gamma_1, \ldots, \Gamma_n ]\!]$ is defined as a suitable composition of exchange morphisms.


For every operation symbol $f: \mathbb{A}_{1}, \ldots, \mathbb{A}_{n} \xrightarrow{} \mathbb{A}$ it is assumed the existence of an morphism $[\![f]\!]: [\![\mathbb{A}_{1}]\!] \otimes \ldots \otimes [\![\mathbb{A}_{n}]\!] \xrightarrow{}  [\![\mathbb{A}]\!] $.
The interpretation of judgments is defined by induction over derivations according to the rules in \autoref{fig:denotational_sem}.
\vspace{-7pt}
\begin{figure} [H]
\begin{equation*}
\begin{split}
\begin{aligned}
&
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
      [\![\Gamma_{i} \triangleright v_{i}: \mathbb{A}_{i} ]\!]=m_{i}  \quad f: \mathbb{A}_{1}, \ldots, \mathbb{A}_{n} \xrightarrow{} \mathbb{A} \in \Sigma \quad E \in \text{Sf}(\Gamma_{1}; \ldots; \Gamma_{n})\\
    \hline
  [\![E \triangleright f( v_{1},\ldots,v_{n}): \mathbb{A} ]\!] = [\![ f]\!] \cdot (m_{1}\otimes \ldots \otimes m_{n}) \cdot \text{sp}_{\Gamma_1;\ldots;\Gamma_n}\cdot \text{sh}_{E}
\end{array}
$
\end{minipage}
\hspace{208pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
      \\
    \hline
  [\![ x:\mathbb{A} \triangleright x:\mathbb{A}]\!] = \id_{[\![\mathbb{A} ]\!]}
\end{array}
$ \end{minipage} \\
&
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     \\  
    \hline
   [\![ - \triangleright *: \mathbb{I}]\!] = \id_{[\![\mathbb{I} ]\!]}
\end{array}
$
\end{minipage}
\hspace{-34pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
      [\![\Gamma \triangleright v: \mathbb{A} \otimes \mathbb{B} ]\!]=m  \quad [\![\Delta,x: \mathbb{A}, y: \mathbb{B}  \triangleright w: \mathbb{D} ]\!] =n  \quad E \in \text{Sf}(\Gamma;\Delta)\\
    \hline
  [\![ E\triangleright \text{pm } v \text{ to } x \otimes y. w :\mathbb{D}]\!] = n \cdot \text{jn}_{\Delta;\mathbb{A};\mathbb{B}} \cdot \alpha \cdot \text{sw}\cdot (m \otimes \id) \cdot \text{sp}_{\Gamma;\Delta} \cdot \text{sh}_{E}
\end{array}
$ \end{minipage} \\
&
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}  
     [\![ \Gamma \triangleright v: \mathbb{A} ]\!] =m \quad [\![\Delta \triangleright w: \mathbb{B} ]\!]=n \quad E \in \text{Sf}(\Gamma;\Delta) \\
    \hline
  [\![ E \triangleright v \otimes w: \mathbb{A} \otimes \mathbb{B} ]\!] = (m \otimes n) \cdot \text{sp}_{\Gamma;\Delta} \cdot \text{sh}_{E}
\end{array} 
$
\end{minipage}\\
&
 \begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c} 
    [\![\Gamma \triangleright v: \mathbb{I} ]\!]=m  \quad [\![\Delta \triangleright w: \mathbb{A}]\!]=n \quad E \in \text{Sf}(\Gamma;\Delta)  \\
    \hline
  [\![E \triangleright v \text { to } *.w: \mathbb{A} ]\!]=n \cdot \lambda \cdot (m \otimes \id) \cdot \text{sp}_{\Gamma;\Delta} \cdot \text{sh}_{E}
\end{array}
$ \end{minipage} 
\hspace{130 pt}
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c} 
     [\![\Gamma,x:\mathbb{A} \triangleright v: \mathbb{B} ]\!] = m \\
    \hline
   [\![ \Gamma \triangleright \lambda x:\mathbb{A} . \hspace{2pt } v: \mathbb{A} \multimap \mathbb{B}]\!] = \overline{m \cdot \text{jn}_{\Gamma;\mathbb{A}}}
\end{array}
$
\end{minipage} \\
&
 \begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c} 
     [\![\Gamma \triangleright v: \mathbb{A} \multimap \mathbb{B} ]\!] = m \quad [\![  \Delta \triangleright w: \mathbb{A} ]\!] =n \quad E \in S\hspace{-3pt}f(\Gamma;\Delta)  \\
    \hline
  [\![ E \triangleright v w: \mathbb{B} ]\!] = \text{app} \cdot (m \otimes n) \cdot \text{sp}_{\Gamma;\Delta} \cdot \text{sh}_{E}
\end{array}
$ \end{minipage}
\hspace{190 pt} %[\![ ]\!]
\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
     [\![\Gamma \triangleright v: \mathbb{A}]\!]  = f \\
    \hline
   [\![ \Gamma \triangleright \text{dis}(v):  \mathbb{I} ]\!] = ! \cdot f
\end{array}
$
\end{minipage} \\[5pt]
&\hspace{5pt}
  %
  \begin{prooftree}
      \hypo{ [\![\Gamma \vljud v: \typeA]\!] = m }
      \infer1[]{ [\![ \Gamma \vljud \text{inl}_{\typeB} (v):  \typeA \oplus \typeB  ]\!] = \inl  \comp m}
  \end{prooftree}
  %
  \hspace{140pt}
  %
  \begin{prooftree}
    \hypo{ [\![\Gamma \vljud v: \typeB]\!] = m }
    \infer1[]{ [\![ \Gamma \vljud \text{inr}_{\typeA} (v):  \typeA \oplus \typeB  ]\!] = \inr  \comp m}
\end{prooftree}
  %
  \\
&\begin{minipage}[t]{0.3\textwidth}
$\begin{array}{c}
      [\![\Gamma\vljud v: \typeA \oplus \typeB ]\!] = b 
      \quad [\![\Delta, x:\typeA \vljud w: \typeD ]\!] = p 
      \quad [\![\Delta,y:\typeB \vljud u: \typeD ]\!] = q 
      \quad E \in \Shuff(\Gamma; \Delta)
      \\
    \hline
  [\![E \vljud \text{case } v\,  \{\text{inl}_{\typeB} (x) \Rightarrow w ;\, \text{inr}_{\typeA} (y) \Rightarrow u\}: \typeD ]\!] =   [p,q] \comp (\text{jn}_{\Delta;\typeA}\comp \sw \oplus \text{jn}_{\Delta;\typeB}\comp \sw) 
 \comp \dist \comp  
 \\[-5pt] \hspace{160pt}  (b \otimes \text{id}) \comp \text{sp}_{\Gamma;\Delta} \comp \text{sh}_{E} 
\end{array}
$
\end{minipage} \\
\end{aligned}
\end{split}
\end{equation*}
\caption{Judgment interpretation}
\label{fig:denotational_sem} 
\end{figure}



The following diagrams are useful for a clearer understanding of the interpretation of judgements given in \autoref{fig:denotational_sem}.
\begin{align*}
   & \hspace{-20pt} \llbracket \text{ax} \rrbracket : & [\![E]\!] & \xrightarrow{\hspace{2pt}\text{sh}_{E}\hspace{2pt}}   [\![\Gamma_1,\ldots,\Gamma_n ]\!]   \xrightarrow{\hspace{1pt}\text{sp}_{\Gamma;\Delta}\hspace{1pt}}  \llbracket \Gamma_1 \rrbracket \otimes \ldots \otimes \llbracket \Gamma_n \rrbracket  \\
  & & & \xrightarrow{\hspace{2pt}m_1 \otimes \ldots \otimes m_n \hspace{2pt}} \llbracket \mathbb{A}_1 \rrbracket \otimes \ldots \otimes \llbracket \mathbb{A}_n \rrbracket \xrightarrow{\hspace{2pt}\llbracket f \rrbracket \hspace{2pt}} \llbracket \mathbb{A} \rrbracket \\
  & \hspace{-20pt} \llbracket \text{hyp} \rrbracket : & \llbracket \mathbb{A} \rrbracket & \xrightarrow{\hspace{2pt}\id_{\llbracket \mathbb{A}\rrbracket}\hspace{2pt}} \llbracket \mathbb{A}\rrbracket \\
  & \hspace{-20pt}\llbracket \mathbb{I}_i \rrbracket :&  \llbracket \mathbb{I} \rrbracket & \xrightarrow{\hspace{2pt}\id_{\llbracket \mathbb{I}\rrbracket}\hspace{2pt}}  \llbracket \mathbb{I} \rrbracket\\
  & \hspace{-20pt}\llbracket \otimes_e \rrbracket : & [\![E]\!] & \xrightarrow{\hspace{2pt}\text{sh}_{E}\hspace{2pt}}   [\![\Gamma,\Delta ]\!]   \xrightarrow{\hspace{1pt}\text{sp}_{\Gamma;\Delta}\hspace{1pt}}  [\![\Gamma ]\!] \otimes [\![\Delta ]\!] \xrightarrow{ m \hspace{1pt} \otimes \hspace{1pt} \id} ([\![\mathbb{A} ]\!] \otimes [\![\mathbb{B} ]\!]) \otimes [\![\Delta ]\!]   \\
     & &  &\xrightarrow{\hspace{2pt}\text{sw}\hspace{2pt}}  [\![\Delta ]\!] \otimes ([\![\mathbb{A} ]\!] \otimes [\![\mathbb{B} ]\!]) \xrightarrow{\hspace{3pt}\alpha\hspace{3pt}}  ([\![\Delta ]\!] \otimes [\![\mathbb{A} ]\!]) \otimes [\![\mathbb{B} ]\!] \xrightarrow{\hspace{2pt}\text{jn}_{\Delta; \mathbb{A}; \mathbb{B} }\hspace{2pt}}  \llbracket \Delta, \mathbb{A},\mathbb{B}  \rrbracket  \\
     &&& \xrightarrow{\hspace{3pt}n\hspace{3pt}} \llbracket \mathbb{D} \rrbracket \\
    & \hspace{-20pt}\llbracket \otimes_i \rrbracket : & [\![E]\!] & \xrightarrow{\hspace{2pt}\text{sh}_{E}\hspace{2pt}}   [\![\Gamma,\Delta ]\!]   \xrightarrow{\hspace{1pt}\text{sp}_{\Gamma;\Delta}\hspace{1pt}}  [\![\Gamma ]\!] \otimes [\![\Delta ]\!]  \xrightarrow{\hspace{2pt} m \hspace{1pt} \otimes \hspace{1pt} n \hspace{2pt}} [\![\mathbb{A} ]\!] \otimes [\![\mathbb{B} ]\!] \\
    &\hspace{-20pt}\llbracket \mathbb{I}_e \rrbracket :  & [\![E]\!] & \xrightarrow{\hspace{2pt}\text{sh}_{E}\hspace{2pt}}   [\![\Gamma,\Delta ]\!]   \xrightarrow{\hspace{1pt}\text{sp}_{\Gamma;\Delta}\hspace{1pt}}  [\![\Gamma ]\!] \otimes [\![\Delta ]\!] \xrightarrow{ m \hspace{1pt} \otimes \hspace{1pt} \id} [\![\mathbb{I} ]\!]\otimes [\![\Delta ]\!] \xrightarrow{\hspace{2pt} \lambda \hspace{3pt}} [\![\Delta ]\!]  \xrightarrow{\hspace{2pt} n \hspace{3pt}}  \llbracket \mathbb{A} \rrbracket   \\
    &\hspace{-20pt}\llbracket \multimap_i \rrbracket : & \llbracket \Gamma \rrbracket \otimes \llbracket \mathbb{A} \rrbracket & \xrightarrow{\hspace{2pt} \overline{m \cdot \text{jn}_{\Gamma;\mathbb{A}}}  \hspace{2pt}}  \llbracket \mathbb{A} \rrbracket \multimap \llbracket \mathbb{B} \rrbracket  \hspace{60pt} (\llbracket \Gamma \rrbracket \otimes \llbracket \mathbb{A} \rrbracket  \xrightarrow{\hspace{2pt} \text{jn}_{\Gamma; \mathbb{A}} \hspace{2pt}} \llbracket \Gamma, \mathbb{A} \rrbracket  \xrightarrow{\hspace{2pt} m \hspace{2pt}}  \llbracket \mathbb{B} \rrbracket )\\
    &\hspace{-20pt}\llbracket \multimap_e \rrbracket : & [\![E]\!] & \xrightarrow{\hspace{2pt}\text{sh}_{E}\hspace{2pt}}   [\![\Gamma,\Delta ]\!]   \xrightarrow{\hspace{1pt}\text{sp}_{\Gamma;\Delta}\hspace{1pt}}  [\![\Gamma ]\!] \otimes [\![\Delta ]\!]  \xrightarrow{\hspace{2pt} m \hspace{1pt} \otimes \hspace{1pt} n \hspace{2pt}} \llbracket \mathbb{A} \rrbracket \multimap \llbracket \mathbb{B} \rrbracket \otimes \llbracket \mathbb{A} \rrbracket  \xrightarrow{\hspace{2pt} \text{app} \hspace{2pt}} \llbracket \mathbb{B} \rrbracket \\
    & \hspace{-20pt} \llbracket \text{dis} \rrbracket : & [\![\Gamma]\!] & \xrightarrow{\hspace{2pt} f \hspace{2pt}} \llbracket \mathbb{A} \rrbracket \xrightarrow{\hspace{2pt} \text{!} \hspace{2pt}} \llbracket \mathbb{I} \rrbracket\\
    & \hspace{-20pt}\llbracket \text{inl} \rrbracket : & [\![\Gamma]\!] & \xrightarrow{\hspace{2pt} m \hspace{2pt}} \llbracket \mathbb{A} \rrbracket \xrightarrow{\hspace{2pt}\inl \hspace{2pt}} \llbracket \typeA \oplus \typeB \rrbracket\\
    &\hspace{-20pt}\llbracket \text{inr} \rrbracket : & [\![\Gamma]\!] & \xrightarrow{\hspace{2pt} m \hspace{2pt}} \llbracket \mathbb{B} \rrbracket \xrightarrow{\hspace{2pt}\inr \hspace{2pt}} \llbracket \typeA \oplus \typeB \rrbracket\\
    &\hspace{-20pt}\llbracket \text{case} \rrbracket : & [\![E]\!] & \xrightarrow{\hspace{2pt}\text{sh}_{E}\hspace{2pt}}   [\![\Gamma,\Delta ]\!]   \xrightarrow{\hspace{1pt}\text{sp}_{\Gamma;\Delta}\hspace{1pt}}  [\![\Gamma ]\!] \otimes [\![\Delta ]\!] \xrightarrow{ b \hspace{1pt} \otimes \hspace{1pt} \id} ([\![\mathbb{A} ]\!] \oplus[\![\mathbb{B} ]\!]) \otimes [\![\Delta ]\!]   \\
     & &  &\xrightarrow{\hspace{2pt}\dist\hspace{1pt}} \left([\![\mathbb{A} ]\!] \otimes [\![\Delta ]\!]\right) \oplus \left([\![\mathbb{B} ]\!] \otimes [\![\Delta ]\!]\right)  \\
     & & &  \xrightarrow{\hspace{1pt}\text{jn}_{\Delta;\typeA}\comp \sw \,\oplus  \, \text{jn}_{\Delta;\typeB}\comp \sw\hspace{1pt}}  \left([\![ \Delta,\mathbb{A}]\!]\right) \oplus \left([\![\Delta, \mathbb{B} ]\!]\right) \xrightarrow{\hspace{2pt}[p,q]\hspace{2pt}} \llbracket \mathbb{D} \rrbracket \\
\end{align*} 


Regarding the interpretation of the exhange and substitution properties, we have the following lemma.

\begin{lemma} \label{lem:sub_exch}
  For any judgements $\Gamma, x: \mathbb{A}, y: \mathbb{B}, \Delta \triangleright v : \mathbb{D}$, $\Gamma, x: \mathbb{A} \triangleright v: \mathbb{B}$, and $\Delta \triangleright w: \mathbb{A}$, the following holds:
\begin{equation*}
\begin{split}
  \llbracket \Gamma, x: \mathbb{A}, y: \mathbb{B}, \Delta \triangleright v : \mathbb{D} \rrbracket & = \llbracket \Gamma,  y: \mathbb{B}, x: \mathbb{A}, \Delta \triangleright v : \mathbb{D} \rrbracket \cdot \text{exch}_{\Gamma, \underline{x: \mathbb{A}, y: \mathbb{B}}, \Delta}\\
  \llbracket \Gamma,\Delta \triangleright v[w/x]: \mathbb{B} \rrbracket & = \llbracket \Gamma, x: \mathbb{A} \triangleright v: \mathbb{B} \rrbracket \cdot \text{jn}_{\Gamma;\mathbb{A}} \cdot (\id \otimes \llbracket \Delta \triangleright w: \mathbb{A} \rrbracket ) \cdot \text{sp}_{\Gamma; \Delta} 
\end{split}
\end{equation*}
\end{lemma}


\begin{proof}
  This lemma is proved in \cite[Lemma 2.2]{dahlqvist2022syntactic} for the lambda calculus without conditionals, so we only need to address the conditional cases.
\todo[inline,size=\normalsize]{Passar para aqui as provas qd elas estiverem vistas} 
\end{proof}




\begin{theorem} \label{thm:soundness_classical}
The equations presented in \autoref{fig:equations-linear-lambda} are sound with respect to judgement interpretation. 
More specifically, if \(\Gamma \vljud v = w : \typeA\) is one of the equations in \autoref{fig:equations-linear-lambda}, then 
$\llbracket \Gamma \vljud v : A  \rrbracket = \llbracket  \Gamma \vljud w : \typeA\rrbracket.$
\end{theorem}

\begin{proof}
  Given the theorem is already proven in \cite[Theorem 2.3]{dahlqvist2022syntactic} for the lambda calculus without conditionals, it suffices to consider the cases involving conditionals.
  \todo[inline,size=\normalsize]{Passar para aqui as provas qd elas estiverem vistas} 
\end{proof}

\begin{definition} [Models of linear \(\lambda\)-theories]
Consider a linear \(\lambda\)-theory \(((G, \Sigma), Ax)\) and a symmetric monoidal closed category with coproducts \(\catC\). 
Suppose that for each \(X \in G\), we have an interpretation \(\llbracket X \rrbracket\), which is an object of \(\catC\), 
and analogously for the operation symbols in \(\Sigma\). 
This interpretation structure is a \emph{model} of the theory if all axioms in \(Ax\) are satisfied by the interpretation.
\end{definition}

\begin{theorem}[Completeness] \label{thm:completeness_classical}
Consider a linear $\lambda$-theory $\mathscr{T}$. Then an equation 
$\Gamma \vljud v = w : \typeA$
is a theorem of $\mathscr{T}$ if and only if it is satisfied by all models of the theory.
\end{theorem}

 \begin{proof}
   This theorem is proved in \cite[Lemma 2.6]{dahlqvist2022syntactic} for the lambda calculus without conditionals, so we only need to address the conditional cases.
\todo[inline,size=\normalsize]{Passar para aqui as provas qd elas estiverem vistas} 
 \end{proof}


\begin{example}
  We now illustrate how the programs presented in Examples \ref{example:prog_swap}, \ref{example:prog_Dis2nd}, and \ref{example:prog_Dis1stor2nd}, with slight modifications, are interpreted in $\catSet$.
   To this effect, we consider a type $N$ representing set of natural numbers, along with a family of operations $\{n : \typeI \to \emph{N} \mid n \in \mathbb{N}\}$, each mapping the monoidal unit to a corresponding natural number $n$. In $\catSet$ have $\sem{\typeI}= \{*\}$, and define $\sem{N} = \mathbb{N}$, and  $\sem{n} = \{*\} \to \mathbb{N}, \, * \mapsto n.$ 
   Consider the following $\lambda$-term:
   \begin{equation*}
   \begin{split}
    &   x : N \otimes N \triangleright \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a \, [1(*)\otimes 2 (*)/x]: N \otimes N 
  \end{split}
  \end{equation*}

   Attending to \autoref{fig:denotational_sem} this program is interpreted as follows:
   \begin{align*}
   & \hspace{-30pt} \sem{ \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a \, [1(*)\otimes 2 (*)/x]} \\
   & \hspace{-30pt} = \sem{\text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1pt} b \otimes a} \cdot \jn \cdot \left( \id \otimes \sem{1(*)\otimes 2 (*)} \right) \cdot \spt & (\text{\autoref{lem:sub_exch}})\\
   & \hspace{-30pt} = \sem{b \otimes a} \cdot \jn \cdot \alpha \cdot \sw \cdot \left( \sem{x} \otimes \id\right) \cdot \spt \cdot \sh \cdot \lambda \cdot ( \id \otimes (\sem{1(*)} \otimes \sem{2(*)}   & (\otimes_e, \otimes_i) \\
   & \hspace{-20pt} \cdot \spt \cdot \sh))  \cdot \lambda^{-1} \\
   & \hspace{-30pt} = \sem{b} \otimes \sem{a} \cdot \spt \cdot \sh \cdot (\lambda \otimes \id) \cdot \alpha \cdot \sw \cdot \rho^{-1} \cdot \lambda \cdot (\id \otimes ( 1 \cdot \sem{*} \cdot \spt \cdot \sh \, \otimes  & (\otimes_i, \text{hyp}, \\
   & \hspace{-14pt}   2 \cdot \sem{*} \cdot \spt \cdot \sh \cdot \lambda^{-1}))  \cdot \lambda^{-1}  &  \text{ax}) \\
   & \hspace{-30pt} = \sw \cdot \lambda \cdot (\id \otimes ( 1 \otimes 2 \cdot \lambda^{-1} )) \cdot \lambda^{-1} & (\text{hyp}, \typeI_i, \text{coh}) \\
   & \hspace{-30pt} = 2 \otimes 1 \cdot \rho^{-1}  & (\text{nat}, \text{coh}) 
   \end{align*}

   Next, consider the  $\lambda$-terms below.

   \begin{equation*}
    \begin{split}
    & \textbf{Dis2nd} \triangleq - \triangleright \lambda x: N \otimes N. \text{pm} \hspace{3 pt} x \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1 pt} \text{dis}(b) \hspace{3 pt} \text{ to } *.a: N \otimes N \multimap N \\
    & \textbf{Dis2nd} \,(1 (*) \otimes 2(*)) \\
    \end{split}
   \end{equation*}

   \begin{comment}
     &  \textbf{Dis1stOR2nd} \triangleq - \triangleright  \lambda y: N \otimes N . \, 
\text{case } \text{inl} (*) \,  
  \left\{
    \begin{aligned} 
    &\inl_{\typeB}(w) \Rightarrow  \textbf{Dis1st} \, x; \\
    &\inr_{\typeB}(z) \Rightarrow  \textbf{Dis2nd} \, x   \\ 
  \end{aligned}  
  \right\} : N \otimes N \multimap N  & \\
  & \textbf{Dis1stOR2nd} \, (1 (*) \otimes 2(*)) : N 
    \end{comment}

  In this case, we will ``cheat'' slightly by first using the equations in \autoref{fig:equations-linear-lambda} to simplify the term. We note that here we will not explicitly  illustrate the interpretation of the \text{case} statement, because in~$\catSet$, any term of type~$\typeA \otimes \typeA$, for any type~$\typeA$, corresponds to an injection ($\text{inl}(v)$ or $\text{inr}(v)$). As a result, we can ``escape'' any \text{case} statement by  applying the equations~$\beta_{case}^{inl}$ and~$\beta_{case}^{inr}$.


   %In this case, we will ``cheat'' slightly by first using the equations in \autoref{fig:equations-linear-lambda} to simplify the term. However, to explicitly illustrate the interpretation of the \text{case} statement---and because in~$\catSet$, any term of type~$\typeA \otimes \typeA$, for any type~$\typeA$, corresponds to an injection ($\text{inl}(v)$ or $\text{inr}(v)$)---we will avoid applying the equations~$\beta_{case}^{inl}$ and~$\beta_{case}^{inr}$.
   
   Applying the $\beta$ equation we have 
   \begin{equation*}
    \begin{split}
    \textbf{Dis2nd} \,(1 (*) \otimes 2(*)) & = \textbf{Dis2nd} \,[1 (*) \otimes 2(*)/x] \\
    & = \text{pm} \hspace{3 pt}  1 (*) \otimes 2(*) \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1 pt} \text{dis}(b) \hspace{3 pt} \text{ to } *.a
\end{split}
   \end{equation*}
    The resulting program is interpreted as follows:
    \begin{align*}
   & \hspace{-30pt} \sem{ \text{pm} \hspace{3 pt}  1 (*) \otimes 2(*) \hspace{3 pt} \text{to} \hspace{3 pt} a \otimes b. \hspace{1 pt} \text{dis}(b) \hspace{3 pt} \text{ to } *.a} \\
   & \hspace{-30pt} = \sem{\text{dis}(b) \hspace{3 pt} \text{ to } *.a} \cdot \jn \cdot \alpha \cdot \sw \cdot \left( \sem{1 (*) \otimes 2(*)} \otimes \id\right) \cdot \spt \cdot \sh   & (\otimes_e) \\
   & \hspace{-30pt} = \sem{a} \cdot \lambda \cdot (\sem{\text{dis}(b)} \otimes \id) \cdot \spt \cdot \sh \cdot (\lambda \otimes \id) \cdot \alpha \cdot \sw \cdot (  (\sem{1(*)} \otimes \sem{2(*)}     & (\typeI_e, \otimes_i) \\
   & \hspace{-20pt} \cdot \spt \cdot \sh)\otimes \id) \cdot  \lambda^{-1}  \\
   & \hspace{-30pt} = \lambda \cdot (! \cdot \sem{b} \otimes \id) \cdot \sw \cdot   (\lambda \otimes \id)  \cdot \alpha \cdot \sw \cdot (( 1 \cdot \sem{*} \cdot \spt \cdot \sh \, \otimes 2 \cdot \sem{*} \cdot \spt  & (\text{hyp},\text{dis},  \text{ax})\\
   &  \hspace{-14pt}    \cdot \sh \cdot \lambda^{-1})\otimes \id) \cdot \lambda^{-1} \\
    & \hspace{-30pt} = \lambda \cdot (! \otimes \id) \cdot \sw \cdot \rho \cdot (( 1  \, \otimes 2 \cdot \lambda^{-1}  ) \otimes \id) \cdot \lambda^{-1}  & (\text{hyp},\typeI_i,\text{coh})\\
   & \hspace{-30pt} =\lambda \cdot (! \otimes \id) \cdot  2  \, \otimes 1 \cdot \rho^{-1}   & (\text{nat}, \text{coh}) 
   \end{align*}

   The diagram below illustrates the resulting interpretation.
   \begin{align*}
   & \hspace{-40pt} & \{*\} & \xrightarrow{\hspace{2pt}\rho^{-1}\hspace{2pt}}    \{(*,*)\}  \xrightarrow{2  \, \otimes 1}   \{(2,1)\}     \xrightarrow{(! \otimes \id)} \{(*,1)\} \xrightarrow{\hspace{2pt}\lambda \hspace{2pt}} \{1\}
   \end{align*}
\end{example}


\subsection{Semantics of metric equations} \autoref{subsec:semantics_metric}
 We will now turn our attention to the semantics of the metric equations. First, we recall the definition of a metric space.

 \begin{definition}
    A \emph{metric space} is a pair $(X, d)$ where $X$ is a set and $d: X \times X \to [0, \infty)$ is a function known as \emph{distance} satisfying:
    \begin{enumerate}
        \item $0 \leq d(x, y)$, with equality if and only if $x = y$,
        \item $d(x, y) = d(y, x)$,
        \item $d(x, z) \leq d(x, y) + d(y, z)$ for all $x, y, z \in X$.
    \end{enumerate}
  \end{definition}
 
 Here, we equip each hom-set $\catC(A, B)$ of a category $\catC$ with a metric $d_{A,B}$, and impose that both composition and precomposition are non-expansive. That is, for all morphisms $f,f_1, f_2 \in \catC(A, B)$ and any $g, g_1, g_2 \in \catC(B, C)$, the following inequalities holds:
$$
d_{A,C}(g \circ f_1, g \circ f_2) \leq d_{A,B}(f_1, f_2) \hspace{40pt} d_{A,C}(g_1 \circ f, g_2 \circ f) \leq d_{B,C}(g_1, g_2).
$$
Note that, given the triangle inequality, we have:
\begin{equation*}
  d_{A,C}(g_1 \circ f_1, g_2 \circ f_2) \leq  d_{A,C}(g_1 \circ f_1, g_1 \circ f_2) + d_{A,C}(g_1 \circ f_2, g_2 \circ f_2) \leq  d_{A,B}(f_1, f_2) +   d_{B,C}(g_1, g_2).
\end{equation*}

This is known as \emph{enriching} the category $\catC$ \emph{over metric spaces}. 

In this context, \emph{soundness} and \emph{completeness} concepts are extended to emcompass not only the classical equations but also the metric equations. Note that the $=$ in classical equations can be written as $=_0$. As a result, in this setting we define 

\begin{itemize}
  \item \emph{Soundness} is the property 
  \[
  M =_{\varepsilon} N \;\Rightarrow\;  d (\llbracket N \rrbracket, \llbracket M \rrbracket) \leq \varepsilon
  \quad \text{for all interpretations in the class.}
  \]
  That is, if two terms are provably at a maximum distance $_{\varepsilon}$, so are there respective interpretations
  \item \emph{Completeness} is the property 
  \[
  d (\llbracket N \rrbracket, \llbracket M \rrbracket) \leq \varepsilon
  \;\Rightarrow\;  M =_{\varepsilon} N 
  \quad \text{for all interpretations in the class.}
  \]
  That is, if $_{\varepsilon}$ is the maximum distance between the interpretation of two programs, then they are provably at a maximum distance $_{\varepsilon}$.
\end{itemize}



In this context, the notions of \emph{soundness} and \emph{completeness} are extended to encompass not only classical equations but also \emph{metric equations}. Note that classical equations, written as $M = N$, can be viewed as a special case of metric equations where the distance is zero, \ie, $M =_0 N$. We define:

\begin{itemize}
  \item \emph{Soundness} is the property 
  \[
  M =_{\varepsilon} N \;\Rightarrow\; d\big(\llbracket M \rrbracket, \llbracket N \rrbracket\big) \leq \varepsilon
  \quad \text{for all interpretations in the class.}
  \]
  That is, if two terms are provably within a distance $\varepsilon$, then so are their interpretations.

  \item \emph{Completeness} is the property 
  \[
  d\big(\llbracket M \rrbracket, \llbracket N \rrbracket\big) \leq \varepsilon \;\Rightarrow\; M =_{\varepsilon} N
  \quad \text{for all interpretations in the class.}
  \]
  That is, if the interpretations of two terms are within a distance $\varepsilon$, then the terms are provably at most a distance $\varepsilon$ apart.
\end{itemize}

\vspace{20pt}

\todo[inline,size=\normalsize]{Esta parte acho que vai ficar um pouco dúbia, melhor confirmar com prof. Renato.} 
\todo[inline,size=\normalsize]{O que fazer com a def de adjunção -> theorem 4.2 artigo} 

\begin{definition}
    $\catMet$ denotes the category whose objects are metric spaces and whose morphisms are non-expansive maps, \ie, functions that do not increase the distance between points. More precisely, for two metric spaces $(X, d_X)$ and $(Y, d_Y)$, a morphism $f: (X, d_X) \to (Y, d_Y)$ is a function $f: X \to Y$ such that
$$
d_Y(f(x), f(y)) \leq d_X(x, y) \quad \text{for all } x, y \in X.
$$
  \end{definition}

\begin{definition} \label{def:monoidal_closed_met_cat}
A \emph{$\catMet$-category} $\catC$ is a category in which each hom-set $\catC(A, B)$ is equipped with a metric $d_{A,B}$, such that for all morphisms $f_1, f_2 \in \catC(A, B)$ and $g_1, g_2 \in \catC(B, C)$, the following inequality holds:
\[
  d_{A,C}(g_1 \circ f_1, g_2 \circ f_2) \leq d_{A,B}(f_1, f_2) + d_{B,C}(g_1, g_2).
\]

A \emph{symmetric monoidal $\catMet$-category} $\catC$ is a category that is both symmetric monoidal and a $\catMet$-category, such that for all morphisms $f_1, f_2 \in \catC(A, B)$ and $g_1, g_2 \in \catC(C, D)$, we have:
\[
  d_{A \otimes C, B \otimes D}(f_1 \otimes g_1, f_2 \otimes g_2) \leq d_{A,B}(f_1, f_2) + d_{C,D}(g_1, g_2).
\]

A \emph{symmetric monoidal closed $\catMet$-category} $\catC$ is a category that is both symmetric monoidal closed and a symmetric monoidal $\catMet$-category, such that for all objects $A$, $B$, and $C$, there exists a $\catMet$-isomorphism
\[
  \catC(B \otimes A, C ) \cong \catC(C, A \multimap B)
\]
natural in $B$ and $C$.
\end{definition}


\begin{theorem}\cite[Theorem 4.2]{dahlqvist2022syntactic} 
Let $\catC$ be a category that is both monoidal closed and a symmetric monoidal $\catMet$- category, and suppose that for all objects $X \in \catC$, the functor $X \multimap - : \catC \to \catC$ is non-expansive. Then $\catC$ is a \emph{symmetric monoidal closed $\catMet$-category}.
\end{theorem}


\begin{theorem} [Soundness] \cite[Theorem 3.14]{dahlqvist2022syntactic}  \label{thm:soundness_metric_no_cond}
  The rules in Figures \ref{fig:equations-linear-lambda} and \ref{fig:metric deductive system} are sound for a  symmetric monoidal closed $\catMet$-category  $\catC$ over metric spaces. Specifically, if $\Gamma \vljud v =_{q} w : \typeA $ results from the rules in Figures \ref{fig:equations-linear-lambda} and \ref{fig:metric deductive system} then $q \geq d( \llbracket \Gamma  \vljud v : \typeA \rrbracket, \llbracket\Gamma \vljud w : \typeA \rrbracket)$.
\end{theorem}


\begin{definition} \label{def:model_metric_no_cond}
  Consider a metric $\lambda$-theory $((G,\Sigma),Ax)$ and a symmetric monoidal closed $\catMet$-category $\catC$. Suppose that for each $X \in G$ we have an interpretation $\llbracket X \rrbracket$ as a $\catC$-object and analogously for the operation symbols. This interpretation structure is a model of the theory if all axioms in $Ax$ are satisfied by the interpretation.
\end{definition}


For two types $\typeA$ and $\typeB$ of a metric $\lambda$-theory $\mathscr{T}$ , consider the class \texttt{Values}$(\typeA,\typeB)$ of values $v$ such that $x : \typeA \vljud v : \typeB$. We equip $\texttt{Values}(\typeA,\typeB)$ with the function $d :\texttt{Values}(\typeA,\typeB) \times \texttt{Values}(\typeA,\typeB) \rightarrow \mathbb{Q}^{+}_0$ defined by,
$$d(v,w)=\inf{\{q \, \vert \, v=_q w \text{ is a theorem of } \mathscr{T} \}}$$

\texttt{Values}$((\typeA,\typeB),d)$ is a (possibly large)  $\catMet$- category. We  quotient this  $\catMet$-category into a separated  $\catMet$-category which we denote by (\texttt{Values}$(\typeA,\typeB),d)$/$\sim$. We call  $\mathscr{T}$ \emph{varietal} if (\texttt{Values}$(\typeA,\typeB),d)$/$\sim$ is a small  $\catMet$-category for all types $\typeA$ and $\typeB$. For the rest of this work, we will focus exclusively on varietal theories and locally small categories.

